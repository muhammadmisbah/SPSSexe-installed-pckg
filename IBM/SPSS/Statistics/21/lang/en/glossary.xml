<?xml version="1.0" encoding="UTF-8"?>
<!--Arbortext, Inc., 1988-2007, v.4002-->
<!DOCTYPE sourcedocument SYSTEM "\pubs\standards\dtd\1source.dtd">
<?Pub EntList copy trade reg mdash ndash minus times divide scaron?>
<sourcedocument>
<title></title>
<synopsis></synopsis>
<metadata>
<owner></owner>
</metadata>
<glossary>
<term termid="casenum_system_variable"><linktext>$CASENUM</linktext
> Current case sequence number. For each case, $CASENUM is the number
of cases read up to and including that case. The format is F8.0. The
value of $CASENUM is not necessarily the row number in a Data Editor
window (available in windowed environments), and the value changes
if the file is sorted or new cases are inserted before the end of
the file.</term>
<term termid="sysmis_system_variable"><linktext>$SYSMIS</linktext
> System-missing value. The system-missing value displays as a period
(.) or whatever is used as the decimal indicator. </term>
<term termid="jdate_system_variable"><linktext>$JDATE</linktext> Current
date in number of days from October 14, 1582 (day 1 of the Gregorian
calendar). The format is F6.0.</term>
<term termid="date_system_variable"><linktext>$DATE</linktext> Current
date in international date format with two-digit year. The format
is A9 in the form dd-mmm-yy. </term>
<term termid="date11_system_variable"><linktext>$DATE11</linktext
> Current date in international date format with four-digit year.
The format is A11 in the form dd-mmm-yyyy.</term>
<term termid="time_system_variable"><linktext>$TIME</linktext> Current
date and time. $TIME represents the number of seconds from midnight,
October 14, 1582, to the date and time when the transformation command
is executed. The format is F20. You can display this as a date in
a number of different date formats. You can also use it in date and
time functions.</term>
<term termid="_pct_of_cases_1"><linktext>% of cases</linktext>The
value graphed represents the number of cases in each category, expressed
as a percentage of the total number of cases.</term>
<term termid="_pct_of_total"><linktext>% of Total</linktext>Displays
the total percentage falling into the defined category.</term>
<term termid="_pct_of_variable"><linktext>% of Variable</linktext
>Displays the percentage of the variable falling into the defined
cell.</term>
<term termid="_pct_of_variance"><linktext>% of variance</linktext
>The percentage of variance in the observed variables accounted for
by the factor.</term>
<term termid="_i_1_j_1"><linktext>(I) ^1, (J) ^1</linktext>Levels
of the factor for which pairwise comparisons are requested.  The mean
difference I-J is tested.</term>
<term termid="_1pct_confidence_interval_for_difference"><linktext
>^1% Confidence Interval for Difference</linktext>The confidence interval
for the difference in the mean values of a given pairwise comparison.</term>
<term termid="_lt_1_missing"><linktext>&lt; 1 missing</linktext>This
procedure treat values &lt; 1 as missing.  If you wish to use them
in the analysis, recode them to values &gt;= 1.</term>
<term termid="_lt_control"><linktext>&lt; Control</linktext>Tests
if the mean at any level of the factor is smaller than that of the
control category.</term>
<term termid="_gt_control"><linktext>&gt; Control</linktext>Tests
if the mean at any level of the factor is larger than that of the
control category.</term>
<term termid="_-2_log_likelihood_2"><linktext>-2 Log Likelihood</linktext
>A measure of how well the model fits the data, also called the deviance.
The smaller the value the better the fit.</term>
<term termid="_-2_log_likelihood"><linktext>-2 Log Likelihood</linktext
>A measure of how well the model fits the data, also called the deviance.
The smaller the value the better the fit. In stepwise methods, the
change in -2 log likelihood tests the null hypothesis that the coefficients
of the terms removed from the model are zero.</term>
<term termid="_-2_log_likelihood_for_the_reduced_model"><linktext
>-2 log likelihood for the reduced model</linktext>This is the -2
log likelihood for the model when the effect is removed from the fitted
model.</term>
<term termid="_-2_log-likelihood"><linktext>-2 log-likelihood</linktext
>A measure of how well the model fits the data, also called the deviance.
The smaller the value the better the fit.</term>
<term termid="_25th_percentile"><linktext>25th Percentile</linktext
>The value above which 75 percent of the observed cases fall and below
which 25 percent of the observed cases fall.</term>
<term termid="_2-sided"><linktext>2-sided</linktext>Pairwise multiple
comparison t test that compares a set of treatments against a single
control mean. Tests that the mean at any level (except the control
category) of the factor is not equal to that of the control category.</term>
<term termid="_2-way_interaction"><linktext>2-Way Interaction</linktext
>Limits the maximum order of factor interaction in the model to 2.</term>
<term termid="_3-d_scatterplot"><linktext>3-D Scatterplot</linktext
>Plots 3 variables on three axes. The points are plotted in a 3-D
coordinate system that can be rotated by editing the chart after it
appears in the Viewer.</term>
<term termid="_3-way_interaction"><linktext>3-Way Interaction</linktext
>Limits the maximum order of factor interaction in the model to 3.</term>
<term termid="_4th_order_term"><linktext>4th Order Term</linktext
>Displays the results of the 4th-degree polynomial term. A 4th-degree
polynomial term is the term with a variable raised to the 4th power.</term>
<term termid="_4-way_interaction"><linktext>4-Way Interaction</linktext
>Limits the maximum order of factor interaction in the model to 4.</term>
<term termid="_5pct_trimmed_mean"><linktext>5% Trimmed Mean</linktext
>The arithmetic mean calculated when the largest 5% and the smallest
5% of the cases have been eliminated. Eliminating extreme cases from
the computation of the mean results in a better estimate of central
tendency, especially when the data are nonnormal.</term>
<term termid="_50th_percentile"><linktext>50th Percentile</linktext
>The median. The value above and below which half the observed values
of a variable fall.</term>
<term termid="_5th_order_term"><linktext>5th Order Term</linktext
>Displays the results of the 5th-degree polynomial term. A 5th-degree
polynomial term is the term with a variable raised to the 5th power.</term>
<term termid="_5-way_interaction"><linktext>5-Way Interaction</linktext
>Limits the maximum order of factor interaction in the model to 5.</term>
<term termid="_75th_percentile"><linktext>75th Percentile</linktext
>The value above which 25 percent of the observed cases fall and below
which 75 percent of the observed cases fall.</term>
<term termid="_95pct_confidence_interval"><linktext>95% Confidence
Interval</linktext>If the 95% confidence interval does not contain
1, this indicates that the incidence rates of the defined event are
not the same for the two groups.</term>
<term termid="_95pct_trimmed_range"><linktext>95% Trimmed Range</linktext
>Bootstrapping produces parameter estimates for each repeated sample.
 The bounds of the 95% trimmed range are produced by taking the 2.5th
and 97.5th percentiles of the set of estimates.</term>
<term termid="a"><linktext>A</linktext>Phamtom group A contains Category,
N, Observed Proportion, Test Proportion, and Asymptotic Significance.</term>
<term termid="aad"><linktext>AAD</linktext>Average Absolute Deviation.
The result of summing the absolute deviations of the ratios about
the median and dividing the result by the total number of ratios.</term>
<term termid="abs_numexpr"><linktext>ABS(numexpr)</linktext>ABS(numexpr).
Numeric. Returns the absolute value of numexpr, which must be numeric.</term>
<term termid="absolute_extreme_difference"><linktext>Absolute Extreme
Difference</linktext>The largest absolute difference between two cumulative
distribution functions.</term>
<term termid="absolute_values"><linktext>Absolute Values</linktext
>Similarity or dissimilarity measures with signs removed. Used when
the only information of interest is the magnitudes of relationships.</term>
<term termid="accounted_for"><linktext>Accounted for</linktext>The
inertia for each dimension divided by total inertia.</term>
<term termid="activation_function"><linktext>Activation Function</linktext
>The activation function "links" the weighted sums of units in a layer
to the values of units in the succeeding layer.</term>
<term termid="activation_function_rbf"><linktext>Activation Function
(RBF)</linktext>The activation function for the hidden layer is the
radial basis function, which "links" the units in a layer to the values
of units in the succeeding layer.  For the output layer, the activation
function is the identity function, thus the output units are simply
weighted sums of the hidden units.</term>
<term termid="active_cases_with_missing_values"><linktext>Active Cases
with Missing Values</linktext>The number of cases (objects) that could
be used to determine a solution that have missing values.</term>
<term termid="working_data_file"><linktext>Active Dataset</linktext
>The dataset that is currently active. When you open a data file,
it becomes the active dataset. All analysis is performed on the active
dataset, and most analytical commands are unavailable unless you have
an active dataset.</term>
<term termid="active_margin"><linktext>Active margin</linktext>The
total for each category of a variable over the categories of the other
variable.</term>
<term termid="active_proximities"><linktext>Active Proximities</linktext
>The number of nonmissing proximities.</term>
<term termid="active_total"><linktext>Active total</linktext>Displays
summary statistics for the active (not supplementary) points.</term>
<term termid="actual"><linktext>Actual</linktext>The actual number
or proportion of units sampled.</term>
<term termid="actual_pct_outside"><linktext>Actual % outside</linktext
>The observed percentage outside the specification limits.  A point
is defined as outside the specification limits when its value is greater
than or equal to the upper specification limit or is less than or
equal to the lower specification limit.</term>
<term termid="actual_pct_outside_lsl"><linktext>Actual % &lt; LSL</linktext
>The observed percentage outside the lower specification limit. A
point is defined outside the lower specification limit when its value
is less than or equal to the lower specification limit.</term>
<term termid="actual_pct_outside_usl"><linktext>Actual % &gt; USL</linktext
>The observed percentage outside the upper specification limit. A
point is defined outside the upper specification limit when its value
is greater than or equal to the upper specification limit.</term>
<term termid="actual_group"><linktext>Actual Group</linktext>The actual
group the case belongs to.</term>
<term termid="ad1_rho_1"><linktext>AD1 rho ^1</linktext>A correlation
term for the First-Order Ante-Dependence covariance structure.</term>
<term termid="add"><linktext>Add</linktext>Adds a value to the list.</term>
<term termid="add_aggregated_variables_to_working_data_file"><linktext
>Add aggregated variables to active dataset</linktext>New variables
based on aggregate functions are added to the active dataset. The
data file itself is not aggregated. Each case with the same value(s)
of the break variable(s) receives the same values for the new aggregate
variables.</term>
<term termid="add_to_file_1"><linktext>Add to file</linktext>The new
series created by Seasonal Decomposition are saved as regular variables
in your active dataset. Variable names are formed from a three-letter
prefix, an underscore, and a number.</term>
<term termid="add_to_file"><linktext>Add to file</linktext>The new
series are saved as regular variables in your active dataset. Variable
names are formed from a three-letter prefix, an underscore, and a
number.</term>
<term termid="additive"><linktext>Additive</linktext>An outlier that
affects a single observation. For example, a data coding error might
be identified as an additive outlier.</term>
<term termid="additive_patch"><linktext>Additive patch</linktext>A
group of two or more consecutive additive outliers. Selecting this
outlier type results in the detection of individual additive outliers
in addition to patches of them.</term>
<term termid="gseasadd"><linktext>Additive Seasonal Adjustment</linktext
>The seasonal adjustments are added to the seasonally adjusted series
to obtain the observed values. This adjustment attempts to remove
the seasonal effect from a series in order to look at other characteristics
of interest that may be "masked" by the seasonal component. In effect,
seasonal components that do not depend on the overall level of the
series. Observations without seasonal variation have a seasonal component
of 0.</term>
<term termid="exsm_sadd"><linktext>Additive Seasonal Component (Exponential
Smoothing)</linktext>The series has seasonal variation that is additive:
the magnitude of seasonal variation does not depend on the overall
level of the series.</term>
<term termid="adjusted"><linktext>Adjusted</linktext>Values adjusted
for cells with an expected count of 0.</term>
<term termid="adjusted_for_independents"><linktext>Adjusted for Independents</linktext
>The adjusted mean values for each category expressed as deviations
from the grand mean when other independent factors are adjusted for.</term>
<term termid="adjusted_for_independents_and_covariates"><linktext
>Adjusted for Independents and Covariates</linktext>The adjusted mean
values for each category expressed as deviations from the grand mean
when other independent factors and covariates are adjusted for.</term>
<term termid="adjusted_predicted_value"><linktext>Adjusted Predicted
Value</linktext>The predicted value for a case when that case is excluded
from the calculation of the regression coefficients.</term>
<term termid="adjusted_residual_general_loglinear"><linktext>Adjusted
Residual (General Loglinear)</linktext>The standardized residual divided
by its estimated standard error. Since the adjusted residuals are
asymptotically standard normal when the selected model is correct,
they are preferred over the standardized residuals for checking for
normality.</term>
<term termid="adjusted_residual_sum_of_squares"><linktext>Adjusted
Residual Sum of Squares</linktext>Equal to the residual sum of squares
adjusted for the covariation between the observations.</term>
<term termid="adjusted_r_squared"><linktext>Adjusted R-Squared</linktext
>The sample R squared tends to optimistically estimate how well the
models fits the population. The model usually does not fit the population
as well as it fits the sample from which it is derived. Adjusted R
squared attempts to correct R squared to more closely reflect the
goodness of fit of the model in the population.</term>
<term termid="adjusted_standardized_residual"><linktext>Adjusted Standardized
Residual</linktext>The residual for a cell (observed minus expected
value) divided by an estimate of its standard error. The resulting
standardized residual is expressed in standard deviation units above
or below the mean.</term>
<term termid="aempirical"><linktext>Aempirical</linktext>A method
of estimating percentiles using the empirical distribution function
with averaging.</term>
<term termid="after_effects"><linktext>After Effects</linktext>Enters
the covariates after the factor effects are evaluated. Available only
if one or more covariates are in the model and the Hierarchical or
Experimental method is used.</term>
<term termid="after_transformation_1"><linktext>After Transformation</linktext
>Statistics in this category apply to the quantified (optimally scaled)
variables.</term>
<term termid="agglomeration_schedule"><linktext>Agglomeration Schedule</linktext
>The results of the cluster analysis are summarized in a listing of
the cluster numbers being combined at each stage in the cluster analysis.</term>
<term termid="agglomerative_hierarchical_clustering"><linktext>Agglomerative
Hierarchical Clustering</linktext>A method for creating clusters in
which each case starts out as a cluster. At every step, clusters are
combined until all cases are members of a single cluster. Once a cluster
is formed it cannot be split, it can only be combined with other clusters.</term>
<term termid="aic"><linktext>AIC</linktext>A measure for selecting
and comparing mixed models based on the -2 (Restricted) log likelihood.
Smaller values indicate better models. The AIC "penalizes" overparametrized
models.</term>
<term termid="aic_change"><linktext>AIC change</linktext>The difference
in Akaike Information Criterion between the given model and the next
smaller model.  Thus, the AIC change for 2 clusters is the difference
in the AIC between the model with 2 clusters and the model with 1
cluster.</term>
<term termid="aic_reduced"><linktext>AIC reduced</linktext>This is
the AIC for the model when the effect is removed from the fitted model.</term>
<term termid="aicc"><linktext>AICC</linktext>A measure for selecting
and comparing mixed models based on the -2 (Restricted) log likelihood.
Smaller values indicate better models.  The AICC "corrects" the AIC
for small sample sizes.  As the sample size increases, the AICC converges
to the AIC.</term>
<term termid="akaike_information_criterion"><linktext>Akaike Information
Criterion</linktext>A statistic that helps you choose between models.
The AIC takes into account both how well the model fits the observed
series, and the number of parameters used in the fit. Smaller values
of the AIC indicate better models.</term>
<term termid="akaike_information_criterion_aic_1"><linktext>Akaike
Information Criterion (AIC)</linktext>A measure for selecting and
comparing models based on the -2 log likelihood. Smaller values indicate
better models. The AIC "penalizes" overparametrized models.</term>
<term termid="akaike_information_criterion_aic"><linktext>Akaike Information
Criterion (AIC)</linktext>A statistic that helps you decide the order
of a model. The AIC takes into account both how well the model fits
the observed series, and the number of parameters used in the fit.
Look for a model that adequately describes the series and has the
minimum AIC.</term>
<term termid="alignment"><linktext>Alignment</linktext>Defined alignment
(left, right, center).</term>
<term termid="all_22"><linktext>All</linktext>This row contains the
results of the test of the joint hypothesis for all logits.</term>
<term termid="all_cases_1"><linktext>All Cases</linktext>Turns case
filtering off and uses all cases.</term>
<term termid="all_cases"><linktext>All Cases</linktext>Includes all
cases in the casewise diagnostics. For large files the output will
be lengthy, since a line is displayed for each case.</term>
<term termid="all_cases_constant"><linktext>All Cases Constant</linktext
>All valid cases take the same value for these variables.</term>
<term termid="all_cases_missing"><linktext>All Cases Missing</linktext
>The values of all cases are missing for these variables.</term>
<term termid="all_categories_equal"><linktext>All Categories Equal</linktext
>All categories have equal expected values.</term>
<term termid="all_clusters"><linktext>All Clusters</linktext>Displays
icicle plot for all cluster solutions.</term>
<term termid="all_groups_equal"><linktext>All Groups Equal</linktext
>Prior probabilities of group membership are assumed to be equal.</term>
<term termid="all_other_values"><linktext>All Other Values</linktext
>Any remaining values not included in one of the specifications on
the Old-New list. This appears as ELSE on the Old-New list.</term>
<term termid="all_points_equal"><linktext>All points equal</linktext
>Moving averages are calculated with a span equal to the periodicity
and with all points weighted equally. This method is always used if
the periodicity is odd.</term>
<term termid="alpha"><linktext>Alpha</linktext>The significance level
used for rejecting the null hypothesis. The probability of rejecting
the null hypothesis when in fact the null hypothesis is true. Commonly
used alpha values are 0.01, 0.05, and 0.10.</term>
<term termid="alpha_1"><linktext>Alpha</linktext>A factor extraction
method that considers the variables in the analysis to be a sample
from the universe of potential variables. This method maximizes the
alpha reliability of the factors.</term>
<term termid="gexsmooth05"><linktext>Alpha (Exponential Smoothing)</linktext
>Exponential smoothing parameter that controls the relative weight
given to recent observations, as opposed to the overall series mean.
When alpha equals 1, the single most recent observation is used exclusively;
when alpha equals 0, old observations count just as heavily as recent
ones. Alpha is used for all models.</term>
<term termid="alpha_homogeneous_subsets"><linktext>Alpha (Homogeneous
Subsets)</linktext>Each column displays the subset of means that do
not differ from one another. The alpha value is displayed.</term>
<term termid="alpha_level_and_trend"><linktext>Alpha (Level and Trend)</linktext
>The single parameter for a Brown's linear trend model. It controls
the relative weight given to recent observations in estimating both
the series level and trend. It ranges from 0 to 1, with higher values
giving more weight to recent observations.</term>
<term termid="alphabetical"><linktext>Alphabetical</linktext>Sorts
the variables alphabetically by name or by variable label.</term>
<term termid="alphanumeric_variable"><linktext>Alphanumeric Variable</linktext
>A variable whose values are stored as characters and are not available
for arithmetic operations. The values can contain letters or special
characters as well as numbers. Also known as string variable.</term>
<term termid="amemiyas_prediction_criterion_pc"><linktext>Amemiya's
Prediction Criterion (PC)</linktext>A criterion for model selection.
It is akin to adjusted R-squared, with a higher penalty for adding
variables.</term>
<term termid="analysis"><linktext>Analysis</linktext>Lists the analysis
variables.</term>
<term termid="analysis_information"><linktext>Analysis Information</linktext
>These fields provide information useful to analysis of the sample.</term>
<term termid="analysis_n"><linktext>Analysis N</linktext>The number
of cases use for analysis</term>
<term termid="analysis_of_variance"><linktext>Analysis of Variance</linktext
>Analysis of variance, or ANOVA, is a method of testing the null hypothesis
that several group means are equal in the population, by comparing
the sample variance estimated from the group means to that estimated
within the groups.</term>
<term termid="analysis_of_variance_1"><linktext>Analysis of Variance</linktext
>Used to test the hypothesis that there is no linear relationship
between the dependent variable and the independent variable(s). The
total variation in the dependent variable is divided into two components--one
that can be attributed to a particular regression model (labeled Regression)
and one that cannot (labeled Residual). If the observed significance
level for the F-test is small, the hypothesis that there is no linear
relationship can be rejected.</term>
<term termid="ancillary_parameter"><linktext>Ancillary Parameter</linktext
>A fixed parameter (k) of the negative binomial distribution.  When
k=0, the distribution reduces to the Poisson distribution.  When k=1,
the distribution is equivalent to the Geometric distribution.</term>
<term termid="anderbergs_d_distance_and_proximities_measures"><linktext
>Anderberg's D (Distance and Proximities Measures)</linktext>Similarity
measure for binary data. Assesses the predictability of the state
(presence or absence) of a characteristic on one item, given the state
of the other. Measures the actual reduction in the error probability
when one item is used to predict the other. D ranges from 0 to 1.</term>
<term termid="anderson-rubin_method"><linktext>Anderson-Rubin Method</linktext
>A method of estimating factor score coefficients; a modification
of the Bartlett method which ensures orthogonality of the estimated
factors. The scores that are produced have a mean of 0, have a standard
deviation of 1, and are uncorrelated.</term>
<term termid="andrews_wave"><linktext>Andrews'  Wave</linktext>A type
of redescending M-estimator that does not have abrupt changes in the
weights assigned to cases. Instead, a smooth sine curve is used to
determine case weights. Standardized values in absolute value greater
than c are assigned a weight of 0.</term>
<term termid="annotation_charts"><linktext>Annotation (Charts)</linktext
>Specifies text labels placed on the chart. After specifying the text
and position for the annotation, click Add or Change to update the
annotations list.</term>
<term termid="anomaly_index"><linktext>Anomaly Index</linktext>The
anomaly index is a measure that reflects the unusualness of a case
with respect to its peer group.</term>
<term termid="anova_table"><linktext>ANOVA table</linktext>Displays
an analysis-of-variance table which includes univariate F tests for
each clustering variable. The F tests are only descriptive and the
resulting probabilities should not be interpreted. The ANOVA table
is not displayed if all cases are assigned to a single cluster.</term>
<term termid="anova_table_and_eta"><linktext>ANOVA Table and Eta</linktext
>Displays a one-way analysis-of-variance table and calculates eta
and eta-squared (measures of association) for each independent variable
in the first layer.</term>
<term termid="anti-image_matrices_factor_analysis"><linktext>Anti-Image
Matrices (Factor Analysis)</linktext>The anti-image correlation matrix
contains the negatives of the partial correlation coefficients, and
the anti-image covariance matrix contains the negatives of the partial
covariances. In a good factor model, most of the off-diagonal elements
will be small. The measure of sampling adequacy for a variable is
displayed on the diagonal of the anti-image correlation matrix.</term>
<term termid="any_test_value_value_..."><linktext>ANY(test,value[,value,...])</linktext
>ANY(test,value[,value,...]). Logical. Returns 1 or true if the value
of test matches any of the subsequent values; returns 0 or false otherwise.
This function requires two or more arguments. For example, ANY(var1,
1, 3, 5) returns 1 if the value of var1 is 1, 3, or 5 and 0 for other
 values. ANY can also be used to scan a list of variables or expressions
for a value. For example, ANY(1, var1, var2, var3) returns 1 if any
of the three specified variables has a value of 1 and 0 if all three
variables have  values other than 1.</term>
<term termid="apply_from_previous_model"><linktext>Apply from previous
model</linktext>The parameter estimates from the previous execution
of ARIMA (in the same session) are used as initial estimates. This
can save time if the data and model are similar to the last one used.</term>
<term termid="applymodel_function"><linktext>ApplyModel(handle,"function",category)</linktext
>ApplyModel(handle, "function", value). Numeric. Applies a particular
scoring function to the input case data using the model specified
by handle and where "function" is one of the following string literal
values enclosed in quotes: predict, stddev, probability, confidence,
nodeid, cumhazard, neighbor, distance. The model handle is the name
associated with the external XML file, as defined on the MODEL HANDLE
command. The optional third argument applies when the function is
"probability", "neighbor", or "distance". For "probability", it specifies
a category for which the probability is calculated. For "neighbor"
and "distance", it specifies a particular neighbor (as an integer)
for nearest neighbor models. ApplyModel returns system-missing if
a value can not be computed.</term>
<term termid="approximate_probability"><linktext>Approximate Probability</linktext
>The observed significance level has been estimated using an approximation.
That is, the significance level printed is not exact, but is based
on a formula that usually results in values close to the true ones.</term>
<term termid="approximate_t"><linktext>Approximate t</linktext>Transformation
of a statistic so that the t distribution can be used in significance
calculations.</term>
<term termid="approximately"><linktext>Approximately</linktext>Generates
a random sample of approximately the specified percentage of cases.
Since this routine makes an independent pseudorandom decision for
each case, the percentage of cases selected can only approximate the
specified percentage. The more cases there are in the data file, the
closer the percentage of cases selected is to the specified percentage.</term>
<term termid="ar1_diagonal"><linktext>AR1: diagonal</linktext>This
parameter defines the diagonal elements of a First-Order Autoregressive
covariance matrix.</term>
<term termid="ar1_rho"><linktext>AR1: rho</linktext>This parameter
defines the correlation between two successive levels of an effect
in a First-Order Autoregressive covariance matrix.</term>
<term termid="area_3"><linktext>Area</linktext>An estimate of the
true area under the ROC curve. It is the probability that when "positive"
and "negative" cases are randomly selected, the model assigns a higher
score to the positive case than to the negative.</term>
<term termid="area_below_the_curve"><linktext>Area Below the Curve</linktext
>Specifies whether the area below a curve is filled. If the plotted
variable has missing or negative values, the area is not filled.</term>
<term termid="area_to_the_left_of_the_curve"><linktext>Area to the
Left of the Curve</linktext>For plots where the independent variable
is along the vertical axis, this specifies whether the area to the
left of the curve is filled. If the plotted variable has missing or
negative values, the area is not filled.</term>
<term termid="arima"><linktext>ARIMA</linktext>AutoRegressive Integrated
Moving Average. A general model widely used in time series analysis.
Based upon prior investigation of the behavior of a series, you specify
three numbers that represent the order of autoregression (p), the
degree of differencing (d), and the order of moving average (q). The
general model is written as ARIMA(p,d,q). The model can be extended
to incorporate seasonality.</term>
<term termid="arma_1_1"><linktext>ARMA(1,1)</linktext>This is a first-order
autoregressive moving average structure.  It has homogenous variances.
 The correlation between two elements is equal to phi*rho for adjacent
elements, phi*(rho^2) for elements separated by a third, and so on.
 rho and phi are the autoregressive and moving average parameters,
respectively, and their values are constrained to lie between -1 and
1, inclusive.</term>
<term termid="arma11_diagonal"><linktext>ARMA11 diagonal</linktext
>A variance term for the ARMA(1,1) covariance structure.</term>
<term termid="arma11_phi"><linktext>ARMA11 phi</linktext>The moving
average parameter for the ARMA(1,1) covariance structure.</term>
<term termid="arma11_rho"><linktext>ARMA11 rho</linktext>The autoregressive
term for the ARMA(1,1) covariance structure.</term>
<term termid="arsin_numexpr"><linktext>ARSIN(numexpr)</linktext>ARSIN(numexpr).
Numeric. Returns the inverse sine (arcsine), in radians, of numexpr,
which must evaluate to a numeric value between -1 and +1.</term>
<term termid="artan_numexpr"><linktext>ARTAN(numexpr)</linktext>ARTAN(numexpr).
Numeric. Returns the inverse tangent (arctangent), in radians, of
numexpr, which must be numeric.</term>
<term termid="as_is_crosstabs_case_weights"><linktext>As is (Crosstabs
case weights)</linktext>Case weights are used as is and fractional
cell counts are used. However, when Exact Statistics (available only
with the Exact Tests option) are requested, the accumulated weights
in the cells are either truncated or rounded before computing the
Exact test statistics.</term>
<term termid="ascending"><linktext>Ascending</linktext>Displays row
variable values in ascending order from lowest to highest.</term>
<term termid="ascending_counts"><linktext>Ascending Counts</linktext
>Arranges the frequency table according to the frequency of occurrence
of the values. Frequencies are arranged in ascending order (least
frequent to most frequent).</term>
<term termid="ascending_means"><linktext>Ascending Means</linktext
>Sorts the variables in ascending order (smallest to largest) of the
Mean statistic.</term>
<term termid="ascending_values"><linktext>Ascending Values</linktext
>Arranges the frequency table according to the actual data values
in ascending order (smallest to largest value).</term>
<term termid="association_tree"><linktext>Association</linktext>Measures
the similarity between a surrogate predictor and the primary predictor
for a split at a node. Ranges in value from 0 to 1.</term>
<term termid="assumes_equal_variances"><linktext>Assumes Equal Variances</linktext
>Displays the results with the assumption that the population variances
are equal. If the population variances are unequal, these results
may not be valid.</term>
<term termid="assumptions"><linktext>Assumptions</linktext>Lists the
assumptions. The population variances are assumed to be equal or no
assumptions are made about the population variances.</term>
<term termid="assumptions_1"><linktext>Assumptions</linktext>Displays
statistics and tests under the assumptions of equal variances and
unequal variances.</term>
<term termid="asymptotic"><linktext>Asymptotic</linktext>These are
the "usual" parameter estimates, obtained via the sequential quadratic
programming algorithm or the Levenberg-Marquardt algorithm.</term>
<term termid="asymptotic_approximations"><linktext>Asymptotic Approximations</linktext
>Approximations based on large samples. Used when it is not possible
to obtain exact confidence intervals for each of the parameters.</term>
<term termid="asymptotic_confidence_interval"><linktext>Asymptotic
Confidence Interval</linktext>The confidence interval based on the
asymptotic distribution of a test statistic. A range of values that
has an N% chance of including the population value of the parameter
estimate. You can change the level of the confidence interval. Typical
confidence levels are 90, 95, and 99.</term>
<term termid="asymptotic_significance"><linktext>Asymptotic Significance</linktext
>The significance level based on the asymptotic distribution of a
test statistic. Typically, a value of less than 0.05 is considered
significant. The asymptotic significance is based on the assumption
that the data set is large. If the data set is small or poorly distributed,
this may not be a good indication of significance.</term>
<term termid="asymptotic_significance_1"><linktext>asymptotic significance</linktext
>There are some situations in which the distribution of a test statistic
is not well defined.  Often, however, as the number of observations
used to compute the statistic increases, its distribution begins to
approximate a known distribution.       This well-defined distribution
is then used to calculate the significance value of the test statistic.</term>
<term termid="asymptotic_standard_error"><linktext>Asymptotic Standard
Error</linktext>An asymptotic estimate of the standard error of a
statistic. This estimate approaches the population standard error
as the sample size increases. So it is a better estimate for large
data sets than it is for small ones.</term>
<term termid="at_least_one_missing_discriminating_variable"><linktext
>At Least One Missing Discriminating Variable</linktext>Indicates
the number of cases where at least one discriminating variable is
missing.</term>
<term termid="at_mean_of_covariates"><linktext>at mean of covariates</linktext
>The estimated functions evaluated at the mean of the covariates</term>
<term termid="autocorrelation_function_acf"><linktext>Autocorrelation
Function (ACF)</linktext>Correlations of a series with lagged values
of itself; also known as serial correlation. The autocorrelation function
is often represented as a plot. The ACF plot is a useful identification
and diagnostic aid.</term>
<term termid="gacf07"><linktext>Autocorrelations</linktext>Correlates
the values of a series with the values lagged by 1 or more cases.
Autocorrelations are calculated for lags of 1, 2, ..., up to a specified
number.</term>
<term termid="automatic_3"><linktext>Automatic</linktext>ARIMA chooses
initial values.</term>
<term termid="automatic_1"><linktext>Automatic</linktext>The program
calculates suitable starting and trend values from the data. This
is usually desirable.</term>
<term termid="autoregressive_orders_ar"><linktext>Autoregressive (AR)
Orders</linktext>Specifies that the model contains autoregressive
orders. The details for each lag (order) are provided.</term>
<term termid="autoregressive_model"><linktext>Autoregressive Model</linktext
>Model of a time series in which the current value of the series is
a linear combination of previous values of the series, plus a random
error.</term>
<term termid="garima01"><linktext>Autoregressive Order (ARIMA)</linktext
>The number of autoregressive parameters in the model. Specify a non-negative
integer. Each parameter measures the independent effect of values
with a specified lag. Thus, an autoregressive order of 2 means that
a series value is affected by the preceding two values (independently
of one another).</term>
<term termid="average_log-likelihood"><linktext>Average Log-Likelihood</linktext
>The (conditional) log-likelihood divided by the number of cases.</term>
<term termid="average_matrix"><linktext>Average Matrix</linktext>Transformation
coefficients matrix averaged over the measures.</term>
<term termid="average_measures"><linktext>Average Measures</linktext
>Average measure applies to the average of multiple measurements,
for example, the average rating for k judges, or the average score
for a k-item test.</term>
<term termid="averaged_variable"><linktext>Averaged Variable</linktext
>The transformed variable that is the average of dependent variables.</term>
<term termid="averate_matrix"><linktext>Averate Matrix</linktext>The
transformation matrix that corresponds to the transformation for the
between-subjects test. The dimension is the number of measures.</term>
<term termid="gaxis02"><linktext>Axis Scale</linktext>The numerical
values corresponding to positions on an axis, and typically shown
by the axis labels. On a linear scale the numerical values are proportional
to length along the axis; on a logarithmic scale the logarithms of
the values are proportional to length along the axis.</term>
<term termid="axis_title"><linktext>Axis Title</linktext>Text for
the axis title, up to 72 characters.</term>
<term termid="b"><linktext>B</linktext>Estimate of the change in the
dependent variable that can be attributed to a change of one unit
in the independent variable.</term>
<term termid="b_coefficient"><linktext>B Coefficient</linktext>Regression
coefficient for a factor specified as having a linear relationship
with the rankings or scores.</term>
<term termid="b_column"><linktext>B column</linktext>The set of coefficients
estimated for the model.</term>
<term termid="b0"><linktext>B0</linktext>The value of the parameter
associated with the intercept.</term>
<term termid="b1"><linktext>b1</linktext>A parameter in the model
equation.  Its interpretation will depend on the model.</term>
<term termid="backward_elimination"><linktext>Backward Elimination</linktext
>A variable selection procedure in which all variables are entered
into the equation and then sequentially removed. The variable with
the smallest partial correlation with the dependent variable is considered
first for removal. If it meets the criterion for elimination, it is
removed. After the first variable is removed, the variable remaining
in the equation with the smallest partial correlation is considered
next. The procedure stops when there are no variables in the equation
that satisfy the removal criteria.</term>
<term termid="backward_elimination_conditional"><linktext>Backward
Elimination (Conditional)</linktext>Backward stepwise selection. Removal
testing is based on the probability of the likelihood-ratio statistic
based on conditional parameter estimates.</term>
<term termid="backward_elimination_likelihood_ratio"><linktext>Backward
Elimination (Likelihood Ratio)</linktext>Backward stepwise selection.
Removal testing is based on the probability of the likelihood-ratio
statistic based on the maximum partial likelihood estimates.</term>
<term termid="backward_elimination_wald"><linktext>Backward Elimination
(Wald)</linktext>Backward stepwise selection. Removal testing is based
on the probability of the Wald statistic.</term>
<term termid="badness_of_fit"><linktext>Badness of Fit</linktext>Lower
values of badness-of-fit statistics indicate better models.</term>
<term termid="bar_chart_s"><linktext>Bar Chart(s)</linktext>Creates
a bar chart that displays the frequency count for each value as a
separate bar. Bar charts are appropriate for categorical variables
(variables with a limited number of distinct categories). Values for
which the count is zero do not appear in a bar chart.</term>
<term termid="bartlett_scores_factor_analysis"><linktext>Bartlett
Scores (Factor Analysis)</linktext>A method of estimating factor score
coefficients. The scores that are produced have a mean of 0. The sum
of squares of the unique factors over the range of variables is minimized.</term>
<term termid="gspectra06"><linktext>Bartlett Window</linktext>The
shape of a spectral window for which the weights of the upper half
of the window are computed as Wk = Fp (2*pi*fk), for k = 0, ... p,
where p is the integer part of half the span and Fp is the Fejer kernel
of order p. The lower half is symmetric with the upper half.</term>
<term termid="bartlett-box_f"><linktext>Bartlett-Box F</linktext>Test
that several groups come from populations with the same variance.
For sufficiently large sample sizes, a nonsignificant p value means
there is insufficient evidence that the variances differ. The test
depends heavily on the observations being from normal distributions.</term>
<term termid="gacf06"><linktext>Bartlett's Approximation (Autocorrelations)</linktext
>Calculates standard error with an approximation that is appropriate
when the series represents a moving average process of order k-1.
With this method standard errors grow at increasing lags.</term>
<term termid="bartletts_test_of_sphericity"><linktext>Bartlett's Test
of Sphericity</linktext>Tests the null hypothesis that the correlation
matrix is an identity matrix. The data must be a sample from a multivariate
normal population. If the null hypothesis cannot be rejected, and
the sample size is reasonably large, you should reconsider the use
of multivariate analysis, since the dependent variables are not correlated.</term>
<term termid="based_on_time_or_case_range"><linktext>Based on Time
or Case Range</linktext>Selects cases based on a range of case numbers
or a range of dates/times.</term>
<term termid="baseline_cumulative_hazard"><linktext>Baseline cumulative
hazard</linktext>The value of the cumulative hazard function at the
specified time point independent of the effects of predictors.</term>
<term termid="baseline_strata"><linktext>Baseline strata</linktext
>A separate baseline hazard and survival function is computed for
each value of this variable, while a single set of model coefficients
is estimated across strata.</term>
<term termid="bayesian_information_criterion_bic"><linktext>Bayesian
Information Criterion (BIC)</linktext>A measure for selecting and
comparing models based on the -2 log likelihood. Smaller values indicate
better models.  The BIC also penalizes overparametrized models, but
more strictly than the AIC.</term>
<term termid="bca_confidence_interval"><linktext>BCa Confidence Interval</linktext
>A bias-corrected and adjusted interval that is considered more accurate
but more computationally expensive than the usual Percentile intervals
computed by bootstrapping.</term>
<term termid="before_effects"><linktext>Before Effects</linktext>Enters
the covariates before the factor effects are evaluated. Available
only one or more covariates are in the model and the Hierarchical
or Experimental method is used.</term>
<term termid="before_transformation_1"><linktext>Before Transformation</linktext
>Statistics in this category apply to the original variables.</term>
<term termid="bernoulli"><linktext>Bernoulli</linktext>The Bernoulli
distribution takes values of 0 and 1.  A Bernoulli variate takes the
value of 1 with probability equal to the specified probability parameter.</term>
<term termid="beta_coefficients"><linktext>Beta Coefficients</linktext
>Beta coefficients, sometimes called standardized regression coefficients,
are the regression coefficients when all variables are expressed in
standardized (z-score) form. Transforming the independent variables
to standardized form makes the coefficients more comparable since
they are all in the same units of measure.</term>
<term termid="beta_in"><linktext>Beta In</linktext>The standardized
regression coefficient that would result if the variable were entered
into the equation at the next step.</term>
<term termid="between_groups"><linktext>Between Groups</linktext>The
Mahalanobis distance is calculated for the distance between these
two groups.</term>
<term termid="between_items"><linktext>Between Items</linktext>The
part of the within-people variability that can be accounted for by
differences between items.</term>
<term termid="between_people"><linktext>Between People</linktext>The
part of total variability in the scale that can be accounted for by
differences in people.</term>
<term termid="between-groups_variability"><linktext>Between-Groups
Variability</linktext>The part of total variability in the dependent
variable that can be accounted for by differences in group means.</term>
<term termid="between-subject_variable"><linktext>Between-Subject
Variable</linktext>Between-subjects factors divide the sample into
discrete subgroups. Each subject has only one value for a between-subjects
variable.</term>
<term termid="bias_bootstrap"><linktext>Bias</linktext>The difference
between the value of the statistic computed using the bootstrap samples
and the value computed using the original sample.</term>
<term termid="bic"><linktext>BIC</linktext>A measure for selecting
and comparing mixed models based on the -2 (Restricted) log likelihood.
Smaller values indicate better models.  The BIC also penalizes overparametrized
models, but more strictly than the AIC.</term>
<term termid="bic_change"><linktext>BIC change</linktext>The difference
in Schwarz's Bayesian Criterion between the given model and the next
smaller model.  Thus, the BIC change for 2 clusters is the difference
in the BIC between the model with 2 clusters and the model with 1
cluster.</term>
<term termid="bic_rbf"><linktext>BIC (RBF)</linktext>The Bayesian
information criterion (BIC) can be used by the procedure to determine
the "best" number of hidden units.  The number of units that results
in the smallest BIC is considered "best".</term>
<term termid="bic_reduced"><linktext>BIC reduced</linktext>This is
the BIC for the model when the effect is removed from the fitted model.</term>
<term termid="bimodal"><linktext>bimodal</linktext>If two of a variable's
values share the greatest frequency of occurrence, each of them is
a mode.  The variable is then said to be bimodal.</term>
<term termid="bin"><linktext>bin</linktext>The number assigned to
each bin.</term>
<term termid="binomial_test"><linktext>Binomial Test</linktext>A test
of whether a sample comes from a binomial distribution with the specified
probability of success (p).</term>
<term termid="bivariate_analysis_-_first_plot_with_each"><linktext
>Bivariate analysis - first plot with each</linktext>Plots the first
variable selected with each of the other variables on the list. Univariate
plots are still produced for each variable.</term>
<term termid="block"><linktext>Block</linktext>The model building
block for which statistics are shown</term>
<term termid="blom_rank_cases"><linktext>Blom (Rank Cases)</linktext
>Creates new ranking variable based on proportion estimates that uses
the formula (r-3/8) / (w+1/4), where w is the sum of the case weights
and r is the rank.</term>
<term termid="bloms_probability_plots"><linktext>Blom's (Probability
Plots)</linktext>Calculates the expected normal distribution using
the formula (r-3/8) / (n+1/4), where n is the number of observations
and r is the rank, ranging from 1 to n.</term>
<term termid="bonferroni_1"><linktext>Bonferroni</linktext>This method
adjusts the observed significance level for the fact that multiple
contrasts are being tested.</term>
<term termid="bonferroni"><linktext>Bonferroni</linktext>Uses t tests
to perform pairwise comparisons between group means, but controls
overall error rate by setting the error rate for each test to the
experimentwise error rate divided by the total number of tests. Hence,
the observed significance level is adjusted for the fact that multiple
comparisons are being made.</term>
<term termid="bootstrap_1"><linktext>Bootstrap</linktext>Statistics
computed using the bootstrap samples.</term>
<term termid="bootstrap"><linktext>Bootstrap</linktext>Bootstrap estimates
of the standard error are obtained using repeated samples from the
original data set. This is done by sampling with replacement to get
samples of the same size as the original data set.</term>
<term termid="bootstrap_estimates"><linktext>Bootstrap Estimates</linktext
>A method of estimating the standard error of a statistic using repeated
samples from the original data set. This is done by sampling (with
replacement) to get many samples of the same size as the original
data set. The nonlinear equation is estimated for each of these samples.
The standard error of each parameter estimate is then calculated as
the standard deviation of the bootstrapped estimates. Parameter values
from the original data are used as starting values for each bootstrap
sample.  This requires the sequential quadratic programming algorithm.</term>
<term termid="both_missing"><linktext>Both Missing</linktext>Indicates
the number of cases where both of the group codes are missing or out-of-range
and at least one discriminating variable is missing.</term>
<term termid="both_parts"><linktext>Both Parts</linktext>Statistics
are computed for all items.</term>
<term termid="both_radio_button"><linktext>Both Radio Button</linktext
>Displays both plots and statistics. Box plots, stem-and-leaf plots,
and basic descriptive statistics for each variable are displayed by
default.</term>
<term termid="box_plots"><linktext>Box plots</linktext>Summary plot
based on the median, quartiles, and extreme values. The box represents
the interquartile range which contains the 50% of values. The whiskers
are lines that extend from the box to the highest and lowest values,
excluding outliers. A line across the box indicates the median.</term>
<term termid="box-ljung_statistic"><linktext>box-ljung statistic</linktext
>Used to test the null hypothesis that observed autocorrelations are
from a series with no autocorrelation at any lag. Also known as the
modified Box-Pierce statistic</term>
<term termid="boxplot"><linktext>boxplot</linktext>Boxplots characterize
the distribution of a variable, displaying its median and quartiles.
 Special symbols identify the position of outliers, if any.</term>
<term termid="boxs_m_test"><linktext>Box's M test</linktext>A test
for the equality of the group covariance matrices. For sufficiently
large samples, a nonsignificant p value means there is insufficient
evidence that the matrices differ. The test is sensitive to departures
from multivariate normality.</term>
<term termid="bradley-terry-luce_btl"><linktext>Bradley-Terry-Luce
(BTL) Model</linktext>Determines the probability as the ratio of a
profile's utility to that for all simulation profiles, averaged across
all respondents.</term>
<term termid="gbrkgrp"><linktext>Break Group</linktext>A break group
consists of all of the cases that have identical values for the break
variable(s). In summary reports, statistics are displayed for every
such group.</term>
<term termid="break_variables"><linktext>Break Variables</linktext
>Variables used to define aggregate groups. Cases are grouped together
based on the values of the break variables. Each unique combination
of break variable values defines a group and generates one case in
the new aggregated file.</term>
<term termid="breslow"><linktext>Breslow</linktext>A test for comparing
the equality of survival distributions. Time points are weighted by
the number of cases at risk at each time point.</term>
<term termid="breslow-day"><linktext>Breslow-Day</linktext>Breslow
Day's statistic tests for homogeneity of the common odds ratio.</term>
<term termid="brown-forsythe"><linktext>Brown-Forsythe</linktext>The
Brown-Forsythe statistic tests for the equality of group means.  This
statistic is preferable to the F statistic when the assumption of
equal variances does not hold.</term>
<term termid="browns_linear_trend"><linktext>Brown's linear trend</linktext
>This model is appropriate for series in which there is a linear trend
and no seasonality. Its smoothing parameters are level and trend,
which are assumed to be equal. Brown's model is therefore a special
case of Holt's model. Brown's exponential smoothing is most similar
to an ARIMA model with zero orders of autoregression, two orders of
differencing, and two orders of moving average, with the coefficient
for the second order of moving average equal to the square of one-half
of the coefficient for the first order.</term>
<term termid="browse"><linktext>Browse</linktext>Opens a dialog box
for selecting files.</term>
<term termid="by_frequency"><linktext>By frequency</linktext>All plots
are produced by frequency, ranging from frequency 0 (the constant
or mean term) to frequency 0.5 (the term for a cycle of two observations).</term>
<term termid="by_period"><linktext>By period</linktext>All plots are
produced by period, ranging from 2 (the term for a cycle of two observations)
to a period equal to the number of observations (the constant or mean
term). Period is displayed on a logarithmic scale.</term>
<term termid="gcontrol04"><linktext>c and u Charts</linktext>Plots
the number or ratio of nonconformities. C charts plot the total number
of nonconformities per subgroup, for a constant sample of items. U
charts plot the average nonconformities per unit within a subgroup,
for varying samples of units. Use c or u charts rather than p or np
charts when each unit can have multiple nonconformities.</term>
<term termid="c_coefficient"><linktext>C Coefficient</linktext>Regression
coefficient for a factor specified as having a quadratic relationship
with the rankings or scores.</term>
<term termid="cache_data_locally"><linktext>Cache data locally</linktext
>A data cache is a complete copy of the data file, stored in temporary
disk space. Caching the data file can improve performance.</term>
<term termid="caic"><linktext>CAIC</linktext>A measure for selecting
and comparing mixed models based on the -2 (Restricted) log likelihood.
Smaller values indicate better models. The CAIC "penalizes" overparametrized
models more strictly than the AIC.  As the sample size increases,
the CAIC converges to the BIC.</term>
<term termid="calculate_from_data"><linktext>Calculate from Data</linktext
>Estimate the natural response rate from the sample data. Your data
should contain a case representing the control level, for which the
value of the covariate(s) is 0. Probit estimates the natural response
rate using the proportion of responses for the control level as an
initial value.</term>
<term termid="cancel"><linktext>Cancel</linktext>Cancels any changes
made in the dialog box since the last time it was opened, and closes
the dialog box without executing any command.</term>
<term termid="canonical"><linktext>Canonical</linktext>For each dimension,
the rows are the weighted average of the columns divided by the matching
singular value. Use this method if you want to examine the differences
or similarities between the two variables.</term>
<term termid="canonical_correlation"><linktext>Canonical Correlation</linktext
>The canonical correlation for a discriminant function is the square
root of the ratio of the between-groups sum of squares to the total
sum of squares. Squared, it is the proportion of the total variability
explained by differences between groups.</term>
<term termid="capability_indices"><linktext>Capability Indices</linktext
>Measures of the capability of the process.  Most of these statistics
are based upon the capability sigma and/or specification limits.</term>
<term termid="capability_sigma"><linktext>Capability Sigma</linktext
>The capability sigma.</term>
<term termid="card_number"><linktext>Card Number</linktext>Identifier
used to number the simulation cases from the plan file. The first
simulation case in the plan file is card 1, the second simulation
case is card 2, etc.</term>
<term termid="carryover_effect"><linktext>carryover effect</linktext
>In a crossover trial, there is a possibility of treatments having
an (undesired) effect in the period following the one in which they
are administered.  One way to deal with carryover effects is to allow
sufficient time between periods, or washout, for the carryover effect
to become negligible.  If, however, this is impractical or unethical,
you can add the carryover effect to the model.</term>
<term termid="case"><linktext>Case</linktext>Indicates the case described</term>
<term termid="case_labels"><linktext>Case Labels</linktext>Designates
a variable to identify points on plots. For each point in a scatterplot
you can use the point selection tool to display the value of the selected
case label variable.</term>
<term termid="case_number"><linktext>Case Number</linktext>Identifies
the number of the case in the data file.</term>
<term termid="case_number_of_nonmissing_values"><linktext>Case Number
of Non-Missing Values</linktext>Case (row) number for first and last
nonmissing value for the new variable. If original variable values
at the beginning or end of the file have missing values, then those
values may also be missing for the new variable, depending on the
method used.</term>
<term termid="case_percentage"><linktext>Case Percentage</linktext
>The cell count of cases as a percentage of the total.</term>
<term termid="case_type_1"><linktext>Case type</linktext>Dimension
containing case type.</term>
<term termid="case_eights"><linktext>Case Weights</linktext>The value
of the weight variable for the specified case.</term>
<term termid="cases_18"><linktext>Cases</linktext>The number of cases,
or rows, in the data file.</term>
<term termid="cases_3"><linktext>Cases</linktext>Produces a casewise
cluster analysis. Distances are computed between cases.</term>
<term termid="cases_1"><linktext>Cases</linktext>Codes for actual
group, predicted group, posterior probabilities, and discriminant
scores are displayed for each case.</term>
<term termid="cases"><linktext>Cases</linktext>Basic units of analysis
for which measurements are taken. For example, each survey respondent
is a case. Cases correspond to records in a database.</term>
<term termid="cases_19"><linktext>Cases</linktext>Rows or observations
in the data file.</term>
<term termid="cases_constant_gt_bpct"><linktext>Cases Constant &gt;
b%</linktext>The percentage of cases with the same value exceeds a
chosen threshold for these variables.</term>
<term termid="cases_dropped"><linktext>Cases dropped</linktext>Number
of cases dropped from the analysis due to missing values, non-positive
time, or censored cases before the earliest event</term>
<term termid="cases_greater_than_or_equal_to_test_value"><linktext
>Cases Greater than or Equal to Test Value</linktext>The number of
cases with values greater than or equal to the test value. Cases with
values smaller than the test value fall into one group, cases with
values greater than or equal to the test value fall into the other
group.</term>
<term termid="cases_less_than_test_value"><linktext>Cases Less than
Test Value</linktext>The number of cases with values less than the
test value. Cases with values smaller than the test value fall into
one group, cases with values greater than or equal to the test value
fall into the other group.</term>
<term termid="cases_missing"><linktext>Cases missing</linktext>The
number of cases/objects that have values outside the valid range for
one or more variables. The valid range for a variable is the range
1 through the specified number of categories.</term>
<term termid="cases_missing_gt_apct"><linktext>Cases Missing &gt;
a%</linktext>The percentage of cases with missing values exceeds a
chosen threshold for these variables.</term>
<term termid="cases_used_1"><linktext>Cases Used</linktext>Displays
the number of cases used in the analysis.</term>
<term termid="cases_used"><linktext>Cases Used</linktext>Reports what
cases (observations, records) are used in the analysis.</term>
<term termid="cases_used_in_analysis"><linktext>Cases Used in Analysis</linktext
>The number of nonmissing cases, or objects.</term>
<term termid="cases_with_duplicate_identifiers"><linktext>Cases with
Duplicate Identifiers</linktext>The list of cases with duplicate identifiers.</term>
<term termid="cases_with_missing_values_2"><linktext>Cases with missing
values</linktext>Number of cases with missing values. These cases
are excluded from the analysis.</term>
<term termid="cases_with_non-positive_time"><linktext>Cases with non-positive
time</linktext>Number of cases with a non-positive value for time.
These cases are excluded from the analysis</term>
<term termid="casewise_diagnostics"><linktext>Casewise Diagnostics</linktext
>Produces casewise diagnostics for the cases meeting the selection
criterion (outliers outside n standard deviations).</term>
<term termid="casewise_diagnostics_1"><linktext>Casewise Diagnostics</linktext
>Table displays information about cases that fall outside a specified
number of standard deviations.</term>
<term termid="categorical_6"><linktext>categorical</linktext>A variable
with a discrete number of values; an ordinal or nominal variable.
 Categorical variables are often used as grouping variables or factors.</term>
<term termid="gmeans03"><linktext>Categorical Variables</linktext
>Variables that have a limited number of distinct values or categories.</term>
<term termid="categorical_variables_1"><linktext>Categorical Variables</linktext
>Variables that take a finite number of values.</term>
<term termid="categories"><linktext>Categories</linktext>Elementary
variables having more than two categories. After selecting this option
to create a multiple category set having the same range as the component
variable, enter an integer value for the minimum and maximum values
of the range for categories of the multiple category set.</term>
<term termid="categories_containing_one_case_gt_cpct"><linktext>Categories
Containing One Case &gt; c%</linktext>The percentage of categories
represented by a single case exceeds a chosen threshold for these
variables.</term>
<term termid="gseries02"><linktext>Categories for Bar Charts</linktext
>Categories are the individual values of a variable represented on
the category axis of a chart. For example, in a simple bar chart there
is one bar for each category on the category axis. In a clustered
bar chart, there is one cluster of bars for each category.</term>
<term termid="categories_for_dimensions"><linktext>Categories for
Dimensions</linktext>Lists all categories in the selected dimension
in the layer.</term>
<term termid="category_16"><linktext>Category</linktext>Lists the
different categories.</term>
<term termid="category"><linktext>Category</linktext>Lists types of
formats available.</term>
<term termid="category_after_discretization"><linktext>Category after
Discretization</linktext>Several categories will often be given a
single category value for the purposes of the analysis.</term>
<term termid="gcategory1"><linktext>Category Axis</linktext>A category
axis on a chart is an axis that displays values individually, without
necessarily arranging them to scale. (A scale axis, in contrast, displays
numerical values to scale.) Bar charts, line charts, and area charts
usually have one category axis and at least one scale axis. Scatterplots
and histograms do not have a category axis.</term>
<term termid="category_centroids_1"><linktext>Category Centroids</linktext
>Or "actual" centroid values.  By looking at the centroid plots, you
can determine whether ordinal or numerical quantifications are appropriate.</term>
<term termid="category_quantifications_1"><linktext>Category quantifications</linktext
>Optimal scale values assigned to the categories of a variable.</term>
<term termid="cdf.bernoulli_quant_prob"><linktext>CDF.BERNOULLI(quant,
prob)</linktext>CDF.BERNOULLI(quant, prob). Numeric. Returns the cumulative
probability that a value from the Bernoulli distribution, with the
given probability parameter, will be less than or equal to quant.</term>
<term termid="cdf.beta_quant_shape1_shape2"><linktext>CDF.BETA(quant,
shape1, shape2)</linktext>CDF.BETA(quant, shape1, shape2). Numeric.
Returns the cumulative probability that a value from the Beta distribution,
with the given shape parameters, will be less than quant.</term>
<term termid="cdf.binom_quant_n_prob"><linktext>CDF.BINOM(quant, n,
prob)</linktext>CDF.BINOM(quant, n, prob). Numeric. Returns the cumulative
probability that the number of successes in n trials, with probability
prob of success in each, will be less than or equal to quant. When
n is 1, this is the same as CDF.BERNOULLI.</term>
<term termid="cdf.bvnor"><linktext>CDF.BVNOR</linktext>CDF.BVNOR(quant1,
quant2, corr). Numeric. Returns the cumulative probability that a
value from the standard bivariate normal distribution, with the given
correlation parameter, will be less than quant1 and quant2.</term>
<term termid="cdf.cauchy_quant_loc_scale"><linktext>CDF.CAUCHY(quant,
loc, scale)</linktext>CDF.CAUCHY(quant, loc, scale). Numeric. Returns
the cumulative probability that a value from the Cauchy distribution,
with the given location and scale parameters, will be less than quant.</term>
<term termid="cdf.chisq_quant_df"><linktext>CDF.CHISQ(quant, df)</linktext
>CDF.CHISQ(quant, df). Numeric. Returns the cumulative probability
that a value from the chi-square distribution, with df degrees of
freedom, will be less than quant.</term>
<term termid="cdf.exp_quant_scale"><linktext>CDF.EXP(quant, scale)</linktext
>CDF.EXP(quant, scale). Numeric. Returns the cumulative probability
that a value from the exponential distribution, with the given scale
parameter, will be less than quant.</term>
<term termid="cdf.f_quant_df1_df2"><linktext>CDF.F(quant, df1, df2)</linktext
>CDF.F(quant, df1, df2). Numeric. Returns the cumulative probability
that a value from the F distribution, with degrees of freedom df1
and df2, will be less than quant.</term>
<term termid="cdf.gamma_quant_shape_scale"><linktext>CDF.GAMMA(quant,
shape, scale)</linktext>CDF.GAMMA(quant, shape, scale). Numeric. Returns
the cumulative probability that a value from the Gamma distribution,
with the given shape and scale parameters, will be less than quant.</term>
<term termid="cdf.geom_quant_prob"><linktext>CDF.GEOM(quant, prob)</linktext
>CDF.GEOM(quant, prob). Numeric. Returns the cumulative probability
that the number of trials to obtain a success, when the probability
of success is given by prob, will be less than or equal to quant.</term>
<term termid="cdf.halfnrm"><linktext>CDF.HALFNRM</linktext>CDF.HALFNRM(quant,
mean, stddev). Numeric. Returns the cumulative probability that a
value from the half normal distribution, with specified mean and standard
deviation, will be less than quant.</term>
<term termid="cdf.hyper_quant_total_sample_hits"><linktext>CDF.HYPER(quant,
total, sample, hits)</linktext>CDF.HYPER(quant, total, sample, hits).
Numeric. Returns the cumulative probability that the number of objects
with a specified characteristic, when sample objects are randomly
selected from a universe of size total in which hits have the specified
characteristic, will be less than or equal to quant.</term>
<term termid="cdf.igauss"><linktext>CDF.IGAUSS</linktext>CDF.IGAUSS(quant,
loc, scale). Numeric. Returns the cumulative probability that a value
from the inverse Gaussian distribution, with the given location and
scale parameters, will be less than quant.</term>
<term termid="cdf.laplace_quant_mean_scale"><linktext>CDF.LAPLACE(quant,
mean, scale)</linktext>CDF.LAPLACE(quant, mean, scale). Numeric. Returns
the cumulative probability that a value from the Laplace distribution,
with the specified mean and scale parameters, will be less than quant.</term>
<term termid="cdf.lnormal_quant_a_b"><linktext>CDF.LNORMAL(quant,
a, b)</linktext>CDF.LNORMAL(quant, a, b). Numeric. Returns the cumulative
probability that a value from the log-normal distribution, with the
specified parameters, will be less than quant.</term>
<term termid="cdf.logistic_quant_mean_scale"><linktext>CDF.LOGISTIC(quant,
mean, scale)</linktext>CDF.LOGISTIC(quant, mean, scale). Numeric.
Returns the cumulative probability that a value from the logistic
distribution, with the specified mean and scale parameters, will be
less than quant.</term>
<term termid="cdf.negbin_quant_thresh_prob"><linktext>CDF.NEGBIN(quant,
thresh, prob)</linktext>CDF.NEGBIN(quant, thresh, prob). Numeric.
Returns the cumulative probability that the number of trials to obtain
a success, when the threshold parameter is thresh and the probability
of success is given by prob, will be less than or equal to quant.</term>
<term termid="cdf.normal_quant_mean_stddev"><linktext>CDF.NORMAL(quant,
mean, stddev)</linktext>CDF.NORMAL(quant, mean, stddev). Numeric.
Returns the cumulative probability that a value from the normal distribution,
with specified mean and standard deviation, will be less than quant.</term>
<term termid="cdf.pareto_quant_threshold_shape"><linktext>CDF.PARETO(quant,
threshold, shape)</linktext>CDF.PARETO(quant, threshold, shape). Numeric.
Returns the cumulative probability that a value from the Pareto distribution,
with the specified threshold and shape parameters, will be less than
quant.</term>
<term termid="cdf.poisson_quant_mean"><linktext>CDF.POISSON(quant,
mean)</linktext>CDF.POISSON(quant, mean). Numeric. Returns the cumulative
probability that a value from the Poisson distribution, with the specified
mean or rate parameter, will be less than or equal to quant.</term>
<term termid="cdf.smod"><linktext>CDF.SMOD</linktext>CDF.SMOD(quant,
a, b). Numeric.  Returns the cumulative probability that a value from
the Studentized maximum modulus, with the specified parameters, will
be less than quant.</term>
<term termid="cdf.srange"><linktext>CDF.SRANGE</linktext>CDF.SRANGE(quant,
a, b). Numeric. Returns the cumulative probability that a value from
the Studentized range statistic, with the specified parameters, will
be less than quant.</term>
<term termid="cdf.t_quant_df"><linktext>CDF.T(quant, df)</linktext
>CDF.T(quant, df). Numeric. Returns the cumulative probability that
a value from Student's t distribution, with the specified degrees
of freedom df, will be less than quant.</term>
<term termid="cdf.uniform_quant_min_max"><linktext>CDF.UNIFORM(quant,
min, max)</linktext>CDF.UNIFORM(quant, min, max). Numeric. Returns
the cumulative probability that a value from the uniform distribution,
with the specified minimum and maximum, will be less than quant.</term>
<term termid="cdf.weibull_quant_a_b"><linktext>CDF.WEIBULL(quant,
a, b)</linktext>CDF.WEIBULL(quant, a, b). Numeric. Returns the cumulative
probability that a value from the Weibull distribution, with the specified
parameters, will be less than quant.</term>
<term termid="cdfnorm_zvalue"><linktext>CDFNORM(zvalue)</linktext
>CDFNORM(zvalue). Numeric. Returns the probability that a random variable
with mean 0 and standard deviation 1 would be less than zvalue, which
must be numeric.</term>
<term termid="cell"><linktext>cell</linktext>A cell is the cross-classification
of levels from one or more factors.  For example, if you have customer
factors for geographic region, marital status, and educational level,
then married college graduates in your northern sales territory constitute
a cell.</term>
<term termid="ganova01"><linktext>Cell (ANOVA)</linktext>In analysis
of variance, a cell is formed by every distinct combination of factor
values. All of the cases in each cell have the same values for each
factor in the ANOVA. In a three-factor analysis where one factor has
two categories, the second has three, and the third has five, there
are 2x3x5=30 cells.</term>
<term termid="cell_structure"><linktext>Cell Structure</linktext>The
value of the cell structure variable.</term>
<term termid="cells"><linktext>Cells</linktext>Cells defined by the
cross-classification of factors.</term>
<term termid="cells_pushbutton"><linktext>Cells Pushbutton</linktext
>Controls the statistics displayed in the cells of the table. You
can display row, column, and total percentages, expected values, and
residuals.</term>
<term termid="censored"><linktext>Censored</linktext>Subjects that
are withdrawn from observation before experiencing the terminal event.</term>
<term termid="censored_cases"><linktext>Censored cases</linktext>Number
of cases with censored time values, where tracking on the case stopped
before the status event occurred</term>
<term termid="censored_cases_before_the_eariliest_event_in_a_stratum"
><linktext>Censored cases before the earliest event in a stratum</linktext
>Number of cases which were followed for less time than the shortest
time to event in their stratum. These cases are excluded from the
analysis.</term>
<term termid="censored_percent"><linktext>Censored Percent</linktext
>The percentage of subjects which are censored within a stratum.</term>
<term termid="gcontrol05"><linktext>Center Line (Control Charts)</linktext
>A line that represents the average value of the quality characteristic.
This line serves as a baseline against which to examine the process
under consideration.</term>
<term termid="center_variables"><linktext>Center variables</linktext
>Adjusts the series to have a mean of 0 before calculating the spectrum
and to remove the large term that may be associated with the series
mean.</term>
<term termid="centering"><linktext>centering</linktext>A method of
removing the effect of a variable's location from the analysis. This
is accomplished by subtracting its location (most often the mean,
but sometimes the median or some other measure of central tendency)
from each of the variable's values.</term>
<term termid="central_tendency"><linktext>central tendency</linktext
>An attribute of a distribution concerning where the values of the
distribution tend to "congregate".  Measures of central tendency include
the mean, median, and mode.</term>
<term termid="centroid"><linktext>Centroid</linktext>The average variable
values for the cases within a cluster.</term>
<term termid="centroid_coordinates"><linktext>Centroid Coordinates</linktext
>The loss, or variance not accounted for by the centroid coordinates.</term>
<term termid="centroids"><linktext>Centroids</linktext>Category quantifications,
and the projected and the actual averages of the object scores for
the objects (cases) included in each set for those belonging to the
same category of the variable.</term>
<term termid="cfvar_numexpr_numexpr_.."><linktext>CFVAR(numexpr,numexpr[,..])</linktext
>CFVAR(numexpr,numexpr[,...]). Numeric. Returns the coefficient of
variation (the standard deviation divided by the mean) of its arguments
that have valid values. This function requires two or more arguments,
which must be numeric. You can specify a minimum number of valid arguments
for this function to be evaluated.</term>
<term termid="chaid"><linktext>CHAID</linktext>Chi-squared Automatic
Interaction Detection.  At each step, CHAID chooses the independent
(predictor) variable  that has the strongest interaction with the
dependent variable. Categories of each predictor are merged if they
are not significantly different with respect to the dependent variable.</term>
<term termid="change"><linktext>Change</linktext>Modifies a value
in the list.</term>
<term termid="change_from_previous_block"><linktext>Change from Previous
Block</linktext>The change in model fit in the current block relative
to the previous block. Used to evaluate whether the current block
improves the model fit sufficiently to justify the added complexity.</term>
<term termid="change_from_previous_step"><linktext>Change from Previous
Step</linktext>The change in model fit in the current step relative
to the previous step. Used to evaluate whether the current step improves
the model fit sufficiently to justify the added complexity.</term>
<term termid="change_in_cluster_centers"><linktext>Change in cluster
centers</linktext>Indicates the change in the cluster centers at each
iteration</term>
<term termid="change_in_v"><linktext>Change in V</linktext>When an
additional variable is entered, this value is the change in V. When
positive this change has approximately a chi-square distribution.</term>
<term termid="change_statistics"><linktext>Change Statistics</linktext
>Change statistics include R square change, F change, significance
F change.</term>
<term termid="def_chart"><linktext>Chart (output item type)</linktext
>A chart created either through the Graphics menu or in some statistical
procedures. The user can cut, copy, paste or delete a chart item or
change its label text when it is selected. To export a chart, it is
necessary to activate it.</term>
<term termid="charts"><linktext>Charts</linktext>Requests bar charts
and histograms.</term>
<term termid="char.index"><linktext>CHAR.INDEX(haystack, needle, divisor)</linktext
>CHAR.INDEX(haystack, needle[, divisor]).  Numeric. Returns a number
indicating the character position of the first occurrence of needle
in haystack.  The optional third argument, divisor, is a number of
characters used to divide needle into separate strings. Each substring
is used for searching and the function returns the first occurrence
of any of the substrings. For example,  CHAR.INDEX(var1, 'abcd') will
return the value of the starting position of the complete string "abcd"
in the string variable var1; CHAR.INDEX(var1, 'abcd', 1) will return
the value of the position of the first occurrence of any of the values
in the string; and  CHAR.INDEX(var1, 'abcd', 2) will return the value
of the first occurrence of either "ab" or "cd". Divisor must be a
positive integer and must divide evenly into the length of needle.
Returns 0 if needle does not occur within haystack.</term>
<term termid="char.length"> <linktext>CHAR.LENGTH(strexpr)</linktext
>CHAR.LENGTH(strexpr). Numeric. Returns the length of strexpr in characters,
with any trailing blanks removed. </term>
<term termid="char.lpad"><linktext>CHAR.LPAD(strexpr1, length, strexpr2)</linktext
>CHAR.LPAD(strexpr1,length[,strexpr2]). String. Left-pads strexpr1
to make its length the value specified by length using as many complete
copies as will fit of strexpr2 as the padding string. The value of
length represents the number of characters and must be a positive
integer. If the optional argument strexpr2 is omitted, the value is
padded with blank spaces.</term>
<term termid="char.mblen"><linktext>CHAR.MBLEN(strexpr,pos)</linktext
>CHAR.MBLEN(strexpr,pos). Numeric.  Returns the number of bytes in
the character at character position pos of  strexpr.</term>
<term termid="char.rindex"><linktext>CHAR.RINDEX(haystack,needle,divisor)</linktext
>CHAR.RINDEX(haystack,needle[,divisor]). Numeric.  Returns an integer
that indicates the starting character position of the last occurrence
of the string needle in the string haystack.  The optional third argument,
divisor, is the number of characters used to divide needle into separate
strings. For example, CHAR.RINDEX(var1, 'abcd') will return the starting
position of the last occurrence of the entire string "abcd" in the
variable var1; CHAR.RINDEX(var1, 'abcd', 1) will return the value
of the position of the last occurrence of any of the values in the
string; and  CHAR.RINDEX(var1, 'abcd', 2) will return the value of
the starting position of the last occurrence of either "ab" or "cd".
Divisor must be a positive integer and must divide evenly into the
length of needle. If needle is not found, the value 0 is returned.</term>
<term termid="char.rpad"><linktext>CHAR.RPAD(strexpr1,length,strexpr2)</linktext
>CHAR.RPAD(strexpr1,length[,strexpr2]). String. Right-pads strexpr1
with strexpr2 to extend it to the length given by length using as
many complete copies as will fit of strexpr2 as the padding string.
The value of length represents the number of characters and must be
a positive integer. The optional third argument strexpr2 is a quoted
string or an expression that resolves to a string. If strepxr2 is
omitted, the value is padded with blanks.</term>
<term termid="char.substr"><linktext>CHAR.SUBSTR(strexpr,pos,length)</linktext
>CHAR.SUBSTR(strexpr,pos[,length]). String. Returns the substring
beginning at character position pos of strexpr. The optional third
argument represents the number of characters in the substring. If
the optional argument length is omitted, returns the substring beginning
at character position pos of strexpr and running to the end of strexpr.
For example CHAR.SUBSTR('abcd', 2) returns 'bcd' and CHAR.SUBSTR('abcd',
2, 2) returns 'bc'.</term>
<term termid="chebychev_distance_measure_distance_and_proximity_measures"
><linktext>Chebychev Distance Measure (Distance and Proximity Measures)</linktext
>Dissimilarity measure for continuous data. The distance between two
cases is the maximum absolute difference between the values for the
cases.</term>
<term termid="chi-square"><linktext>Chi-Square</linktext>Used to label
any statistic that has approximately a chi-square distribution.</term>
<term termid="chi-square_6"><linktext>Chi-Square</linktext>When the
chi-square statistic is large, and the p-value is small (say less
than 0.10), the null hypothesis can be rejected.</term>
<term termid="chi-square_crosstabs"><linktext>Chi-Square (Crosstabs)</linktext
>Tests the hypothesis that the row and column variables are independent,
without indicating strength or direction of the relationship. Pearson
chi-square, likelihood-ratio chi-square, and linear-by-linear association
chi-square are displayed. Fisher's exact test and Yates' corrected
chi-square are computed for 2x2 tables.</term>
<term termid="chi-square_crosstabs_1"><linktext>Chi-Square (Crosstabs)</linktext
>Statistic used to test the hypothesis that the row and column variables
are independent. This should not be used if any cell has an expected
value less than 1, or if more than 20% of the cells have expected
values less than 5. For general purposes, the significance value is
more important than the actual value of the statistic.</term>
<term termid="chi-square_distance_measure"><linktext>Chi-Square Distance
Measure</linktext>A dissimilarity measure for frequency count data
based on the chi-square statistic. The magnitude of this measure depends
on the total frequencies of the two cases or variables whose distance
is computed. Expected values are from the model of independence of
cases or variables. The value produced by this measure is the square
root of the chi-square statistic.</term>
<term termid="chi-square_goodness-of-fit_test"><linktext>Chi-Square
Goodness-of-Fit Test</linktext>A test of how well a model fits the
observed data. Small observed significance levels (say less than 0.10)
indicate that the model does not fit well.</term>
<term termid="cholesky_manova"><linktext>Cholesky (MANOVA)</linktext
>A method used to decompose the design matrix. Sometimes less accurate
than the QR method.</term>
<term termid="ci_for_exp_b"><linktext>CI for Exp(B)</linktext>A range
of values which N% of the time includes the value of e (2.718) raised
to the parameter value. To change the default value, enter a number
between 1 and 99 (common values are 90, 95, and 99). If the population
value of the parameter is 0, the confidence limits of Exp(B) should
include the value 1.</term>
<term
termid="city-block_distance_measure_distance_and_proximity_measures"
><linktext>City-Block Distance Measure (Distance and Proximity Measures)</linktext
>Dissimilarity measure for continuous data. The distance between two
items is the sum of the absolute differences in values for each variable.</term>
<term termid="classification_plots"><linktext>Classification Plots</linktext
>Histogram of the actual and predicted values of the dependent variable.</term>
<term termid="classification_results"><linktext>Classification Results</linktext
>The number of cases correctly and incorrectly assigned to each of
the groups based on the discriminant analysis. Sometimes called the
"Confusion Matrix."</term>
<term termid="classify"><linktext>Classify</linktext>Allows you to
control the computation of prior probabilities, to obtain summary
classification output, and to control the classification of cases
with missing values.</term>
<term termid="clementine_information"><linktext>Clementine Information</linktext
>Indicates whether or not the data file contains Clementine information.</term>
<term termid="close"><linktext>Close</linktext>Saves defined multiple
response sets and closes the dialog box.</term>
<term termid="cluster"><linktext>Cluster</linktext>A cluster is a
group of cases that are similar to each other in terms of the variables
in the analysis</term>
<term termid="cluster_10"><linktext>Cluster</linktext>Cluster variables
define groups of observational units.  These are useful when directly
sampling observational units from the population is expensive or impossible;
instead you can sample clusters from the population and then sample
observational units from the selected clusters.</term>
<term termid="cluster_9"><linktext>Cluster</linktext>Refers to any
of the procedure-defined groupings of cases.</term>
<term termid="cluster_1"><linktext>Cluster 1</linktext>One of the
cluster joined to form a new cluster. The label of this cluster is
assigned to the newly formed cluster.</term>
<term termid="cluster_2"><linktext>Cluster 2</linktext>One of the
clusters joined to form a new cluster.</term>
<term termid="cluster_combined"><linktext>Cluster combined</linktext
>Indicates the clusters that are combined at each stage.</term>
<term termid="cluster_information_for_each_case"><linktext>Cluster
Information for Each Case</linktext>Displays for each case the final
cluster assignment and the Euclidean distance between the case and
the cluster center used to classify the case. Also displays Euclidean
distance between final cluster centers.</term>
<term termid="cluster_membership_1"><linktext>Cluster membership</linktext
>Cluster membership for the solution with the specified number of
clusters.</term>
<term termid="clustered_bar_chart"><linktext>Clustered Bar Chart</linktext
>Contains a group of bars for each category on the category axis.
Bars within each cluster can represent groups of cases, separate variables,
or individual cases.</term>
<term termid="clustered_box_plot"><linktext>Clustered Box plot</linktext
>Contains a cluster of box plots for each category or variable on
the category axis. The box plots within each cluster are defined by
a separate definition variable.</term>
<term termid="clustered_error_bar"><linktext>Clustered Error Bar</linktext
>Contains a cluster of error bars for each category or variable on
the category axis. The bars within each cluster are defined by a cluster
definition variable.</term>
<term termid="clustered_high-low-close_chart"><linktext>Clustered
High-Low-Close Chart</linktext>Displays two or three values for a
sequence of categories. There is a cluster of high-low-close bars
for each category, variable or case on the category axis. The classical
use of these is to display daily or weekly high, low, and closing
prices of commodities or securities.</term>
<term termid="clustered_range_bar_chart"><linktext>Clustered Range
Bar Chart</linktext>Displays bars that "float" to show the range between
high and low values. There is a cluster of bars for each category
or case on the category axis. The high and low values are defined
by pairs of summary variables.</term>
<term termid="coc"><linktext>COC</linktext>Concentration Index. The
coefficient of concentration measures the percentage of ratios that
fall within an interval. It can be computed either explicitly by defining
the low and high values of the interval, or implicitly by specifying
the percentage of the median.</term>
<term termid="cochran_chi-square"><linktext>Cochran Chi-Square</linktext
>Displays Cochran's Q. This option is appropriate for data that are
dichotomous. The Q statistic replaces the usual F statistic in the
ANOVA table.</term>
<term termid="cochrane-orcutt_method"><linktext>Cochrane-Orcutt</linktext
>A simple and widely used estimation procedure for estimating a regression
equation whose errors follow a first-order autoregressive process.
It cannot be used when a series contains embedded missing values.</term>
<term termid="cochrans_and_mantel-haenszel_statistics"><linktext>Cochran's
and Mantel-Haenszel statistics</linktext>Cochran's and Mantel-Haenszel
statistics can be used to test for independence between a dichotomous
factor variable and a dichotomous response variable, conditional upon
covariate patterns defined by one or more layer (control) variables.
 Note that while other statistics are computed layer by layer, the
Cochran's and Mantel-Haenszel statistics are computed once for all
layers.</term>
<term termid="cochrans_c"><linktext>Cochran's C</linktext>Test that
groups come from populations with the same variance. It is based on
the ratio of the largest group variance to the sum of all the group
variances. For sufficiently large sample sizes, a nonsignificant p
value means there is insufficient evidence that the variances differ.
The test is sensitive to departures from normality.</term>
<term termid="cochrans_q"><linktext>Cochran's Q</linktext>A nonparametric
test of the hypothesis that several related dichotomous variables
have the same mean. The variables are measured on the same individual
or on matched individuals. This is an extension of the McNemar test
to the k-sample situation.</term>
<term termid="cochrans_q_1"><linktext>Cochran's Q</linktext>Cochran's
Q is used to test the hypothesis that the items do not account for
a significant portion of the within-person variability.</term>
<term termid="cochrans_statistic"><linktext>Cochran's statistic</linktext
>Cochran's statistic tests independence between a dichotomous factor
variable and a dichotomous response variable, conditional upon covariate
patterns defined by one or more "stratifying" variables.</term>
<term termid="cod"><linktext>COD</linktext>Coefficient of Dispersion.
 The result of expressing the average absolute deviation as a percentage
of the median.</term>
<term termid="coefficient_iteration_history"><linktext>Coefficient
(iteration history)</linktext>Shows the values of the coefficients
for each iteration. This can be useful for identifying problems with
your predictors if the procedure fails to find a solution.</term>
<term termid="coefficient_correlations"><linktext>Coefficient Correlations</linktext
>Table displays correlations and covariances for each model.</term>
<term termid="coefficient_of_variation_1"><linktext>Coefficient of
Variation</linktext>The ratio of the standard error to the estimate.</term>
<term termid="coefficient_of_variation"><linktext>Coefficient of Variation</linktext
>The ratio of the standard deviation to the mean.</term>
<term termid="coefficient_of_variation_lt_e"><linktext>Coefficient
Of Variation &lt; e</linktext>The absolute value of the coefficient
of variation (the standard deviation divided by the mean) falls below
a chosen threshold.</term>
<term termid="coefficient_total"><linktext>Coefficient Total</linktext
>The sum of the coefficients which should sum to 0; if they do not,
a warning message is displayed.</term>
<term termid="coefficients_2"><linktext>Coefficients</linktext>Table
displays information about regression coefficients.</term>
<term termid="coefficients"><linktext>Coefficients</linktext>Enter
a numeric coefficient value for each group (category) of the Factor
variable and click Add. You can Change or Remove a coefficient after
adding it to the list by selecting it and clicking the appropriate
button. The number of coefficients must equal the number of groups
or the analysis will not be performed.</term>
<term termid="coefficients_3"><linktext>Coefficients</linktext>Index
of the distance between the two clusters joined.</term>
<term termid="coefficients_list_box"><linktext>Coefficients List Box</linktext
>Lists the values of the coefficients for the levels of the factor
variable. Values are added to the bottom of the coefficient list.</term>
<term termid="collinearity_pivot_table_regression"><linktext>Collinearity
 (Pivot Table Regression)</linktext>Collinearity (or multicollinearity)
is the undesirable situation where the correlations among the independent
variables are strong.</term>
<term termid="collinearity_diagnostics"><linktext>Collinearity Diagnostics</linktext
>Displays the tolerances for individual variables and a variety of
statistics for diagnosing collinearity problems. Collinearity (or
multicollinearity) is the undesirable situation when one independent
variable is a linear function of other independent variables.</term>
<term termid="collinearity_diagnostics_1"><linktext>Collinearity Diagnostics</linktext
>Table displays information about intercorrelations among the independent
variables.</term>
<term termid="column_15"><linktext>Column</linktext>Dimension containing
the category numbers or category value labels of the column variable.</term>
<term termid="column_objects"><linktext>Column objects</linktext>The
column objects are represented by variables, or columns, in the dataset.</term>
<term termid="column_percentage_crosstabsdivtables"><linktext>Column
Percentage (Crosstabs/Tables)</linktext>The percentage of all the
cases in a column that fall into a particular cell.</term>
<term termid="column_width_3"><linktext>Column Width</linktext>Data
Editor display column width.</term>
<term termid="column_s_variable_list"><linktext>Column(s) Variable
List</linktext>Selected variables are displayed in columns in the
crosstabulation. Selected variables should be categorical (variables
with a limited number of distinct values). A crosstabulation is produced
for each combination of row and column variables.</term>
<term termid="columns"><linktext>Columns</linktext>Variables on this
list are displayed as columns.</term>
<term termid="combined"><linktext>Combined</linktext>Displays the
results for the between-groups variability, combining all components.</term>
<term termid="combined_1"><linktext>Combined</linktext>The statistics
computed for all cases from all clusters, including the outliers.</term>
<term termid="combined-groups_plots"><linktext>Combined-Groups Plots</linktext
>Creates an all-groups scatterplot of the first two discriminant function
values. If there is only one function, a histogram is displayed instead.</term>
<term termid="comma_variable"><linktext>Comma Variable</linktext>Defines
a numeric variable whose values are displayed with commas delimiting
every three places, and with the period as a decimal delimiter. The
Data Editor accepts numeric values for comma variables with or without
commas; or in scientific notation.</term>
<term termid="common_inter-item_correlation"><linktext>Common Inter-Item
Correlation</linktext>The common off-diagonal term of the inter-item
correlation matrix.</term>
<term termid="common_space_dimension"><linktext>Common Space Dimension</linktext
>The dimensions of the common space.</term>
<term termid="common_variance"><linktext>common variance</linktext
>The total variance of a scale.  Common variance = true variance +
error variance.</term>
<term termid="communality"><linktext>Communality (Factor Analysis)</linktext
>The proportion of the total variance of a variable accounted for
by the common factors in a factor analysis.</term>
<term termid="component"><linktext>Component</linktext>The variance
of a random effect.</term>
<term termid="component_5"><linktext>Component</linktext>Each component
is a linear combination of the observed variables which serves to
summarize them.</term>
<term termid="compound_model"><linktext>Compound Model</linktext>Model
whose equation is Y = b0 * (b1**t) or ln(Y) = ln(b0) + (ln(b1) * t).</term>
<term termid="compound_symmetry"><linktext>Compound Symmetry</linktext
>This structure has constant variance and constant covariance.</term>
<term termid="compressed"><linktext>Compressed</linktext>Indicates
whether or not the data file is compressed.</term>
<term termid="compute_from_group_sizes"><linktext>Compute From Group
Sizes</linktext>Prior probabilities are based on the sample proportion
of cases in each group (after cases with missing values for any predictor
are deleted).</term>
<term termid="concat_strexpr_strexpr_.."><linktext>CONCAT(strexpr,strexpr[,..])</linktext
>CONCAT(strexpr,strexpr[,..]). String. Returns a string that is the
concatenation of all its arguments, which must evaluate to strings.
This function requires two or more arguments. In code page mode, if
strexpr is a string variable, use RTRIM if you only want the actual
string value without the right-padding to the defined variable width.
For example, CONCAT(RTRIM(stringvar1), RTRIM(stringvar2)).</term>
<term termid="concentration"><linktext>Concentration</linktext>A measure
of dispersion that uses p-squared as the basic measure of error, where
p is the probability an observation falls in a given cell.</term>
<term termid="condition"><linktext>Condition</linktext>The condition
specifies the range of proximities that are compared within a transformation.
 They can be row conditional (proximities are compared within rows),
matrix conditional (proximities are compared within sources), or unconditional
(all proximities compared with each other).</term>
<term termid="condition_index"><linktext>Condition Index</linktext
>The square roots of the ratios of the largest eigenvalue to each
successive eigenvalue. A condition index greater than 15 indicates
a possible problem and an index greater than 30 suggests a serious
problem with collinearity.</term>
<term termid="conditional_least_squares"><linktext>Conditional least
squares</linktext>The forecasts are conditional least squares forecasts.
They are also called infinite memory forecasts</term>
<term termid="confidence_interval_5"><linktext>Confidence interval</linktext
>A range of values based on the difference between the sample means.
If the interval does not contain 0, the sample means differ significantly.</term>
<term termid="confidence_interval_8"><linktext>Confidence interval</linktext
>A range of values that has an n% chance of including the population
value of the parameter estimate, where n is the specified percentage
value.</term>
<term termid="confidence_interval_9"><linktext>Confidence interval</linktext
>A range of values that has a N% chance of including the population
value of the parameter estimate.</term>
<term termid="confidence_interval"><linktext>Confidence interval</linktext
>A range of values has an N% chance of including the population value
of the parameter estimate. You can change the level of the confidence
interval. Typical confidence levels are 90, 95, and 99.</term>
<term termid="confidence_interval_entry_text"><linktext>Confidence
interval entry text</linktext>Enter a value between 1 and 99.99 to
specify the confidence level for the two Prediction Intervals. Mean
or Individual must be selected before entering this value. Typical
confidence interval values are 90, 95, and 99.</term>
<term termid="confidence_interval_for_mean"><linktext>Confidence interval
for mean</linktext>Enter a percentage between 1 and 99 to specify
the likelihood that the confidence interval, a range of values based
on the sample mean, includes the population mean. Higher values will
give more conservative intervals (commonly chosen values are 90, 95
or 99). Descriptives must be selected to obtain a confidence interval.</term>
<term termid="gerrorbar01"><linktext>Confidence interval for mean</linktext
>A range of values, based on the sample mean, that, with a designated
likelihood, include the population mean.</term>
<term termid="confidence_interval_of_b"><linktext>Confidence interval
of B</linktext>A range of values that has a 95% chance of including
the population value of the regression coefficient.</term>
<term termid="confidence_interval_of_the_paired_difference"><linktext
>Confidence Interval of the Paired Difference</linktext>A range of
values based on the paired difference. If the interval does not contain
0, the paired difference differs significantly from 0.</term>
<term termid="confidence_interval_7"><linktext>Confidence intervals</linktext
>Displays confidence limits for the forecast. As long as the process
remains the same, you should expect N% of the series to remain between
the upper and lower confidence limits. Choose a level from the drop-down
list.</term>
<term termid="confidence_intervals"><linktext>Confidence intervals</linktext
>The 95% confidence intervals for the regression coefficients.</term>
<term termid="confidence_level"><linktext>Confidence Level</linktext
>The probability that, in the long run, a specified interval around
the value of a statistic estimated from a sample actually includes
the value that would be calculated from the whole population (if that
were possible).</term>
<term termid="confidence_singular_value"><linktext>Confidence singular
value</linktext>Measure of the stability of the singular value for
each dimension.</term>
<term termid="consistent_aic_caic"><linktext>Consistent AIC (CAIC)</linktext
>A measure for selecting and comparing models based on the -2 log
likelihood. Smaller values indicate better models. The CAIC "penalizes"
overparametrized models more strictly than the AIC.  As the sample
size increases, the CAIC converges to the BIC.</term>
<term termid="constant"><linktext>Constant</linktext>The value of
the dependent variable when the independent variables are all zero.</term>
<term termid="constant_6"><linktext>Constant</linktext>The constant
term added to the model.</term>
<term termid="constant_7"><linktext>Constant</linktext>Specifies the
details of any constant term included in the model.</term>
<term termid="constant_covariate"><linktext>Constant Covariate</linktext
>A covariate whose value does not change during the experiment. For
example, a person's height would be a constant covariate.</term>
<term termid="constant_term"><linktext>Constant Term</linktext>In
a linear model, the term which represents the average response.</term>
<term termid="constrained_matrix"><linktext>Constrained Matrix</linktext
>In the strict parallel model, the inter-item covariance matrix is
constrained to have a single diagonal value and a single off-diagonal
value.  In the parallel model, the covariance matrix has a single
off-diagonal covariance value and potentially different diagonal elements.</term>
<term termid="contain"><linktext>Contain</linktext>For any two effects
F1 and F2 in the model, F1 is said to be "contained" in F2 if (1)
Both effects F1 and F2 have the same covariate, if any. (2) F2 consists
of more factors than F1.(3) All factors in F1 also appear in F2. The
intercept effect is treated as contained in all the pure factor effects.
However, it is not contained in any effect involving a covariate.
Also, no effect is contained in the intercept effect. Thus, for any
one effect F of interest, all other effects in the model may be classified
as in one of the following two groups: the effects which do not contain
F or the effects which contain F.</term>
<term termid="contingency_coefficient"><linktext>Contingency Coefficient</linktext
>A measure of association based on chi-square. The value ranges between
0 and 1, with 0 indicating no association between the row and column
variables and values close to 1 indicating a high degree of association
between the variables. The maximum value possible depends on the number
of rows and columns in a table.</term>
<term termid="continue"><linktext>Continue</linktext>Saves selections
and closes the dialog box.</term>
<term termid="continuousvariables"><linktext>Continuous Variables</linktext
>Variables selected as continuous.  They should take values over an
interval.</term>
<term termid="contrast_3"><linktext>Contrast</linktext>A linear combination
of parameters in the model used in comparing the levels of the effect
of interest.</term>
<term termid="contrast"><linktext>Contrast</linktext>A linear combination
of means. For example, the contrast 0.5 * ( Mean 1) + 0.5 * (Mean
2) - 1 * (Mean 3) can be used to test the hypothesis that the average
of the first two means equals the third mean in the population. For
most applications, the coefficients should sum to 0. Sets of coefficients
that do not sum to 0 produce a warning message.</term>
<term termid="contrast_5"><linktext>Contrast</linktext>The type of
contrast.</term>
<term termid="contrast_coefficients_l_matrix"><linktext>Contrast Coefficients
L Matrix</linktext>Method of computing the coefficients in the L matrix.</term>
<term termid="contrast_estimate_1"><linktext>Contrast Estimate</linktext
>The product of the contrast coefficients matrix and the coefficients
estimates matrix; the LB side of the LB=K equation for testing hypotheses.</term>
<term termid="contrast_estimate"><linktext>Contrast Estimate</linktext
>An estimate of a linear combination of cell means.</term>
<term termid="contrast_minus_tests"><linktext>Contrast minus Tests</linktext
>The value of the contrast minus the test value.</term>
<term termid="gcontrastvars"><linktext>Contrast Variables (Regression)</linktext
>A categorical variable can be used in a regression equation by replacing
it with a set of dichotomous variables which, as a group, are equivalent
to the categorical variable. The simplest way to do this, conceptually,
is to use an indicator or "dummy" variable for each category except
one. Other coding schemes are possible, but the number of dichotomies
is always one less than the number of categories in the original variable.</term>
<term termid="contrasts"><linktext>Contrasts</linktext>Allows you
to partition the between-groups sum of squares into trend components
or to specify a priori contrasts to be tested by the t statistic.</term>
<term termid="contribution"><linktext>Contribution</linktext>Proportion
of inertia.</term>
<term termid="contributions"><linktext>Contributions</linktext>The
contribution of each row and column to the inertia of each dimension,
and the proportion of distance to the origin accounted for in each
dimension.</term>
<term termid="control_group_1"><linktext>Control Group</linktext>Cases
that have been specified as "control groups" for use in computing
the natural response rate.</term>
<term termid="control_group"><linktext>Control Group</linktext>The
group used for comparison with the experimental group to see if the
groups are effected in the same way. The control group was defined
by the group 1 value in the Define Groups dialog box.</term>
<term termid="gcontrol06"><linktext>Control Limits (Control Charts)</linktext
>The upper and lower control limits set boundaries on the process.
If the process is in control, most points in a sample should fall
within these limits. Points falling outside of the control limits
may indicate that a process is out of control. Upper and lower control
limits can either be calculated from the data, or explicitly specified.</term>
<term termid="control_variable"><linktext>Control Variable</linktext
>Variables whose effects are removed from the correlations between
the other (non-control) variables.</term>
<term termid="controlling_for"><linktext>Controlling For</linktext
>List the numeric control variables. This list must contain at least
one variable to run this procedure.</term>
<term termid="convergence_criterion_1"><linktext>Convergence Criterion</linktext
>The value used to determine whether the estimation algorithm has
converged.  By default its value is 0.001 times the largest cell size,
or 0.25, whichever is larger.</term>
<term termid="convergence_criterion"><linktext>Convergence Criterion</linktext
>By default, iterations terminate if the largest change in any cluster
center is less than 2% of the minimum distance between initial centers
(or if the maximum number of iterations has been reached). To override
the convergence value, enter a positive number less than or equal
to 1.</term>
<term termid="convergence_tolerance"><linktext>Convergence tolerance</linktext
>The algorithm stops if the change in expected frequency is less than
this value for all cells.</term>
<term termid="convert_numeric_strings_to_numbers"><linktext>Convert
Numeric Strings to Numbers</linktext>Converts string values containing
numbers to numeric values. Strings containing anything other than
numbers and an optional sign (+ or -) are assigned the system-missing
value.</term>
<term termid="cooks"><linktext>Cook's</linktext>The logistic regression
analog of Cook's influence statistic. A measure of how much the residuals
of all cases would change if a particular case were excluded from
the calculation of the regression coefficients.</term>
<term termid="cooks_distance"><linktext>Cook's Distance</linktext
>A measure of how much the residuals of all cases would change if
a particular case were excluded from the calculation of the regression
coefficients. A large Cook's D indicates that excluding a case from
computation of the regression statistics changes the coefficients
substantially.</term>
<term termid="copy_old_values"><linktext>Copy Old Values</linktext
>Retains the old value. If some values don't require recoding, use
this to include the old values. Any old values that are not specified
are not included in the new variable, and cases with those values
will be assigned the system-missing value for the new variable.</term>
<term termid="corr_1_2"><linktext>Corr(^1,^2)</linktext>A correlation
term for the Unstructured Correlations covariance structure.</term>
<term termid="correlated_item-total_correlation"><linktext>Corrected
Item-Total Correlation</linktext>A measure for examining the relationship
between individual items and the total scale, this is the correlation
between the given item and the item sum if the given item is not included
in the scale.  Smaller values indicate the given item is not well
correlated with the others.</term>
<term termid="corrected_model"><linktext>Corrected Model</linktext
>The variation in the dependent variable by other effects (except
the intercept) in the model after corrected for the mean.</term>
<term
termid="corrected_quasi_likelihood_under_independence_model_criterion_qicc"
><linktext>Corrected Quasi Likelihood under Independence Model Criterion
(QICC)</linktext>A measure for choosing between two sets of model
terms, given a correlation structure. Smaller values indicate better
models.</term>
<term termid="corrected_total"><linktext>Corrected Total</linktext
>The total variation in the dependent variable, corrected for the
mean.</term>
<term termid="corrected_total_1"><linktext>Corrected Total</linktext
>Statistics related to the variation in the dependent variable about
its mean value.</term>
<term termid="correlation"><linktext>Correlation</linktext>An estimate
of the correlation between the singular values for each pair of dimensions.
This measure indicates the stability of the singular value between
dimensions.</term>
<term termid="correlation_7"><linktext>correlation</linktext>Two variables
are correlated if a change in the value of one signifies a change
in the other.  The most common measure of correlation is the Pearson
correlation, which measures the degree to which the relationship between
two variables can be described by a straight line.</term>
<term termid="correlation_between_forms"><linktext>Correlation between
forms</linktext>The correlation between the sums of the items in each
group.</term>
<term termid="correlation_coefficients_factor_analysis"><linktext
>Correlation Coefficients (Factor Analysis)</linktext>Correlation
matrix for the variables specified for the factor analysis.</term>
<term termid="correlation_compound_symmetry"><linktext>Correlation
Compound Symmetry</linktext>This covariance structure has homogenous
variances and homogenous correlations between elements.</term>
<term termid="correlation_matrix"><linktext>Correlation Matrix</linktext
>The standardized form of the residual covariance matrix.</term>
<term termid="correlation_of_estimates"><linktext>Correlation of Estimates</linktext
>Displays the correlation matrix of regression coefficients.</term>
<term termid="correlations_crosstabs"><linktext>Correlations (Crosstabs)</linktext
>The Pearson correlation coefficient r, a measure of linear association
between two variables, and the Spearman correlation coefficient, a
measure of association between rank orders. Values of both range between
-1 (a perfect negative relationship) and +1 (a perfect positive relationship).
A value of 0 indicates no linear relationship.</term>
<term termid="correlations_summaries"><linktext>Correlations Summaries</linktext
>Summary statistics for inter-item correlations. The smallest, largest,
and average inter-item correlations, the range and variance of inter-item
correlations, and the ratio of the largest to the smallest inter-item
correlations are displayed.</term>
<term termid="cos_radians"><linktext>COS(radians)</linktext>COS(radians).
Numeric. Returns the cosine of radians, which must be a numeric value,
measured in radians.</term>
<term termid="cosine_similarity_measure"><linktext>Cosine Similarity
Measure</linktext>A pattern similarity measure for continuous data.
Measures the cosine of the angle between two vectors of values. The
cosine ranges from -1 to +1, with a value of 0 indicating orthogonal
vectors.</term>
<term termid="cospectral_density"><linktext>Cospectral density</linktext
>The real part of the cross-periodogram, which is a measure of the
correlation of the in-phase frequency components of two time series.</term>
<term termid="count_6"><linktext>Count</linktext>Frequency or cell
frequency.</term>
<term termid="count"><linktext>Count</linktext>The number of cases
in a group. Cases are counted only if they have valid (nonmissing)
values for the dependent (summary) variable.</term>
<term termid="count_10"><linktext>Count</linktext>The number of responses
that fall in each cell of the crosstabulation table.</term>
<term termid="count_col_pct_tables_statistic"><linktext>Count Col
% (Tables Statistic)</linktext>The percentage of all the cases in
the column that are in the cell. Available for grouping variables
and their totals.</term>
<term termid="count_layer_pct_tables_statistic"><linktext>Count Layer
% (Tables Statistic)</linktext>The percentage of all the cases in
the layer that are in the cell. Available for grouping variables and
their totals.</term>
<term termid="count_percentage"><linktext>Count Percentage</linktext
>The cell count as a percentage of the total.</term>
<term termid="count_row_pct_tables_statistic"><linktext>Count Row
% (Tables Statistic)</linktext>The percentage of all the cases in
the row that are in the cell. Available for grouping variables and
their totals.</term>
<term termid="count_subtable_pct_tables_statistic"><linktext>Count
Subtable % (Tables Statistic)</linktext>The percentage of all the
cases in the smallest subtable containing the cell that are actually
in the cell. Available for grouping variables and their totals.</term>
<term termid="count_table_pct_tables_statistic"><linktext>Count Table
% (Tables Statistic)</linktext>The percentage of all the cases in
the table that are in the cell. Available for grouping variables and
their totals.</term>
<term termid="cov"><linktext>COV</linktext>Coefficient of Variation.
The result of expressing a measure of variation as a percentage of
a measure of location.</term>
<term termid="covariance_6"><linktext>covariance</linktext>A measure
of the linear association between two variables.  It is equal to the
product of the correlation between and standard deviations of those
two variables.</term>
<term termid="covariance"><linktext>Covariance</linktext>An unstandardized
measure of association between two variables, equal to the cross-product
deviation divided by N-1.</term>
<term termid="covariance_matrix_1"><linktext>Covariance Matrix</linktext
>The residual covariance matrix is the residual SSCP matrix divided
by the degrees of freedom of the residual.</term>
<term termid="covariance_matrix"><linktext>Covariance Matrix</linktext
>Displays covariance and correlation matrices. The variance-covariance
matrix of regression coefficients displays the variances on the diagonal,
and covariances above and below.</term>
<term termid="covariance_parameters"><linktext>Covariance Parameters</linktext
>The parameters that define the covariance matrices for the repeated
measures and random effects.</term>
<term termid="covariance_ratio"><linktext>Covariance Ratio</linktext
>The ratio of the determinant of the covariance matrix with a particular
case excluded from the calculation of the regression coefficients
to the determinant of the covariance matrix with all cases included.
If the ratio is close to 1, the case does not significantly alter
the covariance matrix.</term>
<term termid="covariances"><linktext>Covariances</linktext>An unstandardized
measure of association between two variables. The variances appear
on the diagonal.</term>
<term termid="covariances_summaries"><linktext>Covariances Summaries</linktext
>Summary statistics for inter-item covariances. The smallest, largest,
and average inter-item covariances, the range and variance of inter-item
covariances, and the ratio of the largest to the smallest inter-item
covariances are displayed.</term>
<term termid="covariate"><linktext>Covariate</linktext>A quantitative
or independent variable that is correlated with the dependent variable
and thus is a source of variation in the dependent variable that has
not been controlled for in the experiment. For example, in an experiment
on reading comprehension the covariate might be the subject's IQ.
An analysis of covariance removes the variability in the dependent
variable due to the covariate.</term>
<term termid="covariate_10"><linktext>Covariate</linktext>A scale
variable that has been added to a model.  In a predictive model, changes
in the value of a covariate should be associated with changes in the
value of the target (dependent) variable.</term>
<term termid="covariate_coefficients"><linktext>Covariate Coefficients</linktext
>Requests regression coefficients for the covariates. Available only
if the model contains at least one covariate.</term>
<term termid="covariate_s"><linktext>Covariate(s)</linktext>Variables
measured in addition to the dependent variable in analysis of variance.
These continuous explanatory variables represent a source of variation
in the dependent variable that has not been controlled in the experiment.</term>
<term termid="covpar_estimate"><linktext>Covpar Estimate</linktext
>Parameter estimate associated with a covariance matrix.</term>
<term termid="covpar_standard_error"><linktext>Covpar Standard Error</linktext
>Standard error of a parameter estimate associated with a covariance
matrix.</term>
<term termid="cox_and_snell_r_square"><linktext>Cox &amp; Snell R
square</linktext>The Cox &amp; Snell R-square is a generalized coefficient
of determination, used to estimate the proportion of variance in the
dependent variable which is explained by the predictor (independent)
variables. The Cox &amp; Snell R-square is based on the log likelihood
for the model compared to the log likelihood for a baseline model.</term>
<term termid="cp"><linktext>CP</linktext>The capability of the process,
which is the ratio of the difference between the upper specification
limit and the lower specification limit to six times the estimated
capability sigma.</term>
<term termid="cpk"><linktext>CpK</linktext>Capability of process related
to both dispersion and centeredness.  It is the minimum of CpU and
CpL.</term>
<term termid="cpl"><linktext>CpL</linktext>The ratio of the difference
between the process mean and the lower specification limit to the
capability sigma.</term>
<term termid="cpm"><linktext>CpM</linktext>An index relating the capability
sigma and the difference between the process mean and the target value.
 You must specify a target specification limit to obtain this statistic.</term>
<term termid="cpu"><linktext>CpU</linktext>The ratio of the difference
between the process mean and the upper specification limit to the
capability sigma.</term>
<term termid="cr"><linktext>CR</linktext>The reciprocal of the capability
of the process.</term>
<term termid="cramers_v"><linktext>Cramer's V</linktext>A measure
of association based on chi-square. The value ranges between zero
and 1, with zero indicating no association between the row and column
variables and values close to 1 indicating a high degree of association
between the variables. A measure of association based on chi-square.
Cramer's V can attain a value of 1 for tables of any dimension.</term>
<term termid="create_a_new_data_file"><linktext>Create a New Data
File</linktext>Saves aggregated data to an external data file. The
file includes the break variables that define the aggregated cases
and all aggregate variables defined by aggregate functions. The active
dataset is unaffected.</term>
<term termid="create_a_new_dataset_aggregate"><linktext>Create a New
Dataset (Aggregate)</linktext>Saves aggregated data to a new dataset
in the current session. The dataset includes the break variables that
define the aggregated cases and all aggregate variables defined by
aggregate functions. The active dataset is unaffected.</term>
<term termid="creating_function"><linktext>Creating Function</linktext
>Method used to calculate the new values. The source variable name
and span of values used in the calculation (if any) are indicated
in parentheses.</term>
<term termid="cronbachs_alpha"><linktext>Cronbach's Alpha</linktext
>Cronbach's Alpha is a measure of reliability that is used here as
an indicator of model fit.  Larger values of alpha indicate better
models.</term>
<term termid="cronbachs_alpha_2"><linktext>Cronbach's Alpha</linktext
>Cronbach's Alpha is a measure of reliability that is a lower bound
for the true reliability of the survey.  The computation of Cronbach's
alpha is based on the number of items on the survey and the ratio
of the average inter-item covariance to the average item variance.</term>
<term termid="cronbachs_alpha_based_on_standardized_items"><linktext
>Cronbach's Alpha Based on Standardized Items</linktext>Cronbach's
Alpha computed under the assumption that the item variances are all
equal.  Also known as the Spearman-Brown stepped-up reliability coefficient.</term>
<term termid="cronbachs_alpha_if_item_deleted"><linktext>Cronbach's
Alpha if Item Deleted</linktext>A measure for examining the relationship
between individual items and the total scale, this is the value of
Cronbach's Alpha for the remaining items if the given item is not
included in the scale.</term>
<term termid="cross_amplitude"><linktext>Cross amplitude</linktext
>The square root of the sum of the squared cospectral density and
the squared quadrature spectrum.</term>
<term termid="gcrosscorr"><linktext>Cross-Correlations</linktext>A
correlation between two time series. The observations of one series
are correlated with the observations of another series at various
lags and leads. Cross-correlations are often presented in a plot.
Cross-correlations help identify variables which are leading indicators
of other variables. CCF is a procedure that produces cross-correlations.</term>
<term termid="crossover_trial"><linktext>crossover trial</linktext
>This is a study in which each subject is observed for multiple periods
and receives a different level of treatment at each period.  The advantages
of such a study are an economy of time and subjects and that "within-patient"
comparisons of treatments are possible.  A potential problem with
this design  is the possibility of carryover effects.</term>
<term termid="cross-product_deviations_correlations"><linktext>Cross-Product
Deviations (Correlations)</linktext>The cross-product deviation is
equal to the sum of the products of mean-corrected variables.</term>
<term termid="cross-product_deviations_and_covariances"><linktext
>Cross-Product Deviations and Covariances</linktext>Displays the cross-product
deviations and covariances for each pair of variables.</term>
<term termid="crosstabulation"><linktext>Crosstabulation</linktext
>A table displaying the number of cases falling into each combination
of the categories of two or more categorical variables. In addition
to counts, the table may display percentages, expected values, and
residuals.</term>
<term termid="cross-validated"><linktext>Cross-Validated</linktext
>Displays results for the cross-validated cases.</term>
<term termid="cross-variable"><linktext>Cross-variable</linktext>Cross-variable
rules are user-defined rules that can be applied to a single variable
or a combination of variables.</term>
<term termid="crt"><linktext>CRT</linktext>Classification and Regression
Trees.  CRT splits the data into segments that are as homogeneous
as possible with respect to the dependent variable. A terminal node
in which all cases have the same value for the dependent variable
is a homogeneous, "pure" node.</term>
<term termid="cs_covariance"><linktext>CS: covariance</linktext>This
parameter defines the off-diagonal elements of a Compound Symmetry
covariance matrix.</term>
<term termid="cs_diagonal_offset"><linktext>CS: diagonal offset</linktext
>This parameter, plus the covariance parameter, defines the diagonal
elements of a Compound Symmetry covariance matrix.</term>
<term termid="csglm_cslogistic_test_statistics"><linktext>CSGLM, CSLOGISTIC
test statistics</linktext>One of the statistics available for hypothesis
testing in the Complex Samples modeling procedures.</term>
<term termid="csh_rho"><linktext>CSH rho</linktext>The correlation
term for the Heterogeneous Compound Symmetry covariance structure.</term>
<term termid="csr_diagonal"><linktext>CSR diagonal</linktext>A variance
term for the Compound Symmetry with correlation parameterization covariance
structure.</term>
<term termid="csr_rho"><linktext>CSR rho</linktext>The correlation
term for the Compound Symmetry with correlation parameterization covariance
structure.</term>
<term termid="ctime.days_timevalue"><linktext>CTIME.DAYS(timevalue)</linktext
>CTIME.DAYS(timevalue). Numeric. Returns the number of days, including
fractional days, in timevalue, which is a number of seconds, a time
expression, or a time format variable.</term>
<term termid="ctime.hours_timevalue"><linktext>CTIME.HOURS(timevalue)</linktext
>CTIME.HOURS(timevalue). Numeric. Returns the number of hours, including
fractional hours, in timevalue, which is a number of seconds, a time
expression, or a time format variable.</term>
<term termid="ctime.minutes_timevalue"><linktext>CTIME.MINUTES(timevalue)</linktext
>CTIME.MINUTES(timevalue). Numeric. Returns the number of minutes,
including fractional minutes, in timevalue, which is a number of seconds,
a time expression, or a time format variable.</term>
<term termid="ctime.seconds_timevalue"><linktext>CTIME.SECONDS(timevalue)</linktext
>CTIME.SECONDS(timevalue). Numeric. Returns the number of seconds,
including fractional seconds, in timevalue, which is a number, a time
expression, or a time format variable.</term>
<term termid="cubic"><linktext>Cubic</linktext>Tests the cubic effects
across all levels.</term>
<term termid="cubic_model"><linktext>Cubic Model</linktext>Model that
is defined by the equation Y = b0 + (b1 * t) + (b2 * t**2) + (b3 *
t**3).</term>
<term termid="cubic_regression_scatterplot_options"><linktext>Cubic
Regression (Scatterplot Options)</linktext>A least squares regression
curve, including squared and cubed terms, that best fits the data
points on the scatterplot.</term>
<term termid="cubic_term"><linktext>Cubic Term</linktext>Displays
the results of the 3rd-degree polynomial term. A 3rd-degree polynomial
term is the term with a variable raised to the 3rd power.</term>
<term termid="cumulative_12"><linktext>Cumulative</linktext>The cumulative
estimate through each value of the variable.</term>
<term termid="cumulative"><linktext>Cumulative</linktext>The proportion
of inertia accounted for cumulated over the dimensions.</term>
<term termid="cumulative_pct"><linktext>Cumulative %</linktext>The
percentage of the total dispersion accounted for by the canonical
variables (or canonical discriminant functions).</term>
<term termid="cumulative_pct_of_cases"><linktext>Cumulative % of Cases</linktext
>The value graphed represents the cumulative number of cases as a
percentage of the total number of cases. Cumulative summary functions
should not be used in pie charts, or in stacked bar and area charts
that have been transposed so that the cumulative function is along
the scale axis.</term>
<term termid="cumulative_generic"><linktext>Cumulative (generic)</linktext
>Results displayed are cumulative results for all categories up to
and including each category, based on sort order.</term>
<term termid="cumulative_distribution_function"><linktext>cumulative
distribution function</linktext>A cumulative distribution function
(CDF) returns the probability that a variate of a given distribution
falls below a given value for continuous functions and at or below
 a given value for discrete functions.</term>
<term termid="cumulative_events_k-m"><linktext>Cumulative Events</linktext
>Cumulative frequency of events when cases are sorted by their survival
times and status codes. The default variable name is the prefix cum_
with a sequential number appended to it. For example, if cum_1 already
exists, Kaplan-Meier assigns the variable name cum_2.</term>
<term termid="cumulative_frequency"><linktext>Cumulative Frequency</linktext
>The number of cases in the current category or any lower category</term>
<term termid="cumulative_hazard"><linktext>Cumulative Hazard</linktext
>The estimated cumulative hazard function. The value indicates the
probability of observing the event at or before the specified time,
given the values of the predictors.</term>
<term termid="cumulative_n_of_cases"><linktext>Cumulative N of Cases</linktext
>The value graphed represents the number of cases in the current category
plus all cases in previous categories. Cumulative summary functions
should not be used for pie charts, or for stacked bar and area charts
that have been transposed so that the cumulative function is along
the scale axes.</term>
<term termid="cumulative_odds_ratio"><linktext>Cumulative Odds Ratio</linktext
>Cumulative odds are defined as the ratio of the probability that
the dependent variable takes a value less than or equal to a given
response category to the probability that it takes a value greater
than that response category. The cumulative odds ratio is the ratio
of cumulative odds for different predictor values, and is closely
related to the exponentiated parameter estimates. Interestingly, the
cumulative odds ratio itself does not depend upon the response category.</term>
<term termid="cumulative_percent"><linktext>Cumulative Percent</linktext
>The percentage of cases with nonmissing data that have values less
than or equal to a particular value.</term>
<term termid="cumulative_percentage"><linktext>Cumulative Percentage</linktext
>The percentage of cases in the current category or any lower category</term>
<term termid="cumulative_proportion_prefscal"><linktext>Cumulative
proportion (prefscal)</linktext>The proportion of the total inertia
explained by the dimension and all lower dimensions.</term>
<term termid="cumulative_proportion_surviving_at_end_of_interval"
><linktext>Cumulative Proportion Surviving at End of Interval</linktext
>The proportion of cases surviving from the start of the table to
the end of the interval.</term>
<term termid="cumulative_proportion_surviving_at_the_time"><linktext
>Cumulative Proportion Surviving at the Time</linktext>The proportion
of cases surviving from the start of the table until this time.</term>
<term termid="gcumprop"><linktext>Cumulative Proportions</linktext
>The proportion of the distribution that is less than the specified
value.</term>
<term termid="cumulative_sum"><linktext>Cumulative Sum</linktext>A
transformation which replaces each value by the cumulative sum of
the existing series. This is the inverse function of first-order differencing.</term>
<term termid="gbar15"><linktext>Cumulative Sum (Graph Summary Function)</linktext
>The sum of all values in the current category plus all values in
previous categories. Cumulative summary functions should not be used
in pie charts, or in stacked bar charts and area charts that have
been transposed so that the cumulative function is along the scale
axis.</term>
<term termid="cumulative_variance_explained"><linktext>Cumulative
Variance Explained</linktext>The cumulative amount of observed variable
variance explained by adding each factor (or component) to the model</term>
<term termid="current_periodicity_1"><linktext>Current Periodicity</linktext
>Displays the seasonal periodicity that has been defined with the
Define Dates procedure on the Data menu.</term>
<term termid="current_periodicity"><linktext>Current Periodicity</linktext
>Currently defined period used to calculate seasonal differences for
time series data, as determined by defined date variables (Data menu,
Define Dates).</term>
<term termid="custom_6"><linktext>Custom</linktext>Enter a number
in the Start text box and, for models with a trend, a number in the
Trend text box. Poor choice of initial values can result in an inferior
solution.</term>
<term termid="custom"><linktext>Custom</linktext>User-specified cut
point that assigns cases with values less than the cut point to one
group and cases with values greater than or equal to the other group.
You must select at least one cut point, and one test is performed
for each cut point chosen. After selecting this option, enter a value
for the cut point.</term>
<term termid="custom_currency_variable"><linktext>Custom Currency
Variable</linktext>A numeric variable whose values are displayed in
one of the custom currency formats that you have defined in the Currency
tab of the Options dialog box. Defined custom currency characters
cannot be used in data entry but are displayed in the Data Editor.</term>
<term termid="custom_hypothesis_test"><linktext>Custom Hypothesis
Test</linktext>Lists the tests performed.</term>
<term termid="custom_hypothesis_tests"><linktext>Custom Hypothesis
Tests</linktext>Custom hypotheses are specified by using the matrices
in the model LBM=K.</term>
<term termid="gexsmooth04"><linktext>Custom Model (Exponential Smoothing)</linktext
>A custom model allows you to specify the trend and seasonality components.</term>
<term termid="custom_attributes"><linktext>Custom attributes</linktext
>User-defined variable or dataset attributes.</term>
<term termid="customized_distance"><linktext>Customized distance</linktext
>A dissimilarity measure that defines the distance between two cases
as the rth root of the sum of the absolute differences in values on
each variable to the pth power.</term>
<term termid="customized_distance_measure"><linktext>Customized Distance
Measure</linktext>A dissimilarity measure that defines the distance
between two cases as the rth root of the sum of the absolute differences
in values on each variable to the pth power. Select a power and a
root from the drop-down lists.</term>
<term termid="cut_point"><linktext>Cut Point</linktext>After selecting
this option, specifies a numeric cut point for the Grouping Variable,
a value that separates the two groups. All codes less than the cut
point form one group and all codes greater than or equal to the cut
point form the other group.</term>
<term termid="cut_point_1"><linktext>Cut Point</linktext>User-specified
cut point. Assigns cases with values less than or equal to the cut
point to one group and cases with values greater than the cut point
to the other group. After selecting this option, enter a value for
the cut point.</term>
<term termid="cut_point_2"><linktext>cut point</linktext>A cut point
is used to separate cases into two groups, based upon whether values
of a numeric variable fall above or below the cut point.</term>
<term termid="cut_points_for_equal_groups"><linktext>Cut Points for
Equal Groups</linktext>Values that divide the cases into some number
of equal-sized groups. Enter a value between 2 and 100 to specify
the number of equal-sized groups.</term>
<term termid="czl"><linktext>CZL</linktext>The number of capability
sigmas between the process mean and the lower specification limit.</term>
<term termid="czmax"><linktext>CZMAX</linktext>The maximum number
of capability sigmas between the process mean and the specification
limits.</term>
<term termid="czmin"><linktext>CZMIN</linktext>The minimum number
of capability sigmas between the process mean and the specification
limits.</term>
<term termid="czout"><linktext>CZOUT</linktext>The estimated percentage
outside the specification limits.  The standard normal approximation
is based on Z-upper and Z-lower.</term>
<term termid="czlout"><linktext>CZLOUT</linktext>The estimated percentage
outside the lower specification limit. The standard normal approximation
is based on Z-lower.</term>
<term termid="czuout"><linktext>CZUOUT</linktext>The estimated percentage
outside the upper specification limit. The standard normal approximation
is based on Z-upper.</term>
<term termid="czu"><linktext>CZU</linktext>The number of capability
sigmas between the process mean and the upper specification limit.</term>
<term termid="daf"><linktext>DAF</linktext>The dispersion accounted
for is one minus the normalized raw stress, and is a measure of the
overall fit of the model.  The higher the dispersion accounted for,
the better the fit.</term>
<term termid="damped_trend"><linktext>Damped trend</linktext>This
model is appropriate for series with a linear trend that is dying
out and with no seasonality. Its smoothing parameters are level, trend,
and damping trend. Damped exponential smoothing is most similar to
an ARIMA model with 1 order of autoregression, 1 order of differencing,
and 2 orders of moving average.</term>
<term termid="exsm_tdamp"><linktext>Damped Trend (Exponential Smoothing)</linktext
>The mean level of the series increases or decreases with time, but
the rate of change declines. A trend that is dying out.</term>
<term termid="gspectra07"><linktext>Daniell (Unit) Window</linktext
>The shape of a spectral window for which the weights are all equal
to 1.</term>
<term termid="data_cell_width"><linktext>Data Cell Width</linktext
>Specifies the width for all data cells in the table--in point, inches,
or centimeters, as specified in Options.</term>
<term termid="data_entry_for_windows_information"><linktext>Data Entry
for Windows Information</linktext>Indicates whether or not the data
file contains Data Entry for Windows information.</term>
<term termid="n_of_cases_file_information_1"><linktext>Data Information</linktext
>Information about number of cases, number of variables, and case
weighting.</term>
<term termid="data_type_5"><linktext>Data Type</linktext>Indicates
type of data in the file: case data or matrix data.</term>
<term termid="date.dmy_day_month_year"><linktext>DATE.DMY(day,month,year)</linktext
>DATE.DMY(day,month,year). Numeric. Returns a date value corresponding
to the indicated day, month, and year. The arguments must resolve
to integers, with day between 1 and 31, month between 1 and 13, and
year a four-digit integer greater than 1582.  To display the result
as a date, assign a date format to the result variable.</term>
<term termid="date.mdy_month_day_year"><linktext>DATE.MDY(month,day,year)</linktext
>DATE.MDY(month,day,year). Numeric. Returns a date value corresponding
to the indicated month, day, and year. The arguments must resolve
to integers, with day between 1 and 31, month between 1 and 13, and
year a four-digit integer greater than 1582. To display the result
as a date, assign a date format to the result variable.</term>
<term termid="date.moyr_month_year"><linktext>DATE.MOYR(month,year)</linktext
>DATE.MOYR(month,year). Numeric. Returns a date value corresponding
to the indicated month and year. The arguments must resolve to integers,
with month between 1 and 13, and year a four-digit integer greater
than 1582. To display the result as a date, assign a date format to
the result variable.</term>
<term termid="date.qyr_quarter_year"><linktext>DATE.QYR(quarter,year)</linktext
>DATE.QYR(quarter,year). Numeric. Returns a date value corresponding
to the indicated quarter and year.  The arguments must resolve to
integers, with quarter between 1 and 4, and year a four-digit integer
greater than 1582. To display the result as a date, assign a date
format to the result variable.</term>
<term termid="date.wkyr_weeknum_year"><linktext>DATE.WKYR(weeknum,year)</linktext
>DATE.WKYR(weeknum,year). Numeric. Returns a date value corresponding
to the indicated weeknum and year. The arguments must resolve to integers,
with weeknum between 1 and 53, and year a four-digit integer greater
than 1582. The date value returned represents the first day of the
specified week for that year. The first week starts on January 1 of
each year; so the date returned for any given week value will differ
between years. To display the result as a date, assign a date format
to the result variable.</term>
<term termid="date.yrday_year_daynum"><linktext>DATE.YRDAY(year,daynum)</linktext
>DATE.YRDAY(year,daynum). Numeric. Returns a date value corresponding
to the indicated year and daynum.  The arguments must resolve to integers,
with daynum between 1 and 366 and with year being a four-digit integer
greater than 1582. To display the result as a date, assign a date
format to the result variable.</term>
<term termid="datediff_function"><linktext>DATEDIFF(datetime1, datetime2,
"unit")</linktext>DATEDIFF(datetime2, datetime1, "unit"). Numeric.
Calculates the difference between two date/time values and returns
an integer (with any fraction component truncated) in the specified
date/time units, where datetime2 and datetime1 are both date or time
format variables (or numeric values that represent valid date/time
values), and "unit" is one of the following string literal values,
enclosed in quotes: years, quarters, months, weeks, days, hours, minutes,
seconds.</term>
<term termid="date-format_variable"><linktext>Date-Format Variable</linktext
>A numeric variable whose values are displayed in one of several calendar-date
or clock-time formats. Select a template from the scrolling list.
You can enter dates with slashes, hyphens, periods, commas, or blank
spaces as delimiters. The century range for 2-digit year values is
determined by your Options settings (Edit menu, Options, Data tab).</term>
<term termid="datesum_function"><linktext>DATESUM(datetime, value,
"unit", "method")</linktext>DATESUM(datetime, value, "unit", "method").
Numeric. Calculates a date or time value a specified number of units
from a given date or time value, where datetime is a date or time
format variable (or numeric value that represents a valid date/time
value), and "unit" is one of the following string literal values,
enclosed in quotes: years, quarters, months, weeks, days, hours, minutes,
seconds.  The optional method, enclosed in quotes, can be "rollover"
or "closest". The rollover method advances excess days into the next
month. The closest method uses the closest legitimate date within
the month; this is the default.  The value returned is a date/time
value expressed as a number of seconds. To display the value as a
date/time, assign the appropriate format to the variable.</term>
<term termid="decay_factor"><linktext>Decay factor</linktext>The parameter
controlling the rate at which the effect of a transient outlier decays
to 0. It ranges between 0 and 1, with larger values corresponding
to slower decay rates.</term>
<term termid="decimal_places"><linktext>Decimal Places</linktext>Number
of decimal places in the value. Decimal values longer than the defined
number are stored internally and rounded to the defined number for
display.</term>
<term termid="define_groups"><linktext>Define Groups</linktext>Specifies
which codes of the Grouping Variable distinguish the two groups you
want to compare.</term>
<term termid="define_range"><linktext>Define Range</linktext>Specifies
the minimum and maximum integer value for the selected variable in
the Factor list. Specify a range for each factor.</term>
<term termid="define_range_1"><linktext>Define Range</linktext>Specifies
the minimum and maximum integer value for the Grouping Variable. The
minimum value must be less than the maximum value. Cases with values
outside the bounds are excluded during the analysis.</term>
<term termid="degeneracy_indices"><linktext>Degeneracy indices</linktext
>The degeneracy measures provide indications of degenerate solutions.</term>
<term termid="degree_of_differencing"><linktext>Degree of Differencing</linktext
>The number of times that a time series is transformed by taking differences
between series values and their predecessors. Degrees of differencing
greater than 1 or 2 are unusual. The number of values used in the
calculations decreases by 1 for each seasonal differencing.</term>
<term termid="degrees_of_freedom_2"><linktext>Degrees of Freedom</linktext
>The degrees of freedom associated with calculating F Change. df1
is the numerator degrees of freedom and df2 is the denominator degrees
of freedom.</term>
<term termid="degrees_of_freedom_1"><linktext>Degrees of Freedom</linktext
>Degrees of freedom for the test distribution.</term>
<term termid="degrees_of_freedom"><linktext>Degrees of Freedom</linktext
>Value associated with a test statistic that is used in determining
the observed significance level.</term>
<term termid="delay"><linktext>Delay</linktext>Specifies the value
of any delay set for the associated independent variable.</term>
<term termid="deleted_effect"><linktext>Deleted effect</linktext>The
effect is tested for significance by deleting it from the model. If
the change in chi-square is not significant; for example, if the significance
value is greater than 0.10, the effect can be dropped from the model.</term>
<term termid="deleted_residual"><linktext>Deleted Residual</linktext
>The residual for a case when that case is excluded from the calculation
of the regression coefficients. It is the difference between the value
of the dependent variable and the adjusted predicted value.</term>
<term termid="deleted"><linktext>Deleted residual</linktext>The residual
for a case when that case is excluded from the calculation of the
regression coefficients. It is the difference between the value of
the dependent variable and the adjusted predicted value.</term>
<term termid="dendrogram"><linktext>Dendrogram</linktext>A visual
representation of the steps in a hierarchical clustering solution
that shows the clusters being combined and the values of the distance
coefficients at each step. Connected vertical lines designate joined
cases. The dendrogram rescales the actual distances to numbers between
0 and 25, preserving the ratio of the distances between steps.</term>
<term termid="denominator"><linktext>Denominator</linktext>The denominator
variables.</term>
<term termid="denominator_orders"><linktext>Denominator Orders</linktext
>Specifies that the transfer function for the associated independent
variable contains denominator orders. The details for each lag (order)
are provided.</term>
<term termid="density_plot"><linktext>Density Plot</linktext>Displays
the density function.</term>
<term termid="dependent_list"><linktext>Dependent List</linktext>Displays
the dependent variables you have chosen for the analysis.</term>
<term termid="dependent_variable_2"><linktext>Dependent Variable</linktext
>Also known as a response variable. A variable whose values are being
predicted or modeled, usually because it is thought to be influenced
or caused by the factors and covariates, if any.</term>
<term termid="dependent_variable"><linktext>Dependent Variable</linktext
>Also known as a response variable. A variable whose values are being
predicted or modeled, usually because it is thought to be influenced
or caused by the independent variable(s).</term>
<term termid="dependent_mi"><linktext>Dependent Variables</linktext
>Lists the variables with a role as a dependent during imputation,
broken down by those that were imputed, those that were not imputed
because the percentage of missing values was too high, and those that
were not imputed because they had no missing values.</term>
<term termid="dependent_variable_independent_variable"><linktext>Dependent
Variable Independent Variable</linktext>Displays results for each
independent variable in the first layer.</term>
<term termid="dependent_s"><linktext>Dependent(s)</linktext>The variable
whose values you wish to predict or summarize.</term>
<term termid="dependent_s_1"><linktext>Dependent(s)</linktext>Lists
the variables whose values you wish to predict. If you select Time
instead of an Independent Variable, the dependent variable should
be a time series measure.</term>
<term termid="dependents_together"><linktext>Dependents Together</linktext
>For a given group, displays box plots for each variable side by side.
You can easily compare the values of the dependent variables for a
particular group. This display is particularly useful when the different
variables represent a single characteristic measured at different
times.</term>
<term termid="depth"><linktext>Depth</linktext>The number of levels
in the final tree model below the root node level.</term>
<term termid="derivative"><linktext>Derivative</linktext>The user-specified
partial derivative of the model function.</term>
<term termid="derivatives_nonlin"><linktext>Derivatives</linktext
>One derivative is saved for each model parameter. Derivative names
are created by prefixing 'd.' to the first six characters of parameter
names.</term>
<term termid="derived_axis"><linktext>Derived Axis</linktext>An optional
axis that appears on the opposite side of the chart from the scale
axis. The units displayed on this axis are derived from the scale
axis units. For example, you might want to display counts on the scale
axis and percents on the derived axis.</term>
<term termid="desarbos_intermixedness_indices"><linktext>DeSarbo's
Intermixedness Indices</linktext>A measure of whether the points of
the different sets are reasonably well intermixed.  Values closer
to zero indicate more intermixed solutions, and are less likely to
be degenerate.</term>
<term termid="descending"><linktext>Descending</linktext>Displays
the row variable values in descending order from highest to lowest.</term>
<term termid="descending_counts"><linktext>Descending Counts</linktext
>Arranges frequency table according to the frequency of occurrence
of the values. Frequencies are arranged in descending order (most
frequent to least frequent).</term>
<term termid="descending_means"><linktext>Descending Means</linktext
>Sorts the variables in descending order (largest to smallest) of
the Mean statistic.</term>
<term termid="descending_values"><linktext>Descending Values</linktext
>Arranges frequency table according to the actual values in the data
in descending order (largest to smallest value).</term>
<term termid="description_1"><linktext>Description</linktext>A description
of the rule properties.</term>
<term termid="descriptive_statistics_4"><linktext>Descriptive Statistics</linktext
>Displays variable means, standard deviations, and a correlation matrix
with one-tailed probabilities.</term>
<term termid="descriptive_statistics_5"><linktext>Descriptive Statistics</linktext
>Displays the mean, minimum, maximum, standard deviation, and the
number of nonmissing cases.</term>
<term termid="descriptive_statistics_3"><linktext>Descriptive Statistics</linktext
>Allows you to choose optional statistics and correlation matrices.</term>
<term termid="descriptive_statistics_2"><linktext>Descriptive Statistics</linktext
>Displays the mean, median, mode, 5% trimmed mean, standard error,
variance, standard deviation, minimum, maximum, range, interquartile
range, skewness, skewness standard error, kurtosis and kurtosis standard
error.</term>
<term termid="descriptive_statistics"><linktext>Descriptive Statistics</linktext
>Summary information about the distribution, variability, and central
tendency of a variable.</term>
<term termid="descriptive_statistics_1"><linktext>Descriptive Statistics</linktext
>Displays the number of cases, mean, standard deviation, standard
error, minimum, maximum, and 95% confidence interval for each dependent
variable for each group.</term>
<term termid="design_effect"><linktext>Design Effect</linktext>The
ratio of the variance of the estimate to the variance obtained by
assuming the sample is a simple random sample.  This is a measure
of the effect of specifying a complex design, where values further
from 1 indicate greater effects.</term>
<term termid="design_variables"><linktext>Design Variables</linktext
>The stratification and cluster variables define the structure of
the sample design.</term>
<term termid="determinant_of_correlation_matrix_factor_analysis"><linktext
>Determinant of Correlation Matrix (Factor Analysis)</linktext>The
determinant of the matrix of correlation coefficients.</term>
<term termid="detrended_normal_plots_explore"><linktext>Detrended
Normal Plots (Explore)</linktext>Plots of the differences between
the observed and expected values. If the sample is from a normal distribution
the points should cluster in a horizontal band around zero; there
should not be a pattern.</term>
<term termid="deviance"><linktext>Deviance</linktext>A goodness-of-fit
statistic defined as twice the difference between the maximum possible
value of the log likelihood function and the value of the log likelihood
for the fitted model.  When the maximum possible value of the log-likelihood
function is 0, the deviance is the -2 log likelihood.</term>
<term termid="deviance_residuals"><linktext>Deviance</linktext>Residuals
based on the model deviance.</term>
<term termid="deviance_3"><linktext>Deviance</linktext>Goodness of
fit statistics based upon the deviance function.</term>
<term termid="deviance_residual_general_loglinear"><linktext>Deviance
Residual (General Loglinear)</linktext>The signed square root of an
individual contribution to the likelihood-ratio chi-square statistic
(G squared), where the sign is the sign of the residual (observed
count minus expected count). Deviance residuals have an asymptotic
standard normal distribution.</term>
<term termid="deviation_contrast"><linktext>Deviation Contrast</linktext
>A deviation contrast compares the effect for each category of the
predictor variable or factor, except one, to the overall effect.</term>
<term termid="deviation_contrast_1"><linktext>Deviation Contrast</linktext
>Compares the mean of each level (except a reference category) to
the mean of all of the levels (grand mean). The levels of the factor
can be in any order.</term>
<term termid="deviation_contrasts"><linktext>Deviation Contrasts</linktext
>Compares the effect for each category of the predictor variable or
factor, except one, to the overall effect. Select either First or
Last as the omitted category.</term>
<term termid="deviation_from_linearity"><linktext>Deviation from Linearity</linktext
>The component of the between-groups sums of squares that can not
be attributed to a linear relationship between the factor level and
the dependent variable.</term>
<term termid="deviation_in_detrended_normal_plot"><linktext>Deviation
in Detrended Normal Plot</linktext>All points are ranked from smallest
to largest and each is paired with an expected normal value for a
sample of that size from a standard normal distribution. The deviation
in the detrended normal plot is the difference between the standardized
value for a case and its expected normal value.</term>
<term termid="df"><linktext>df</linktext>The degrees of freedom used
to obtain the observed significance level.</term>
<term termid="df_2"><linktext>df</linktext>Degrees of freedom associated
with this probability</term>
<term termid="df_1"><linktext>df</linktext>Degrees of freedom associated
with this Mean Square</term>
<term termid="df1"><linktext>df1</linktext>Numerator degrees of freedom.
The numerator and denominator degrees of freedom are used to obtain
the observed significance level.</term>
<term termid="df2"><linktext>df2</linktext>Denominator degrees of
freedom. The numerator and denominator degrees of freedom are used
to obtain the observed significance level.</term>
<term termid="dfbeta_s"><linktext>DfBeta(s)</linktext>The difference
in beta value is the change in the regression coefficient that results
from the exclusion of a particular case. A value is computed for each
term in the model, including the constant.</term>
<term termid="dfbeta_coxreg"><linktext>DfBetas</linktext>Estimated
change in a coefficient if a case is removed. One variable is saved
for each covariate in the final model. DfBetas are only available
for models containing at least one covariate.</term>
<term termid="dffit"><linktext>DfFit</linktext>The difference in fit
value is the change in the predicted value that results from the exclusion
of a particular case.</term>
<term termid="diagnostic_checking"><linktext>Diagnostic Checking</linktext
>After estimating the parameters of a model, you use visual and statistical
checks to see how well the estimated model fits the data, and whether
or not you met the necessary assumptions for fitting the model.</term>
<term termid="diagonal_2"><linktext>Diagonal</linktext>This covariance
structure has heterogeneous variances and zero correlation between
elements.</term>
<term termid="dice_similarity_measure"><linktext>Dice Similarity Measure</linktext
>A matching coefficient for binary variables in which joint absences
are excluded from both the denominator and the numerator and double
weight is given to matches. Computed from a fourfold table as 2a/(2a+b+c)
where a represents cases present on both items, and b and c represent
cases present on one item but absent on the other.</term>
<term termid="dichotomies"><linktext>Dichotomies</linktext>Elementary
variables having two categories. After selecting this option to create
a multiple dichotomy set, enter an integer value for Counted value.
Each variable having at least one occurrence of the counted value
becomes a category of the multiple dichotomy set.</term>
<term termid="dichotomous"><linktext>dichotomous</linktext>A term
for a variable that has two possible values.</term>
<term termid="gdichotomy"><linktext>Dichotomous Variable</linktext
>A variable that has precisely two distinct values, apart from missing
values.</term>
<term termid="difference"><linktext>Difference</linktext>d is the
number of times the series must be differenced to make it stationary.
Specify a non-negative integer. Enter 0 if the process is already
stationary.</term>
<term termid="difference_8"><linktext>Difference</linktext>Specifies
the number of differencing (integration) orders for the associated
dependent or independent variable.</term>
<term termid="difference_estimate_-_hypothesized"><linktext>Difference
(Estimate - Hypothesized)</linktext>The difference in the estimated
and hypothesized values.</term>
<term termid="difference_prefscal"><linktext>Difference (prefscal)</linktext
>The difference in penalized stress between the last iteration and
the current iteration.</term>
<term termid="difference_contrast_1"><linktext>Difference Contrast</linktext
>Compares the mean of each level (except the first) to the mean of
previous levels. They are sometimes called reverse Helmert contrasts.</term>
<term termid="difference_contrasts"><linktext>Difference Contrasts</linktext
>The effect for each category of the predictor variable or factor
except the first is compared to the mean effect of the previous categories.
Also known as reverse Helmert contrasts.</term>
<term termid="difference_estimate_hypothesized"><linktext>Difference
Estimate Hypothesized</linktext>The contrast estimate minus the hypothesized
value.</term>
<term termid="difference_line_chart"><linktext>Difference Line Chart</linktext
>Chart containing two lines. Each line connects a series of points,
one for each category, case or variable on the category axis. The
area between the two lines is shaded with a color or pattern, which
changes when the two lines cross to emphasize which line is higher
at any point.</term>
<term termid="gacf02"><linktext>Difference Transformation</linktext
>Calculates the difference between successive values of the variable.
You must enter a positive value to specify the degree of differencing.
A differencing of order 1 takes the current value minus the previous
value, just as this would imply. But a differencing of order 2 does
the same thing to the first order differenced series, rather than
taking each value minus the one two cases prior.</term>
<term termid="differences"><linktext>Differences</linktext>This dimension
lists the negative and positive differences, along with the number
of tied differences, and the total number of pairs.</term>
<term termid="dimension"><linktext>Dimension</linktext>Lists the dimensions.</term>
<term termid="dimension_2"><linktext>Dimension</linktext>Labels the
dimensions of the solution.</term>
<term termid="dimension_3"><linktext>Dimension</linktext>Labels the
dimensions of the solution.</term>
<term termid="dimensionality"><linktext>Dimensionality</linktext>The
number of dimensions in the solution.</term>
<term termid="dimensions_7"><linktext>Dimensions</linktext>The number
of dimensions in the user-provided initial configuration or fixed
coordinates.</term>
<term termid="direct_oblimin"><linktext>Direct Oblimin Method (Factor
Analysis)</linktext>A method for oblique (nonorthogonal) rotation.
When delta equals 0 (the default), solutions are most oblique. As
delta becomes more negative, the factors become less oblique. To override
the default delta of 0, enter a number less than or equal to 0.8.</term>
<term termid="direction"><linktext>Direction</linktext>Directional
measures range from 0 to 1, where 0 means that knowledge of the independent
variable is no help in predicting the dependent variable, and 1 means
that knowing the independent variable perfectly identifies the categories
of the dependent variable.</term>
<term termid="discriminant_score"><linktext>Discriminant Score</linktext
>Scores computed by multiplying the unstandardized discriminant coefficients
by the values of the independent variables, summing these products,
and adding the constant. One score is saved for each discriminant
function derived. The mean score for all cases combined is 0 and the
pooled within-groups variance is 1.</term>
<term termid="dispersion"><linktext>dispersion</linktext>An attribute
of a frequency distribution concerning the spread of the values. 
Measures of dispersion include the variance, standard deviation, and
interquartile range.</term>
<term termid="dispersion_similarity_measure"><linktext>Dispersion
Similarity Measure</linktext>Similarity measure for binary data. Computed
from a fourfold table as (ad-bc)/(a+b+c+d)**2 where a represents the
cell corresponding to cases present on both items, d the cells corresponding
to cases absent of both items, and b and c represent cases present
on one item but absent on the other. Has a range of -1 to +1.</term>
<term termid="display_actual_significance_level"><linktext>Display
Actual Significance Level</linktext>The probability and degrees of
freedom are shown for each coefficient.</term>
<term termid="display_anova_table"><linktext>Display ANOVA Table</linktext
>Displays a summary analysis-of-variance table for each selected model.</term>
<term termid="display_factor_score_coefficient_matrix"><linktext>Display
Factor Score Coefficient Matrix</linktext>Displays the factor score
coefficient matrix. The factor score covariance matrix is also displayed.</term>
<term termid="display_frequency_tables"><linktext>Display Frequency
Tables</linktext>Displays frequency tables for the selected variables.
If deselected, no frequency table is displayed, but any statistics
and charts that you request will appear in the output.</term>
<term termid="display_only_10_best_models_for_grid_search"><linktext
>Display only 10 best models for grid search</linktext>The parameter
value(s) and sum of squared errors (SSE) are displayed for only the
10 parameter combinations with the lowest SSE regardless of the number
of parameter combinations tested. If this option is not selected,
all tested parameter combinations are displayed.</term>
<term termid="distance"><linktext>Distance</linktext>Distance of the
case from its cluster center</term>
<term termid="distance_matrix"><linktext>Distance Matrix</linktext
>Matrix of distances between the items. The type of matrix produced
(similarities or dissimilarities) depends upon the measure selected.
With a large number of items, this specification produces a large
volume of output.</term>
<term termid="distinct_values"><linktext>Distinct Values</linktext
>Displays the number of unique values.</term>
<term termid="distribution_2"><linktext>distribution</linktext>Details
of the distribution type of the data.</term>
<term termid="distribution_type"><linktext>distribution type</linktext
>The distribution type of the data, for example, NORMAL.</term>
<term termid="do_not_create"><linktext>Do not create</linktext>The
new series are not added to the active dataset.</term>
<term termid="does_not_assume_equal_variances"><linktext>Does Not
Assume Equal Variances</linktext>Displays the results without assuming
that the population variances are equal.</term>
<term termid="dollar_variable"><linktext>Dollar Variable</linktext
>Defines a numeric variable whose values include a dollar sign, one
period for the decimal point, and multiple commas.</term>
<term termid="dot_variable"><linktext>Dot Variable</linktext>Defines
a numeric variable whose values are displayed with periods delimiting
every three places, and with the comma as a decimal delimiter. The
Data Editor accepts numeric values for dot variables with or without
dots; or in scientific notation.</term>
<term termid="drop-line_chart"><linktext>Drop-Line Chart</linktext
>The chart has vertical lines connecting the markers within each category.
Each series of markers can represent groups of cases, separate variables,
or individual cases.</term>
<term termid="duncans_multiple_range_test"><linktext>Duncan's Multiple
Range Test</linktext>Makes pairwise comparisons using a stepwise order
of comparisons identical to the order used by the Student-Newman-Keuls
test, but sets a protection level for the error rate for the collection
of tests, rather than an error rate for individual tests. Uses the
Studentized range statistic.</term>
<term termid="dunnett_post_hoc"><linktext>Dunnett (Post Hoc)</linktext
>Pairwise multiple comparison t test that compares a set of treatments
against a single control mean.</term>
<term termid="dunnetts_c_post_hoc"><linktext>Dunnett's C (Post Hoc)</linktext
>Pairwise comparison test based on the Studentized range. This test
is appropriate when the variances are unequal.</term>
<term termid="dunnetts_t3_post_hoc"><linktext>Dunnett's T3 (Post Hoc)</linktext
>Pairwise comparison test based on the Studentized maximum modulus.
This test is appropriate when the variances are unequal.</term>
<term termid="duplicate_identifiers_group"><linktext>Duplicate Identifiers
Group</linktext>The label given to a group of cases with duplicate
identifying values.</term>
<term termid="durbin-watson"><linktext>Durbin-Watson</linktext>Displays
the Durbin-Watson test for serially correlated residuals and summary
statistics for residuals and predicted values.</term>
<term termid="durbin-watson_test"><linktext>Durbin-Watson Test</linktext
>A test for serially correlated (or autocorrelated) residuals. One
of the assumptions of regression analysis is that the residuals for
consecutive observations are uncorrelated. If this is true, the expected
value of the Durbin-Watson statistic is 2. Values less than 2 indicate
positive autocorrelation, a common problem in time-series data. Values
greater than 2 indicate negative autocorrelation.</term>
<term termid="edit_look"><linktext>Edit Look</linktext>Displays the
Table Properties dialog box to enable changes to the current table
look.</term>
<term termid="effect"><linktext>Effect</linktext>A factor or interaction
term in the model. The actual effect being tested is indicated before
all tests.</term>
<term termid="effect_1"><linktext>Effect</linktext>A factor, covariate,
or interaction term in the model.</term>
<term termid="effect_pattern"><linktext>Effect Pattern</linktext>An
effect in the within-subjects design.</term>
<term termid="effect_selection_tests"><linktext>Effect Selection Tests</linktext
>Tests used to determine whether an effect is added or removed from
the model.</term>
<term termid="effective_prior"><linktext>Effective Prior</linktext
>When the user specifies prior probabilities that do not sum to 1,
Discriminant offers a set of prior probabilities that do sum to 1.
A prior probability is an estimate of the likelihood that a case belongs
to a particular group when no other information about it is available.</term>
<term termid="eigenvalue_1"><linktext>Eigenvalue</linktext>Eigenvalues
provide an indication of how many distinct dimensions there are among
the independent variables. When several eigenvalues are close to zero,
the variables are highly intercorrelated and small changes in the
data values may lead to large changes in the estimates of the coefficients.</term>
<term termid="eigenvalue"><linktext>Eigenvalue (Factor Analysis)</linktext
>An index representing the variance associated with a particular factor.
The sum of the eigenvalues cannot exceed the number of variables in
the analysis (when analyzing a correlation matrix) or the sum of the
variances for all variables (when analyzing a covariance matrix).</term>
<term termid="eigenvalues"><linktext>Eigenvalues</linktext>Measure
of the explained variance per dimension. Larger eigenvalues indicate
dimensions that are of more importance in the overall solution.</term>
<term termid="eigenvalues_2"><linktext>Eigenvalues</linktext>The eigenvalue
is the ratio of the between-groups sum of squares to the within-groups
sum of squares. The largest eigenvalue corresponds to the eigenvector
in the direction of the maximum spread of the groups means, the second
largest eigenvalue corresponds to the eigenvector in the direction
that has the next largest spread, and so on.</term>
<term termid="eigenvalues_over_n"><linktext>Eigenvalues Over n</linktext
>By default, factors with eigenvalues greater than 1 (when analyzing
a correlation matrix) or the average item variance (when analyzing
a covariance matrix) are extracted. To use a different eigenvalue
as the cutoff value for factor extraction, enter a number between
0 and the total number of variables in your analysis.</term>
<term termid="em_2"><linktext>EM</linktext>Displays means, correlation
matrix, and covariance matrix, computed using an EM algorithm. The
EM (expectation maximization) method estimates missing values by an
iterative process. Each iteration has an E step to calculate expected
values of parameters and an M step to calculate maximum likelihood
estimates.</term>
<term termid="empirical"><linktext>Empirical</linktext>Method of estimating
percentiles using the empirical distribution function.</term>
<term termid="empty_case_indicator"><linktext>Empty Case Indicator</linktext
>No help for Notes table.</term>
<term termid="end_point"><linktext>End Point</linktext>These values
define the upper and lower bounds of the bins generated for this variable.</term>
<term termid="endpoints_weighted_by_.5"><linktext>Endpoints weighted
by .5</linktext>Moving averages for series with even periodicity are
calculated with a span equal to the periodicity plus 1 and with the
endpoints of the span weighted by 0.5.</term>
<term termid="endogenous"><linktext>Endogenous</linktext>A variable
is endogenous in a model if it is at least partly a function of other
variables in the model. </term>
<term termid="enter"><linktext>Enter</linktext>A procedure for variable
selection in which the named variables are entered in a single step
without checking any of the entry criteria except tolerance.</term>
<term termid="enter_regression"><linktext>Enter (Regression)</linktext
>A procedure for variable selection in which all variables in a block
are entered in a single step.</term>
<term termid="enter_independents_together"><linktext>Enter Independents
Together</linktext>Forced-entry method. All independent variables
that satisfy tolerance criteria are entered simultaneously.</term>
<term termid="entered"><linktext>Entered</linktext>Variable included
in the analysis at a particular step.</term>
<term termid="entropy"><linktext>Entropy</linktext>A measure of dispersion
that uses p * ln(p) as the basic measure of error, where p is the
probability an observation falls in a given cell.</term>
<term termid="epsilon_general_loglinear"><linktext>Epsilon (General
Loglinear)</linktext>The value used for redundancy checking in the
design matrix. </term>
<term termid="equamax_method"><linktext>Equamax Method</linktext>A
rotation method that is a combination of the varimax method, which
simplifies the factors, and the quartimax method, which simplifies
the variables. The number of variables that load highly on a factor
and the number of factors needed to explain a variable are minimized.</term>
<term termid="equation_1"><linktext>Equation</linktext>Displays labels
for the models being fit.</term>
<term termid="equation"><linktext>Equation</linktext>The function
that defines the shape of the curve.</term>
<term termid="error"><linktext>Error</linktext>The variation left
unexplained after the model has been considered, or for a mixed- or
random-effects model, the error term used for testing the hypothesis
directly above.</term>
<term termid="error_2"><linktext>Error</linktext>The residual term.</term>
<term termid="error_3"><linktext>Error</linktext>Statistics for the
within-cluster variance</term>
<term termid="error_6"><linktext>Error</linktext>Measure of the badness-of-fit
of the solution, or how poorly the solution fits the original data.
The error is the square root of the sum of squared differences between
the dependent variable and the prediction.</term>
<term termid="error_df"><linktext>Error DF</linktext>The degrees of
freedom associated with the error term.</term>
<term termid="error_function_mlp"><linktext>Error Function (MLP)</linktext
>The error function is the measure that the neural network is trying
to minimize.  Cross-entropy error is used if the output layer activation
function is Softmax; otherwise sum of squares error is used.</term>
<term termid="error_function_rbf"><linktext>Error Function (RBF)</linktext
>The error function is the measure that the neural network is trying
to minimize.  Sum of squares error is always used.</term>
<term termid="error_term"><linktext>Error Term</linktext>The effect
or combination of effects against which the hypothesis term is being
tested.</term>
<term termid="error_variance"><linktext>error variance</linktext>The
error variance of a scale is the portion of the common variance that
is due to error.  Common variance = true variance + error variance.</term>
<term termid="error_mlp"><linktext>Error (MLP)</linktext>The value
of the error function is displayed for each sample.  If there are
multiple target (dependent) variables, the average error over all
targets is displayed in addition to the error for each target.</term>
<term termid="error_rbf"><linktext>Error (RBF)</linktext>The value
of the error function is displayed for each sample.  If there are
multiple target (dependent) variables, the average error over all
targets is displayed in addition to the error for each target.</term>
<term termid="estimable_function"><linktext>Estimable Function</linktext
>The linear combination of parameters in the design. When applied
to the estimates, the result is an unbiased estimate of the original
combination.</term>
<term termid="estimable_function_for_intercept"><linktext>Estimable
Function for Intercept</linktext>An estimable function of parameters
in the model used to test the intercept. An estimable function is
a function of parameters which is invariant with respect to the choice
of a least-squares solution.</term>
<term termid="estimable_function_for_intercept_1"><linktext>Estimable
Function for Intercept</linktext>The contrast for testing the value
of the intercept.</term>
<term termid="estimate_5"><linktext>Estimate</linktext>The estimated
value of the parameter.</term>
<term termid="estimate_tree"><linktext>Estimate</linktext>The proportion
of cases misclassified by the model if custom costs have not been
defined. Costs can increase or decrease the risk estimate.</term>
<term termid="estimate_9"><linktext>Estimate</linktext>The estimated
value of the regression coefficient.</term>
<term termid="estimate"><linktext>Estimate</linktext>The value estimated
for the variance due to the indicated effect or interaction.</term>
<term termid="estimate_8"><linktext>Estimate</linktext>Estimated value
of the model parameter.</term>
<term termid="estimate_6"><linktext>Estimate</linktext>The value of
the parameter estimate.  Note that the bootstrap estimates are, in
fact, the asymptotic estimates.  Bootstrapping is only used to compute
the standard error, confidence intervals, and trimmed range.</term>
<term termid="estimate_multinomial_logistic_regression"><linktext
>Estimate (multinomial logistic regression)</linktext>Estimate of
the coefficient of the parameter effect on the logit.</term>
<term termid="estimate_of_between_component_variance"><linktext>Estimate
of Between Component Variance</linktext>The component of the total
variation that can be attributed to the differences between groups
under the random effects or variance component model.</term>
<term termid="estimates"><linktext>Estimates</linktext>Regression
coefficients and related measures.</term>
<term termid="estimates_of_effect_size"><linktext>Estimates of Effect
Size</linktext>Partial eta-squared. Partial eta-squared is the ratio
of the variation accounted for by an individual independent variable
(SSH) to the sum of the variation accounted for by the independent
variable and the variation unaccounted for by the model as a whole
(SSH+SSE).</term>
<term termid="estimation_period"><linktext>Estimation Period</linktext
>The range of cases from which the model is estimated. Also called
the historical period. Defined with the Range subdialog box of the
Select Cases option on the Data menu.</term>
<term termid="estimator_assumption"><linktext>Estimator Assumption</linktext
>For the purposes of estimation, analysis procedures should treat
the sample as if the method had these properties.</term>
<term termid="eta"><linktext>Eta</linktext>A measure of association
that is appropriate for a dependent variable measured on an interval
scale and an independent variable with a limited number of categories.
Eta is asymmetric and does not assume a linear relationship between
the variables. Eta-squared can be interpreted as the proportion of
variance in the dependent variable explained by differences among
groups.</term>
<term termid="eta_2"><linktext>Eta</linktext>A measure of association
that that ranges from 0 to 1, with 0 indicating no association between
the variables and values close to 1 indicating a high degree of association.
Eta is appropriate for a dependent variable measured on an interval
scale (for example, income) and an independent variable with a limited
number of categories (for example, gender).</term>
<term termid="eta_crosstabs"><linktext>Eta (Crosstabs)</linktext>A
measure of association that ranges from 0 to 1, with 0 indicating
no association between the row and column variables and values close
to 1 indicating a high degree of association. Eta is appropriate for
a dependent variable measured on an interval scale (for example, income)
and an independent variable with a limited number of categories (for
example, gender). Two eta values are computed: one treats the row
variable as the interval variable, and the other treats the column
variable as the interval variable.</term>
<term termid="eta-squared"><linktext>Eta-Squared</linktext>Eta-squared
is interpreted as the proportion of the total variability in the dependent
variable that is accounted for by variation in the independent variable.
It is the ratio of the between groups sum of squares to the total
sum of squares.</term>
<term termid="euclidean_distance_4"><linktext>Euclidean distance</linktext
>The distance between two points that is computed by joining them
with a straight line.</term>
<term termid="euclidean_distance_measure_binary_data"><linktext>Euclidean
Distance Measure (Binary Data)</linktext>A version of the Euclidean
dissimilarity coefficient for binary data. Computed from a fourfold
table as SQRT(b+c), where b and c represent cases present on one item
but absent on the other.</term>
<term termid="euclidean_distance_measure_interval_data"><linktext
>Euclidean Distance Measure (Interval Data)</linktext>A dissimilarity
measure for continuous data. The distance between two items is the
square root of the sum of the squared differences in values for each
variable.</term>
<term termid="event"><linktext>Event</linktext>The number of subjects
where the event of interest (defined by the status variable) has occurred.</term>
<term termid="event_time"><linktext>Event time</linktext>Ordered observed
event times.</term>
<term termid="events"><linktext>Events</linktext>The number of cases
in which the event of interest occurred.</term>
<term termid="events_variable"><linktext>Events/Trials</linktext>When
the response is a number of events occurring in a set of trials, the
dependent variable contains the number of events and you can select
an additional variable containing the number of trials. Alternatively,
if the number of trials is the same across all subjects, then trials
may be specified using a fixed number of trials.</term>
<term termid="exact"><linktext>Exact</linktext>The probability of
the observed outcome or an outcome more extreme is calculated exactly.
Typically, a significance level less than 0.05 is considered significant,
indicating that there is some relationship between the row and column
variables.</term>
<term termid="exact_2_x_1-tailed_significance"><linktext>Exact 2 x
1-Tailed Significance</linktext>The exact probability of obtaining
results as extreme as the one observed, and in either direction when
the null hypothesis is true. This significance level tests a null
hypothesis in which the direction of an effect is not specified in
advance. Calculated by multiplying the one-tailed significance level
by two.</term>
<term termid="exact_maximum-likelihood"><linktext>Exact maximum-likelihood</linktext
>An estimation technique that for a given model and set of data finds
the parameter estimates that are "most likely" to have produced the
observed data. This method can handle missing data within the series
and can be used when one of the independent variables is the lagged
dependent variable.</term>
<term termid="exact_number_of_runs"><linktext>Exact Number of Runs</linktext
>When there are no tied observations the exact number of runs is displayed.
This value is used to calculate the test statistic.</term>
<term termid="exact_significance"><linktext>Exact Significance</linktext
>The significance level based on the exact distribution of a test
statistic. When the data set is small, sparse, contains many ties,
is unbalanced, or is poorly distributed, it is preferable to calculate
the significance level based on the exact distribution.</term>
<term termid="exactly"><linktext>Exactly</linktext>Selects a random
sample of the specified number of cases from the specified total number
of cases. If the total number of cases specified exceeds the total
number of cases in the data file, the sample will contain proportionally
fewer cases than the requested number.</term>
<term termid="exclude_cases_analysis_by_analysis"><linktext>Exclude
Cases Analysis by Analysis</linktext>Excludes cases that have missing
values for the variable involved in that test.</term>
<term termid="exclude_cases_listwise"><linktext>Exclude Cases Listwise</linktext
>Excludes cases that have missing values for any of the variables
used in any of the analyses.</term>
<term termid="exclude_cases_pairwise"><linktext>Exclude Cases Pairwise</linktext
>Excludes from analysis cases with missing values for either or both
of the pair of variables in computing a specific statistic.</term>
<term termid="excluded"><linktext>Excluded</linktext>Lists the number
and percentage of cases excluded from this analysis.</term>
<term termid="excluded_cases"><linktext>Excluded Cases</linktext>Cases
that are excluded due to missing values.</term>
<term termid="excluded_variables_1"><linktext>Excluded Variables</linktext
>Information is provided about the variables not in the equations.
Use it to track what variables are removed and if there are any closely
contending variables that you might prefer to use instead.</term>
<term termid="exhaustive_chaid"><linktext>Exhaustive CHAID</linktext
>A modification of CHAID that examines all possible splits for each
predictor.</term>
<term termid="exp_b"><linktext>Exp (B)</linktext>e (2.718) raised
to the value of the regression coefficient. This is the value by which
the odds of the event change when the ith independent variable increases
by one unit. If the value is greater than 1, the odds are increased;
if the value is less than 1, the odds are decreased. A value of 1
leaves the odds unchanged.</term>
<term termid="exp_b_1"><linktext>Exp(B)</linktext>The natural logarithm
raised to the parameter value.</term>
<term termid="exp_numexpr"><linktext>EXP(numexpr)</linktext>EXP(numexpr).
Numeric. Returns e raised to the power numexpr, where e is the base
of the natural logarithms and numexpr is numeric. Large values of
numexpr may produce results that exceed the capacity of the machine.</term>
<term termid="expected_cell_frequency"><linktext>Expected Cell Frequency</linktext
>The number of observations expected in a cell if the row and column
variables are independent. The probability of a case falling in a
given row is computed as the proportion of cases in the total sample
falling in that row. The probability of a case falling in a given
column is computed as the proportion of cases in the total sample
falling in that column. The probability of a case falling into a given
cell under the assumption of independence is the product of its row
and column probabilities. The expected cell frequency is the probability
of a case falling into that cell multiplied by the sample size.</term>
<term termid="expected_count"><linktext>Expected Count</linktext>The
number of cases that would be expected in the cell if the row and
column variables are statistically independent or unrelated to one
another.</term>
<term termid="expected_n"><linktext>Expected N</linktext>The number
of cases expected if the null hypothesis is true.</term>
<term termid="expected_responses"><linktext>Expected Responses</linktext
>The expected responses column reports the number of cases you would
expect to see in the cell if the model is correct.</term>
<term termid="expected_values"><linktext>Expected Values</linktext
>The number or percentage of cases expected in an interval if a variable
is normally distributed.</term>
<term termid="experimental"><linktext>Experimental</linktext>Evaluates
factor main effects before factor interactions, adjusting effects
of each kind for all other effects of that kind and for previously-evaluated
kinds.</term>
<term termid="experimental_group"><linktext>Experimental Group</linktext
>The group compared to the control group to see if the groups are
effected in the same way. The experimental group was defined by the
group 2 value in the Define Groups dialog box.</term>
<term termid="exploded_pie_chart"><linktext>Exploded Pie Chart</linktext
>A pie chart in which one (or more) sector is detached from the rest
of the pie for emphasis.</term>
<term termid="exponential_model"><linktext>Exponential Model</linktext
>Model whose equation is Y = b0 * (e**(b1 * t)) or ln(Y) = ln(b0)
+ (b1 * t).</term>
<term termid="exsm_texp"><linktext>Exponential Trend (Exponential
Smoothing)</linktext>The mean level of the series increases or decreases
exponentially with time.</term>
<term termid="exponential_distribution"><linktext>exponential_distribution</linktext
>The exponential distribution takes one scale parameter, which can
represent the rate of decay and must be positive. The exponential
distribution is a special case of the gamma distribution. A major
use of this distribution is life testing.</term>
<term termid="extraction"><linktext>Extraction</linktext>Allows you
to choose an extraction method, obtain a scree plot, control how many
factors are extracted, or specify the maximum iterations for convergence.</term>
<term termid="extraction_communality"><linktext>Extraction communality</linktext
>Communalities for the variables given the (unrotated) solution. The
communality is the squared multiple correlation for the variable using
the factors as predictors.</term>
<term termid="extraction_sums_of_squared_loadings"><linktext>Extraction
Sums of Squared Loadings</linktext>Sums of Squared Loadings for the
unrotated factor solution, which gives the variance accounted for
by each factor (or component)</term>
<term termid="extremes"><linktext>Extremes</linktext>Lists the 5 lowest
and 5 highest observations.</term>
<term termid="gbox02"><linktext>Extremes (Box plots)</linktext>Cases
with values more than 3 box lengths from the upper or lower edge of
the box. The box length is the interquartile range.</term>
<term termid="f"><linktext>F</linktext>Tests if the variances are
equal. When the F value is large and the significance level is small
(smaller than say 0.10, 0.05, or 0.01) the hypothesis of equal variances
can be rejected.</term>
<term termid="f_7"><linktext>F</linktext>The F statistic for the associated
test.  An F statistic has two associated degrees of freedom; the "numerator",
df1, and "denominator", df2.</term>
<term termid="f_change"><linktext>F Change</linktext>Statistic used
for testing the null hypothesis that inclusion of an additional variable
does not result in a significant increase in R-squared.</term>
<term termid="f_for_pairwise_distances"><linktext>F for Pairwise Distances</linktext
>Displays a matrix of pairwise F ratios for each pair of groups. The
F's are for significance tests for Mahalanobis distances between groups.</term>
<term termid="f_test"><linktext>F Test</linktext>Displays a repeated
measures analysis-of-variance table.</term>
<term termid="f_test_with_true_value"><linktext>F Test with True Value</linktext
>A test of the null hypothesis that the value of the intraclass correlation
is equal to the given "true" value.</term>
<term termid="f_value_1"><linktext>F value</linktext>The ratio of
between-cluster variance to within-cluster variance. Large values
indicate that this variable is an important contributor to the classification
of cases. This should not be interpreted as a traditional F statistic,
however, since the cluster algorithm is designed to maximize group
differences.</term>
<term termid="f_value"><linktext>F Value</linktext>The ratio of two
mean squares. When the F value is large and the significance level
is small (typically smaller than 0.05 or 0.01) the null hypothesis
can be rejected. In other words, a small significance level indicates
that the results probably are not due to random chance.</term>
<term termid="fa1_diagonal_offset"><linktext>FA1 diagonal offset</linktext
>The homogenous diagonal offset term for the First Order Factor Analytic
with constant diagonal offset covariance structure.</term>
<term termid="fa1_lambda_1"><linktext>FA1 lambda ^1</linktext>The
heterogeneous diagonal term used to compute covariances for the First
Order Factor Analytic with constant diagonal offset covariance structure.</term>
<term termid="factor_14"><linktext>Factor</linktext>Factors are composites
of the observed variables which can be thought of as underlying or
latent variables which account for relationships among the observed
variables.</term>
<term termid="factor"><linktext>Factor</linktext>An independent variable
defining groups of cases.</term>
<term termid="factor_16"><linktext>Factor</linktext>Factor variables
are the independent variables used in a conjoint analysis.</term>
<term termid="factor_levels"><linktext>Factor Levels</linktext>The
categories in each between-subjects factor.</term>
<term termid="factor_levels_together"><linktext>Factor Levels Together</linktext
>For each dependent variable, box plots for each group are displayed
side by side. You can easily compare how the values of the dependent
variable vary across groups. If no factor variable is selected, only
a box plot for the total sample is shown.</term>
<term termid="factor_list"><linktext>Factor List</linktext>Produces
separate analyses for groups of cases, based on their values for one
or more numeric or short string variable. If you select more than
one factor variable, separate summaries of each dependent variable
are produced for each factor variable.</term>
<term termid="factor_loading_plot"><linktext>Factor Loading Plot</linktext
>Three-dimensional factor loading plot of the first three factors.
For a two-factor solution, a two-dimensional plot is shown. The plot
is not displayed if only one factor is extracted. Plots display rotated
solutions if rotation is requested.</term>
<term termid="gfactorvar"><linktext>Factor Variable</linktext>Defines
groups of cases, typically to test whether the mean of a dependent
variable is the same within each group.</term>
<term termid="factor_s"><linktext>Factor(s)</linktext>Lists one or
more categorical variables, or factors which split your file into
two or more groups.</term>
<term termid="factors"><linktext>Factors</linktext>Independent variables
defining groups of cases, typically to test whether the mean of a
dependent variable is the same within each group.</term>
<term termid="factors_and_covariates"><linktext>Factors and Covariates</linktext
>Displays goodness-of-fit statistics for the factors and the covariates.</term>
<term termid="fah1_diagonal_offset_1"><linktext>FAH1 diagonal offset
^1</linktext>The heterogeneous diagonal offset term for the First
Order Factor Analytic with constant diagonal offset covariance structure.</term>
<term termid="fah1_lambda_1"><linktext>FAH1 lambda ^1</linktext>The
heterogeneous diagonal term used to compute covariances for the First
Order Factor Analytic with heterogeneous diagonal offset covariance
structure.</term>
<term termid="fiducial_confidence_intervals"><linktext>Fiducial Confidence
Intervals</linktext>Confidence intervals for the dosage of agent required
to produce a certain probability of response.</term>
<term termid="file_contents"><linktext>File Contents</linktext>Contents
of the data file</term>
<term termid="file_is_already_sorted_on_break_variable_s"><linktext
>File is already sorted on break variable(s)</linktext>If the data
have already been sorted by values of the break variables, this option
enables the procedure to run more quickly and use less memory. Use
this option with caution.</term>
<term termid="files_saved"><linktext>Files Saved</linktext>This is
the name of the matrix file to which category quantifications were
written.</term>
<term termid="final_fitted_model"><linktext>Final (fitted model)</linktext
>The user-specified model.  If the difference in the fitted and null
or intercept model -2 log likelihoods is large, then you can reject
the hypothesis that all the parameter coefficients are 0.</term>
<term termid="final_absolute_difference"><linktext>Final Absolute
Difference</linktext>This is the maximum absolute change in expected
cell frequency at the final iteration.</term>
<term termid="final_parameters_only"><linktext>Final parameters only</linktext
>Displays the final parameter estimates and goodness-of-fit statistics.</term>
<term termid="final_relative_difference"><linktext>Final Relative
Difference</linktext>This is the maximum relative change in expected
cell frequency at the final iteration.</term>
<term termid="aicc_1"><linktext>Finite Sample Corrected AIC (AICC)</linktext
>A measure for selecting and comparing models based on the -2 log
likelihood. Smaller values indicate better models.  The AICC "corrects"
the AIC for small sample sizes.  As the sample size increases, the
AICC converges to the AIC.</term>
<term termid="first_4"><linktext>First</linktext>Displays the first
data value encountered in the data file.</term>
<term termid="first_value_aggregate_function"><linktext>First Value
(Aggregate Function)</linktext>The value of the source variable for
the first case in the break group. </term>
<term termid="first-order_autoregressive"><linktext>First-Order Autoregressive</linktext
>This structure has constant variance. The correlation between any
two elements is equal to rho for adjacent elements, rho*rho for elements
that are separated by a third, and so on. rho is constrained so that
-1&lt;rho&lt;1.</term>
<term termid="first-order_factor_analytic_const_diag"><linktext>First-Order
Factor Analytic, Const Diag</linktext>This covariance structure has
heterogeneous variances that are comprised of a term that is heterogeneous
across elements and a term that is homogenous across elements.  The
covariance between any two elements is the square root of the product
of their heterogeneous variance terms.</term>
<term termid="first-order_factor_analytic_heter_diag"><linktext>First-Order
Factor Analytic, Heter Diag</linktext>This covariance structure has
heterogeneous variances that are comprised of two terms that are heterogeneous
across elements.  The covariance between any two elements is the square
root of the product of the first of their heterogeneous variance terms.</term>
<term termid="first-order-ante-dependence"><linktext>First-Order-Ante-Dependence</linktext
>This covariance structure has heterogeneous variances and heterogeneous
correlations between adjacent elements.  The correlation between two
non-adjacent elements is the product of the correlations between the
elements that lie between the elements of interest.</term>
<term termid="fishers"><linktext>Fisher's</linktext>Displays Fisher's
classification function coefficients that can be used directly for
classification. A separate set of classification function coefficients
is obtained for each group, and a case is assigned to the group for
which it has the largest discriminant score (classification function
value).</term>
<term termid="fishers_exact_test"><linktext>Fisher's Exact Test</linktext
>A test for independence in a 2 X 2 table. It is most useful when
the total sample size and the expected values are small.</term>
<term termid="fixed_coordinates"><linktext>Fixed Coordinates</linktext
>The number of user-provided fixed coordinates.  The fixed coordinates
plus free coordinates should equal the dimensions times the number
of objects.</term>
<term termid="fixed_effects"><linktext>Fixed Effects</linktext>The
model parameters associated with the fixed effects.</term>
<term termid="fixed-effects_model"><linktext>Fixed-Effects Model</linktext
>A model in which the groups in the sample constitute all groups of
interest. That is, the groups are not considered random samples from
some larger population of groups.</term>
<term termid="flag_significant_correlations"><linktext>Flag Significant
Correlations</linktext>Flags correlation coefficients significant
at the 0.05 level with a single asterisk, and those significant at
the 0.01 level with two asterisks.</term>
<term termid="footnotes"><linktext>Footnotes</linktext>To hide a footnote,
select the footnote and from the View menu choose Hide. Alternately,
right click the footnote and choose Hide.</term>
<term termid="for_cohort"><linktext>For Cohort</linktext>Displays
the estimate of relative risk for the defined event.</term>
<term termid="for_each_observation"><linktext>For Each Observation</linktext
>Specifies whether values for variables in a multi-variable plot are
joined with vertical lines.</term>
<term termid="for_each_stratum"><linktext>For Each Stratum</linktext
>Performs a separate test of equality of all factor levels for each
stratum. If you do not have a stratification variable, the tests are
not performed.</term>
<term termid="forecast"><linktext>Forecast</linktext>Using an estimated
model to predict future series values.</term>
<term termid="forecast_1"><linktext>Forecast</linktext>The forecasted
value. Any empty values are caused by the fact that the forecast period
can vary by model, and simply mean that a forecast value does not
apply for the associated period.</term>
<term termid="forecasting_error"><linktext>Forecasting Error</linktext
>The discrepancy between an observed value and its forecast, based
on a specified model.</term>
<term termid="format"><linktext>Format</linktext>Allows you to change
the format of frequency tables.</term>
<term termid="format_2"><linktext>Format</linktext>Lists the formats
available in the selected category.</term>
<term termid="format_1"><linktext>Format</linktext>Allows you to modify
the row order.</term>
<term termid="format_file_information"><linktext>Format (file information)</linktext
>Variable format (e.g., F8.2, F2, A8, DATE10)</term>
<term termid="forward_selection"><linktext>Forward Selection</linktext
>A stepwise variable selection procedure in which variables are sequentially
entered into the model. The first variable considered for entry into
the equation is the one with the largest positive or negative correlation
with the dependent variable. This variable is entered into the equation
only if it satisfies the criterion for entry. If the first variable
is entered, the independent variable not in the equation that has
the largest partial correlation is considered next. The procedure
stops when there are no variables that meet the entry criterion.</term>
<term termid="forward_selection_conditional"><linktext>Forward Selection
(Conditional)</linktext>Stepwise selection method with entry testing
based on the significance of the score statistic, and removal testing
based on the probability of a likelihood-ratio statistic based on
conditional parameter estimates.</term>
<term termid="forward_selection_likelihood_ratio"><linktext>Forward
Selection (Likelihood Ratio)</linktext>Stepwise selection method with
entry testing based on the significance of the score statistic, and
removal testing based on the probability of a likelihood-ratio statistic
based on the maximum partial likelihood estimates.</term>
<term termid="forward_selection_wald"><linktext>Forward Selection
(Wald)</linktext>Stepwise selection method with entry testing based
on the significance of the score statistic, and removal testing based
on the probability of the Wald statistic.</term>
<term termid="fraction_above_aggregate_function"><linktext>Fraction
Above (Aggregate Function)</linktext>The proportion of the cases in
the break group whose value for the source variable is greater than
the specified value.</term>
<term termid="fraction_below_aggregate_function"><linktext>Fraction
Below (Aggregate Function)</linktext>The proportion of the cases in
the break group whose value for the source variable is less than the
specified value.</term>
<term termid="fraction_inside_aggregate_function"><linktext>Fraction
Inside (Aggregate Function)</linktext>The proportion of the cases
in the break group whose value for the source variable is greater
than or equal to the specified Low value, and less than or equal to
the specified High value.</term>
<term termid="fraction_outside_aggregate_function"><linktext>Fraction
Outside (Aggregate Function)</linktext>The proportion of the cases
in the break group whose value for the source variable is less than
the specified Low value or greater than the specified High value.</term>
<term termid="fractional_rank_rank_cases"><linktext>Fractional Rank
(Rank Cases)</linktext>The value of the new variable equals rank divided
by the sum of the weights of the nonmissing cases.</term>
<term termid="fractional_rank_as_percent_rank_cases"><linktext>Fractional
Rank As Percent (Rank Cases)</linktext>Each rank is divided by the
number of cases with valid values and multiplied by 100.</term>
<term termid="fractional_rank_estimation_method"><linktext>Fractional
Rank Estimation Method</linktext>Specifies the formula to be used
in estimating the empirical distribution in p-p plots and calculating
the expected quantile values in q-q plots.</term>
<term termid="free_coordinates"><linktext>Free Coordinates</linktext
>The number of coordinates that are not fixed by the user.  The fixed
coordinates plus free coordinates should equal the dimensions times
the number of objects.</term>
<term termid="frequencies_as_vertical_axis"><linktext>Frequencies
as Vertical Axis</linktext>Displays bar charts of frequencies (counts,
the number of cases in each category).</term>
<term termid="frequency_4"><linktext>Frequency</linktext>The count
or number of cases/objects having the same value of a variable.</term>
<term termid="frequency"><linktext>Frequency</linktext>The count or
number of cases having each value (or range of values) of a variable.</term>
<term termid="frequency_5"><linktext>Frequency</linktext>Numbers of
cases, by covariate pattern.</term>
<term termid="frequency_table"><linktext>Frequency Table</linktext
>Crosstabulation of the row and column variables. The table includes
the number of cases in each cell and the row and column marginal totals.</term>
<term termid="frequency_tables"><linktext>Frequency Tables</linktext
>Summary of the number of times different values of a variable occur.</term>
<term termid="friedman_chi-square"><linktext>Friedman Chi-Square</linktext
>Displays Friedman's chi-square and Kendall's coefficient of concordance.
This option is appropriate for data that are in the form of ranks.
The chi-square test replaces the usual F test in the ANOVA table.</term>
<term termid="friedman_test"><linktext>Friedman Test</linktext>Tests
the null hypothesis that k related variables come from the same population.
For each case, the k variables are ranked from 1 to k. The test statistic
is based on these ranks.</term>
<term termid="friedmans_chi-square"><linktext>Friedman's Chi-Square</linktext
>Friedman's chi-square is used to test the hypothesis that the items
do not account for a significant portion of the within-person variability.</term>
<term termid="f-test_of_hotellings_t_statistic"><linktext>F-Test of
Hotelling's T2 Statistic</linktext>Hotelling's T-squared statistic
is compared to an F distribution with df1=(# items-1) and df2=(weighted
sum of cases - # items + 1).  If the significance value of the test
is less than 0.05, you must reject the null hypothesis that all items
in the scale have the same mean.</term>
<term termid="f-to-enter"><linktext>F-to-Enter</linktext>The variable
with the largest F-to-enter at each step enters the model.</term>
<term termid="f-to-remove"><linktext>F-to-Remove</linktext>The F-to-remove
statistic for the entering variable is the same as its F-to-enter
statistic at the previous step. F-to-remove and Wilks' lambda are
useful for describing what happens if the variable is removed from
the current model (given that the other variables remain).</term>
<term termid="gman_full"><linktext>Full Factorial Model</linktext
>A model that includes main effects for all factors and all possible
interactions between factors.</term>
<term termid="function_6"><linktext>Function</linktext>Labels the
discriminant functions by number.</term>
<term termid="function_rank_cases"><linktext>Function (Rank Cases)</linktext
>The ranking method used to assign values to the new variable.</term>
<term termid="function_precision"><linktext>Function Precision</linktext
>This is a measure of the accuracy with which the objective function
can be measured. It acts as a relative precision when the function
is large, and an absolute precision when the function is small. To
override the default value, select an alternate precision value from
the drop-down list or select disable to disable this setting.</term>
<term termid="functions_dropped"><linktext>Functions Dropped</linktext
>Lists the functions by number that have been dropped.</term>
<term termid="gabriel_post_hoc"><linktext>Gabriel (Post Hoc)</linktext
>Pairwise comparison test that used the Studentized maximum modulus
and is generally more powerful than Hochberg's GT2 when the cell sizes
are unequal. Gabriel's test may become liberal when the cell sizes
vary greatly.</term>
<term termid="gain"><linktext>Gain</linktext>The quotient of dividing
the cross amplitude by the spectral density for one of the series.
Each of the two series has its own gain value.</term>
<term termid="gain_tree"><linktext>Gain</linktext>The number of cases
in the node that belong to the target category and the percentage
of total cases in the target category in that node.</term>
<term termid="games-howell_post_hoc"><linktext>Games-Howell (Post
Hoc)</linktext>Pairwise comparison test that is sometimes liberal.
This test is appropriate when the variances are unequal.</term>
<term termid="gamma"><linktext>Gamma</linktext>A symmetric measure
of association between two ordinal variables that ranges between -1
and 1. Values close to an absolute value of 1 indicate a strong relationship
between the two variables. Values close to 0 indicate little or no
relationship. For 2-way tables, zero-order gammas are displayed. For
3-way to n-way tables, conditional gammas are displayed.</term>
<term termid="general_model_test_of_parallel_slopes"><linktext>General
model (test of parallel slopes)</linktext>The general model allows
slopes to differ across categories</term>
<term termid="generalized_least-squares_method"><linktext>Generalized
Least-Squares Method</linktext>A factor extraction method that minimizes
the sum of the squared differences between the observed and reproduced
correlation matrices. Correlations are weighted by the inverse of
their uniqueness, so that variables with high uniqueness are given
less weight than those with low uniqueness.</term>
<term termid="generalized_odds_ratio"><linktext>Generalized Odds Ratio</linktext
>An extension of the odds ratio for tables larger than 2 x 2.  The
logarithm of this measure, the generalized log-odds ratio (or GLOR),
is easier to work with in terms of hypothesis testing.</term>
<term termid="generalized_score"><linktext>Generalized Score Statistic</linktext
>For models which do not have a likelihood function, this provides
a rough measure of how well the model fits the data.</term>
<term termid="generating_class"><linktext>Generating Class</linktext
>The highest-order terms in which variables appear in a hierarchical
loglinear model. The generating class (A*B, C) corresponds to a model
which contains the main effects A, B, and C, and the A by B interaction
term.</term>
<term termid="geometric_mean"><linktext>Geometric Mean</linktext>The
nth root of the product of the data values, where n represents the
number of cases.</term>
<term termid="get_from_data_1"><linktext>Get From Data</linktext>Assigns
cases with the lower category of a dichotomous variable to one group
and cases with the higher category to the other group.</term>
<term termid="get_from_data"><linktext>Get From Data</linktext>Each
distinct value encountered is defined as a category.</term>
<term termid="goodman_and_kruskals_tau"><linktext>Goodman and Kruskal's
Tau</linktext>A measure of association which reflects the proportional
reduction in error when values of the independent variable are used
to predict values of the dependent variable. Values range from 0 to
1. Unlike lambda, where the modal category is used for predictions,
tau uses the marginal proportions.</term>
<term termid="goodness_of_fit_2"><linktext>Goodness of Fit</linktext
>Larger values of goodness-of-fit statistics indicate better models.</term>
<term termid="goodness_of_fit"><linktext>Goodness of Fit</linktext
>A measure of how well the model fits the data. It is based on the
squared differences between the observed and predicted probabilities.
A small observed significance level for the goodness-of-fit statistic
indicates that the model does not fit well.</term>
<term termid="gradient_and_hessian"><linktext>Gradient and Hessian</linktext
>The product of the gradient vector and the generalized inverse of
the Hessian matrix determine the initial size and direction of the
"step" the algorithm takes at a given iteration.  If step-halving
is in effect, the size of the step is reduced by the step-halving
factor.</term>
<term termid="grand_mean"><linktext>Grand Mean</linktext>The mean
of the dependent variable calculated for all the cases included in
the analysis.</term>
<term termid="greater_than_median"><linktext>Greater Than Median</linktext
>Lists the number of observations that are greater than the median.</term>
<term termid="greater_than_or_equal_to_varvalue"><linktext>Greater
Than or Equal to Varvalue</linktext>Displays information about the
observations that are greater than or equal to the cut point value.</term>
<term termid="greenhouse-geisser_epsilon"><linktext>Greenhouse-Geisser
Epsilon</linktext>An adjustment used in univariate repeated measures
when the sphericity assumption is violated. Both numerator and denominator
degrees of freedom must be multiplied by epsilon, and the significance
of the F ratio evaluated with the new degrees of freedom. It tends
to be overly conservative for small sample sizes.</term>
<term termid="grid_lines_charts"><linktext>Grid Lines (Charts)</linktext
>Lines that are positioned similarly to tick marks, but extend through
the plot. Grid lines often make it easier to read data values. They
are especially useful to emphasize the use of a logarithmic scale.</term>
<term termid="grid_search"><linktext>Grid Search</linktext>The parameter
is assigned a starting value in the Start box, an increment value
in the By box, and an ending value in the Stop box. Enter these values
after selecting Grid Search. The ending value must be greater than
the starting value, and the increment value must be less than their
difference.</term>
<term termid="group_10"><linktext>group</linktext>A level of the grouping
variable.</term>
<term termid="group"><linktext>Group</linktext>Information is displayed
for the selected and unselected cases.</term>
<term termid="group_8"><linktext>Group</linktext>The second most likely
group that the case belongs to.</term>
<term termid="group_hosmer-lemeshow_goodness-of-fit_table"><linktext
>Group (Hosmer-Lemeshow goodness-of-fit table)</linktext>The decile
of risk described by each row of the table.</term>
<term termid="group_1"><linktext>Group 1</linktext>Lists information
about the observations in group 1.</term>
<term termid="group_1_and_group_2"><linktext>Group 1 and Group 2</linktext
>Enter integer values for Group 1 and Group 2. Cases with other values
are excluded.</term>
<term termid="group_2"><linktext>Group 2</linktext>Lists information
about the observations in group 2.</term>
<term termid="group_means"><linktext>Group Means</linktext>The average
values calculated separately for each group in the analysis.</term>
<term termid="group_percentile"><linktext>Group Percentile</linktext
>The data value below which the specified percentage of values fall,
where the percentile is calculated for data that is coded into groups.
For example, with age data, if each value in the 30s is coded 35,
each value in the 40s is coded 45, and so on, the grouped percentile
is the percentile calculated from the coded data.</term>
<term termid="group_standard_deviations"><linktext>Group Standard
Deviations</linktext>The standard deviations of each variable calculated
separately for each group.</term>
<term termid="grouped_median"><linktext>Grouped Median</linktext>Median
that is calculated for data that is coded into groups. For example,
with age data, if each value in the 30s is coded 35, each value in
the 40s is coded 45, and so on, the grouped median is the median calculated
from the coded data.</term>
<term termid="grouping_variable_1"><linktext>Grouping Variable</linktext
>Displays the variable you have chosen for the analysis, which splits
your data file into two or more groups. After selecting a Grouping
Variable use Define Range to define the categories of your Grouping
Variable.</term>
<term termid="grouping_variable"><linktext>Grouping Variable</linktext
>Variable that divides cases into two groups for the test. The grouping
variable should be a dichotomous variable or a categorical variable
with groups you want to test.</term>
<term termid="groups_2"><linktext>Groups</linktext>Lists the groups
of observations.</term>
<term termid="groups"><linktext>Groups</linktext>Lists the fixed effects,
random effects, and total effects.</term>
<term termid="growing_method"><linktext>Growing method</linktext>The
growing method used to create the tree model.</term>
<term termid="growth_model"><linktext>Growth Model</linktext>Model
whose equation is Y = e**(b0 + (b1 * t)) or ln(Y) = b0 + (b1 * t).</term>
<term termid="guttman_split-half_coefficient"><linktext>Guttman Split-Half
Coefficient</linktext>A measure designed to answer the question: given
two parts of a scale, how reliable would the whole be if they were
combined into one? It is similar to the Spearman-Brown coefficient,
but it does not assume equal reliabilities or equal variances for
the two parts.</term>
<term termid="half-normal_plot"><linktext>Half-Normal Plot</linktext
>Very similar to the normal probability plot except that both positive
and negative values are treated identically. If the population correlation
matrix is an identity matrix, the plot should be fairly linear and
the line should pass through the origin.</term>
<term termid="hamann"><linktext>Hamann</linktext>Similarity measure
for binary data. Gives the probability that a characteristic has the
same state in both items (present in both or absent from both) minus
the probability that a characteristic has different states in the
two items (present in one and absent from the other). Has a range
of -1 to +1. Computed from a fourfold table as ((a+d)-(b+c))/(a+b+c+d)
where a represents the cell corresponding to cases present on both
items, d the cell corresponding to cases absent on both items, and
b and c represent the diagonal cells of the table corresponding to
cases present on one item but absent on the other.</term>
<term termid="hamann_similarity_measure"><linktext>Hamann Similarity
Measure</linktext>Similarity measure for binary data. Gives the probability
that a characteristic has the same state in both items (present in
both or absent from both) minus the probability that a characteristic
has different states in the two items (present in one and absent from
the other). Computed from a fourfold table as ((a+d)-(b+c))/(a+b+c+d)
where a represents cases present on both items, d represents cases
absent on both items, and b and c represent cases present on one item
but absent on the other.</term>
<term termid="hampels_redescending_m-estimator"><linktext>Hampel's
redescending M-estimator</linktext>A three-part redescending M-estimator
that is characterized by three constants (a,b,c). Standardized observed
values with an absolute value greater than c are assigned a weight
of zero. Values between 0 and a are assigned a weight of 1, while
values between a and b and between b and c are assigned weights that
depend on their distance from zero.</term>
<term termid="harmonic_mean"><linktext>Harmonic Mean</linktext>Used
to estimate an average group size when the sample sizes in the groups
are not equal. The harmonic mean is the total number of samples divided
by the sum of the reciprocals of the sample sizes.</term>
<term termid="harmonic_mean_1"><linktext>harmonic mean</linktext>The
reciprocal of the arithmetic mean of the reciprocals.</term>
<term termid="hartleys_f_max"><linktext>Hartley's F Max</linktext
>Homogeneity-of-variance test that is based on the ratio of the largest
variance to the smallest variance. It is sensitive to departures from
normality. Also called Hartley's largest F.</term>
<term termid="haverage"><linktext>Haverage</linktext>The pth percentile
is estimated using a weighted average around the (n+1)-st data point.</term>
<term termid="hazard_k-m"><linktext>Hazard</linktext>Cumulative hazard
function estimate. The default variable name is the prefix haz_ with
a sequential number appended to it. For example, if haz_1 already
exists, Kaplan-Meier assigns the variable name haz_2.</term>
<term termid="hazard_save"><linktext>Hazard Function</linktext>Saves
the cumulative hazard function estimate (also called the Cox-Snell
residual).</term>
<term termid="hazard_plot"><linktext>Hazard Plot</linktext>Displays
the cumulative hazard function on a linear scale.</term>
<term termid="hazard_rate"><linktext>Hazard Rate</linktext>An estimate
of the risk of experiencing the terminal event during the interval,
conditional upon surviving to the start of the interval.</term>
<term termid="def_heading"><linktext>Heading (output item type)</linktext
>The item under the root item. By default, it is labeled by the procedure
name. The user can cut, copy, paste or delete a heading with all items
under it, and can expand or collapse it. A heading item exists only
in the outline. The user can change its label text but cannot activate
it for further editing.</term>
<term termid="helmert_contrast_1"><linktext>Helmert Contrast</linktext
>Compares the mean of each level of the factor (except the last) to
the mean of subsequent levels.</term>
<term termid="helmert_contrasts"><linktext>Helmert Contrasts</linktext
>In a Helmert contrast, the effect for each category of the predictor
variable or factor (except the last) is compared to the mean effect
of subsequent categories.</term>
<term termid="help"><linktext>Help</linktext>Provides context-sensitive
help for this dialog box.</term>
<term termid="heterogeneous_1st_order_autoregressive"><linktext>Heterogeneous
1st Order Autoregressive</linktext>This covariance structure has heterogeneous
variances.  The correlation between any two elements is equal to rho
for adjacent elements, rho^2 for two elements separated by a third,
and so on.  Rho is constrained to lie between -1 and 1.</term>
<term termid="heterogeneous_compound_symmetry"><linktext>Heterogeneous
Compound Symmetry</linktext>This covariance structure has heterogeneous
variances and constant correlation between elements.</term>
<term termid="heterogeneous_toeplitz"><linktext>Heterogeneous Toeplitz</linktext
>This covariance structure has heterogeneous variances and heterogeneous
correlations between elements.  The correlation between adjacent elements
is homogenous across pairs of adjacent elements.  The correlation
between elements separated by a third is again homogenous, and so
on.</term>
<term termid="hf_lambda"><linktext>HF: lambda</linktext>The covariance
between any two elements of a Huynh-Feldt covariance matrix is equal
to the average of their variances minus this parameter.</term>
<term termid="hidden_layer"><linktext>Hidden Layers</linktext>The
hidden layer contains unobservable network nodes (units).  Each hidden
unit is a function of the weighted sum of the inputs. The function
is the activation function, and the values of the weights are determined
by the estimation algorithm.   If the network contains a second hidden
layer, each hidden unit in the second layer is a function of the weighted
sum of the units in the first hidden layer. The same activation function
is used in both layers.</term>
<term termid="hidden_layer_rbf"><linktext>Hidden Layer (RBF)</linktext
>The hidden layer contains unobservable network nodes (units).  Each
hidden unit is a function of the inputs. The functions are the radial
basis functions, and the values of the function parameters (center
and width) are determined by the estimation algorithm. </term>
<term termid="hidden_unit_variance"><linktext>Hidden Unit Variance
(RBF)</linktext>Each radial basis function has a center and width
(variance).  A separate center is computed for each hidden unit for
each input unit.  A separate variance is computed for each hidden
unit.</term>
<term termid="hide_empty_rows_and_columns"><linktext>Hide Empty Rows
and Columns</linktext>Hides rows and columns that have nothing in
any data cells.</term>
<term termid="hierarchical"><linktext>Hierarchical</linktext>Resembles
the experimental approach, except that factor main effects are adjusted
only for the effect of the factors that precede them in the Factor(s)
list.</term>
<term termid="highest"><linktext>Highest</linktext>Largest values.</term>
<term termid="highest_group"><linktext>Highest group</linktext>Statistics
concerning the most strongly predicted group membership for each case,
based on the discriminant function(s)</term>
<term termid="high-low-close_chart"><linktext>High-Low-Close Chart</linktext
>Displays bars that represent two or three values for a sequence of
categories. Simple charts display a single high-low-close bar for
each category, variable, or case on the category axis. The classical
use of these is to display daily or weekly high, low, and closing
prices of a commodity or security.</term>
<term termid="histogram"><linktext>Histogram</linktext>Creates a histogram
of the standardized residuals with a normal curve superimposed.</term>
<term termid="histogram_2"><linktext>histogram</linktext>A histogram
displays the distribution of a quantitative variable by showing the
relative concentration of data points along different intervals or
sections of the scale on which the data are measured.</term>
<term termid="ghistogram"><linktext>Histogram</linktext>A histogram
displays the distribution of a quantitative variable by showing the
relative concentration of data points along different intervals or
sections of the scale on which the data are measured.</term>
<term termid="histogram_s_with_normal_curve_frequencies"><linktext
>Histogram(s) With Normal Curve (Frequencies)</linktext>A graphical
display of the distribution of values. Histograms are appropriate
for continuous, quantitative variables (for example salary or age).
Selecting With normal curve, superimposes a normal curve onto the
histogram.</term>
<term termid="hochbergs_gt2_post_hoc"><linktext>Hochberg's GT2 (Post
Hoc)</linktext>Multiple comparison and range test that uses the Studentized
maximum modulus. Similar to Tukey's honestly significant difference
test.</term>
<term termid="holdout_partition"><linktext>Holdout</linktext>The holdout
sample is an independent set of data records used to assess the final
model.</term>
<term termid="gexsmooth02"><linktext>Holt Model (Exponential Smoothing)</linktext
>The Holt model assumes that the series has a linear trend and no
seasonal variation.</term>
<term termid="holts_linear_trend"><linktext>Holt's linear trend</linktext
>This model is appropriate for series in which there is a linear trend
and no seasonality. Its smoothing parameters are level and trend,
which are not constrained by each other's values. Holt's model is
more general than Brown's model but may take longer to compute for
large series. Holt's exponential smoothing is most similar to an ARIMA
model with zero orders of autoregression, two orders of differencing,
and two orders of moving average.</term>
<term termid="homogeneity"><linktext>Homogeneity</linktext>This procedure
allows you to examine the relationship between two or more unordered
categorical (nominal) variables graphically in a multidimensional
scatterplot. Homogeneity analysis is similar to correspondence analysis,
but it allows you to examine more than two variables.</term>
<term termid="homogeneity-of-variance_tests_1"><linktext>Homogeneity-of-Variance
Tests</linktext>Tests for violations of the equal variance assumption
using the Levene Test.</term>
<term termid="homogeneity-of-variance_tests"><linktext>Homogeneity-of-Variance
Tests</linktext>Tests that the groups defined by an independent grouping
variable are taken from populations with the same variance. The three
tests produced are Hartley's F Max (maximum variance/minimum variance),
Cochran's C, and the Bartlett-Box F. All three require that the dependent
variable be normally distributed.</term>
<term termid="homogeneous_subsets"><linktext>Homogeneous Subsets</linktext
>Groups that have means which are not significantly different from
one another based on the specified multiple comparison procedure.
The maximum number of subsets is 50.</term>
<term termid="homoscedasticity"><linktext>Homoscedasticity</linktext
>Constant variance across time or among groups. Homoscedasticity is
required for many statistical methods.</term>
<term termid="horizontal_icicle_plot"><linktext>Horizontal Icicle
Plot</linktext>A figure used to summarize the formation of clusters
during a cluster analysis. Cases form the rows of the display; steps
form the columns. Cases joined vertically form clusters. This is especially
useful when there are a large number of cases, since a vertical icicle
plot might not fit across a single page.</term>
<term termid="hosmer-lemeshow_goodness-of-fit_statistic"><linktext
>Hosmer-Lemeshow goodness-of-fit statistic</linktext>This goodness-of-fit
statistic is more robust than the traditional goodness-of-fit statistic
used in logistic regression, particularly for models with continuous
covariates and studies with small sample sizes. It is based on grouping
cases into deciles of risk and comparing the observed probability
with the expected probability within each decile.</term>
<term termid="hotellings_t_square"><linktext>Hotelling's T Square</linktext
>A multivariate test of the null hypothesis that all items on the
scale have the same mean.</term>
<term termid="hotellings_trace"><linktext>Hotelling's Trace</linktext
>A multivariate test of significance based on the sum of eigenvalues.</term>
<term termid="hubers_m-estimator_explore"><linktext>Huber's M-Estimator
(Explore)</linktext>An M-estimator of location. Cases with standardized
values less than c receive a weight of 1. Those with larger absolute
values have weights that decrease as their distance from zero increases.</term>
<term termid="huynh-feldt"><linktext>Huynh-Feldt</linktext>This is
a "circular" matrix in which the covariance between any two elements
is equal to the average of their variances minus a constant. Neither
the variances nor the covariances are restricted to be constant.</term>
<term termid="huynh-feldt_epsilon"><linktext>Huynh-Feldt Epsilon</linktext
>An adjustment to the numerator and denominator degrees of freedom
that you may make if the sphericity assumption in a repeated measures
design appears to be violated. Numerator and denominator degrees of
freedom are multiplied by epsilon, and the adjusted degrees of freedom
are used in the computation of the observed significance level. The
Huynh-Feldt epsilon is an attempt to correct the Greenhouse-Geisser
epsilon which tends to be overly conservative, especially for small
sample sizes.</term>
<term termid="hypothesis"><linktext>Hypothesis</linktext>The effect
being tested.</term>
<term termid="hypothesized_value"><linktext>Hypothesized Value</linktext
>The value stipulated by the contrasts results matrix; the K in the
LB=K equation.</term>
<term termid="hypothesiszed_value"><linktext>Hypothesized value</linktext
>The value against which the contrast estimate is tested.</term>
<term termid="icicle"><linktext>Icicle plot (Cluster)</linktext>This
chart shows how cases are merged into clusters. At the bottom (right
for horizontal plots), no cases have been merged; as you read up the
chart (or to the right-to-left for horizontal plots), cases that are
merged are indicated by an X or bar in the column between them, whereas
different cluster are indicated by a white space between them.</term>
<term termid="id"><linktext>ID</linktext>The identification for a
cluster.</term>
<term termid="id_7"><linktext>ID</linktext>The case label given by
the user-specified ID variable.</term>
<term termid="id_8"><linktext>ID</linktext>Identifier used to number
the simulation cases from the plan file. If it exists, this is the
value of the CARD_ variable for the simulation case; otherwise, it
is the case number.</term>
<term termid="id_diagonal"><linktext>ID: diagonal</linktext>This parameter
defines the diagonal entries of a Scaled Identity covariance matrix.</term>
<term termid="ideal_point"><linktext>Ideal Point</linktext>For factors
specified as quadratic, this is the factor level representing either
the maximum preference (IDEAL models) or minimum preference (ANTIIDEAL
models). Distance from the ideal point in either direction is associated
with increased preference for ANTIIDEAL models and decreased preference
for IDEAL models.</term>
<term termid="identifier"><linktext>Identifier</linktext>The combination
of variable values that uniquely identifies a case.</term>
<term termid="identity_2"><linktext>Identity</linktext>This structure
has constant variance. There is assumed to be no correlation between
any elements.</term>
<term termid="identity_matrix"><linktext>Identity Matrix</linktext
>A diagonal matrix whose elements on the main diagonal are all 1's.
The dimension of the identity matrix is the same as the number of
dependent variables being studied.</term>
<term termid="idf.beta_prob_shape1_shape2"><linktext>IDF.BETA(prob,
shape1, shape2)</linktext>IDF.BETA(prob, shape1, shape2). Numeric.
Returns the value from the Beta distribution, with the given shape
parameters, for which the cumulative probability is prob.</term>
<term termid="idf.cauchy_prob_loc_scale"><linktext>IDF.CAUCHY(prob,
loc, scale)</linktext>IDF.CAUCHY(prob, loc, scale). Numeric. Returns
the value from the Cauchy distribution, with the given location and
scale parameters, for which the cumulative probability is prob.</term>
<term termid="idf.chisq_prob_df"><linktext>IDF.CHISQ(prob, df)</linktext
>IDF.CHISQ(prob, df). Numeric. Returns the value from the chi-square
distribution, with the specified degrees of freedom df, for which
the cumulative probability is prob. For example, the chi-square value
that is significant at the 0.05 level with 3 degrees of freedom is
IDF.CHISQ(0.95,3).</term>
<term termid="idf.exp_p_scale"><linktext>IDF.EXP(p, scale)</linktext
>IDF.EXP(p, scale). Numeric. Returns the value of an exponentially
decaying variable, with rate of decay scale, for which the cumulative
probability is p.</term>
<term termid="idf.f_prob_df1_df2"><linktext>IDF.F(prob, df1, df2)</linktext
>IDF.F(prob, df1, df2). Numeric. Returns the value from the F distribution,
with the specified degrees of freedom, for which the cumulative probability
is prob. For example, the F value that is significant at the 0.05
level with 3 and 100 degrees of freedom is IDF.F(0.95,3,100).</term>
<term termid="idf.gamma_prob_shape_scale"><linktext>IDF.GAMMA(prob,
shape, scale)</linktext>IDF.GAMMA(prob, shape, scale). Numeric. Returns
the value from the Gamma distribution, with the specified shape and
scale parameters, for which the cumulative probability is prob.</term>
<term termid="idf.halfnrm"><linktext>IDF.HALFNRM</linktext>IDF.HALFNRM(prob,
mean, stddev). Numeric. Returns the value from the half normal distribution,
with the specified mean and standard deviation, for which the cumulative
probability is prob.</term>
<term termid="idf.igauss"><linktext>IDF.IGAUSS</linktext>IDF.IGAUSS(prob,
loc, scale). Numeric. Returns the value from the inverse Gaussian
distribution, with the given location and scale parameters, for which
the cumulative probability is prob.</term>
<term termid="idf.laplace_prob_mean_scale"><linktext>IDF.LAPLACE(prob,
mean, scale)</linktext>IDF.LAPLACE(prob, mean, scale). Numeric. Returns
the value from the Laplace distribution, with the specified mean and
scale parameters, for which the cumulative probability is prob.</term>
<term termid="idf.lnormal_prob_a_b"><linktext>IDF.LNORMAL(prob, a,
b)</linktext>IDF.LNORMAL(prob, a, b). Numeric. Returns the value from
the log-normal distribution, with specified parameters, for which
the cumulative probability is prob.</term>
<term termid="idf.logistic_prob_mean_scale"><linktext>IDF.LOGISTIC(prob,
mean, scale)</linktext>IDF.LOGISTIC(prob, mean, scale). Numeric. Returns
the value from the logistic distribution, with specified mean and
scale parameters, for which the cumulative probability is prob.</term>
<term termid="idf.normal_prob_mean_stddev"><linktext>IDF.NORMAL(prob,
mean, stddev)</linktext>IDF.NORMAL(prob, mean, stddev). Numeric. Returns
the value from the normal distribution, with specified mean and standard
deviation, for which the cumulative probability is prob.</term>
<term termid="idf.pareto_prob_threshold_shape"><linktext>IDF.PARETO(prob,
threshold, shape)</linktext>IDF.PARETO(prob, threshold, shape). Numeric.
Returns the value from the Pareto distribution, with specified threshold
and scale parameters, for which the cumulative probability is prob.</term>
<term termid="idf.smod"><linktext>IDF.SMOD</linktext>IDF.SMOD(prob,
a, b). Numeric.  Returns the value from the Studentized maximum modulus,
with the specified parameters, for which the cumulative probability
is prob.</term>
<term termid="idf.srange"><linktext>IDF.SRANGE</linktext>IDF.SRANGE(prob,
a, b). Numeric.  Returns the value from the Studentized range statistic,
with the specified parameters, for which the cumulative probability
is prob.</term>
<term termid="idf.t_prob_df"><linktext>IDF.T(prob, df)</linktext>IDF.T(prob,
df). Numeric. Returns the value from Student's t distribution, with
specified degrees of freedom df, for which the cumulative probability
is prob.</term>
<term termid="idf.uniform_prob_min_max"><linktext>IDF.UNIFORM(prob,
min, max)</linktext>IDF.UNIFORM(prob, min, max). Numeric. Returns
the value from the uniform distribution between min and max for which
the cumulative probability is prob.</term>
<term termid="idf.weibull_prob_a_b"><linktext>IDF.WEIBULL(prob, a,
b)</linktext>IDF.WEIBULL(prob, a, b). Numeric. Returns the value from
the Weibull distribution, with specified parameters, for which the
cumulative probability is prob.</term>
<term termid="if_condition_is_satisfied"><linktext>If Condition Is
Satisfied</linktext>Use a conditional expression to select cases.
If the result of the conditional expression is true, the case is selected.
If the result is false or missing, the case is not selected.</term>
<term termid="image_factoring"><linktext>Image Factoring</linktext
>A factor extraction method developed by Guttman and based on image
theory. The common part of the variable, called the partial image,
is defined as its linear regression on remaining variables, rather
than a function of hypothetical factors.</term>
<term termid="importance_1"><linktext>Importance</linktext>Pratt's
measure of relative importance. This measure defines the importance
of predictors additively, that is, the importance of a set of predictors
is the sum of the individual importances of the predictors. This measure
also signals the presence of multicollinearity among the predictors
by a substantive negative importance value and the presence of a suppressor
variable by a low importance value while the regression coefficient
is comparable to the coefficients of variables deemed to be important.</term>
<term termid="importance_2"><linktext>Importance</linktext>Importance
is a measure of the relative contribution of each dimension to the
solution.</term>
<term termid="importance_3"><linktext>Importance</linktext>This is
a raw measure of the effect changes in this predictor have on the
output.</term>
<term termid="improvement_2"><linktext>Improvement</linktext>The improvement
value for a node equals the probability of a case in the node multiplied
by the decrease in impurity value for the node.  (In CRT, the decrease
in impurity is the splitting criterion.)</term>
<term termid="improvement_1"><linktext>Improvement</linktext>The difference
between the normalized raw stress at the previous iteration and the
current.</term>
<term termid="improvement"><linktext>Improvement</linktext>Improvement
in model fit due to the current step</term>
<term termid="imputation_sequence"><linktext>Imputation Sequence</linktext
>The order in which variables are imputed.</term>
<term termid="imputed_values_n"><linktext>Imputed Values</linktext
>The total number of values imputed by the model.</term>
<term termid="imputed_values"><linktext>Imputed Values</linktext>Lists
any constraints placed on scale variables used as dependents during
imputation.</term>
<term termid="include_constant_in_equation"><linktext>Include Constant
in Equation</linktext>Usually the regression model contains a constant
term. To suppress this term and obtain regression through the origin,
deselect this item.</term>
<term termid="include_constant_in_model"><linktext>Include Constant
in Model</linktext>By default, the model includes a constant term
(intercept). To suppress the constant term and obtain regression through
the origin, deselect this item. The constant term is now zero.</term>
<term termid="included"><linktext>Included</linktext>Lists the number
and percentage of cases included in this analysis.</term>
<term termid="inclusion_probability"><linktext>Inclusion Probability</linktext
>The proportion of units drawn at this stage.</term>
<term termid="increase"><linktext>Increase</linktext>The increase
in variance accounted for by the model from the previous iteration
to the current.</term>
<term termid="increase_in_r_square"><linktext>Increase in R square</linktext
>The difference in Multiple R Square between 2 consecutive iterations.</term>
<term termid="increment_chart_axis"><linktext>Increment (Chart Axis)</linktext
>The interval at which points are labeled on a scale axis. It is possible
to subdivide the increment, in which case the larger interval is called
the major increment and the smaller interval the minor increment.</term>
<term termid="gacf05"><linktext>Independence Model (Autocorrelations)</linktext
>Calculates standard error assuming that the underlying process is
white noise.</term>
<term termid="independent_samples"><linktext>Independent Samples</linktext
>Samples selected in such a way that there is no relationship between
the members of the samples. There is no pairing of observations between
the samples.</term>
<term termid="independent_variable"><linktext>Independent Variable</linktext
>The variable used to predict the value of the dependent variable.
Also called predictor variable or explanatory variable.</term>
<term termid="independent_variable_s"><linktext>Independent Variable(s)</linktext
>The variables used to predict the value of the dependent variable.
Also called predictor variables or explanatory variables.</term>
<term termid="independent_variables"><linktext>Independent Variables</linktext
>The number of independent variables specified by the user to define
restrictions on the common space.</term>
<term termid="independent_variables_included"><linktext>Independent
variables included</linktext>Independent (predictor) variables included
in final tree model. For many models, not all independent variable
specified are included in the final model.</term>
<term termid="independent_s"><linktext>Independent(s)</linktext>The
variables used to predict the value of the dependent variable. Also
called predictor variables or explanatory variables. This list must
contain at least one variable to run this procedure.</term>
<term termid="independents"><linktext>Independents</linktext>The predictor
variables or explanatory variables. You must select at least one independent
variable to run this procedure.</term>
<term termid="index_tree"><linktext>Index</linktext>Index is the ratio
of the node  response percentage for the target category compared
to the overall target category response percentage for the entire
sample.</term>
<term termid="index_haystack_needle_divisor"><linktext>INDEX(haystack,
needle,divisor)</linktext>INDEX(haystack,needle[,divisor]). Numeric.
Returns a number that indicates the byte position of the first occurrence
of needle in haystack. The optional third argument, divisor, is a
number of bytes used to divide needle into separate strings. Each
substring is used for searching and the function returns the first
occurrence of any of the substrings. Divisor must be a positive integer
and must divide evenly into the length of needle. Returns 0 if needle
does not occur within haystack.</term>
<term termid="index_haystack_needle"><linktext>INDEX(haystack,needle)</linktext
>INDEX(haystack,needle). Numeric. Returns an integer that indicates
the starting position of the first occurrence of the string needle
in the string haystack. Returns 0 if needle does not occur within
haystack. For example, INDEX(var1, 'b') returns a value of 2  for
a case with a value of 'abcd' for var1.</term>
<term termid="indicate_case_source_as_variable"><linktext>Indicate
Case Source as Variable</linktext>Indicates the source data file for
each case. This variable has a value of 0 for cases from the active
dataset and a value of 1 for cases from the external data file.</term>
<term termid="indicator_contrasts"><linktext>Indicator Contrasts</linktext
>Contrasts indicate the presence or absence of category membership.
This contrast is equivalent to the traditional group of "dummy variables."
Select either First or Last as the omitted reference category.</term>
<term termid="indicator_variables"><linktext>Indicator Variables</linktext
>Variables which indicate the present and absence of a characteristic,
usually denoted by 1 and 0. For example, if red is an indicator variable
then a value of 1 would indicate the object is red and 0 would indicate
the object is not red. Often called dummy variables.</term>
<term termid="individual"><linktext>Individual</linktext>Lower and
upper bounds (two variables) for the prediction interval of the dependent
variable for a single case.</term>
<term termid="gcontrol02"><linktext>Individuals and Moving Range Charts</linktext
>Plot individual measurements of a process rather than subgroup averages.
Individual charts plot each value in sequence. Moving range charts
plots the range of values within a selected span of cases.</term>
<term termid="indvar_i"><linktext>Indvar(I)</linktext>Displays the
categories of the independent variable.</term>
<term termid="inertia"><linktext>Inertia</linktext>Measure of dispersion
for each point, taking the marginal frequency into account.</term>
<term termid="infinite_loop"><linktext>Infinite Loop</linktext>See
"loop, infinite."</term>
<term termid="infinite_step_size"><linktext>Infinite Step Size</linktext
>If the change in the parameters at a step is greater than the value
specified as infinite step size, the problem is considered unbounded,
and estimation stops. Specify any positive number.</term>
<term termid="influence_variable"><linktext>Influence variable</linktext
>The  influence variable  defines how much influence a case has on
the tree-growing process. Cases with lower influence values have less
influence, cases with higher values have more.</term>
<term termid="ginflpts"><linktext>Influential Points</linktext>Cases
(such as outliers) that unduly affect the final coefficients of a
model. If such cases exist, it is prudent to identify them for individual
inspection.</term>
<term termid="initial_and_final_parameters_with_iteration_details"
><linktext>Initial and final parameters with iteration details</linktext
>Displays the initial and final parameter estimates, the parameter
estimates after each iteration, goodness-of-fit statistics, the number
of iterations, and the reason that iteration terminated.</term>
<term termid="initial_and_final_parameters_with_iteration_summary"
><linktext>Initial and final parameters with iteration summary</linktext
>Displays the initial and final parameter estimates, goodness-of-fit
statistics, the number of iterations, and the reason that iteration
terminated.</term>
<term termid="initial_cluster_centers"><linktext>Initial Cluster Centers</linktext
>First estimate of the variable means for each of the clusters. By
default, a number of well-spaced cases equal to the number of clusters
is selected from the data. Initial cluster centers are used for a
first round of classification and are then updated.</term>
<term termid="initial_communalities"><linktext>Initial communalities</linktext
>Communalities for the variables before extraction of factors (or
components). For principal components analyses, the values are the
diagonal elements of the matrix to be analyzed (correlation or covariance
matrix); for factor analysis, they are the sums of squared loadings
for each variable using all other variables as predictors.</term>
<term termid="initial_coordinates"><linktext>Initial Coordinates</linktext
>The number of user-provided initial coordinates.</term>
<term termid="initial_eigenvalues"><linktext>Initial Eigenvalues</linktext
>Eigenvalues of the correlation or covariance matrix. These values
are used to determine which factors (or components) to keep in the
solution.</term>
<term termid="initial_solution"><linktext>Initial Solution</linktext
>Displays initial communalities, eigenvalues, and the percentage of
variance explained.</term>
<term termid="initial_value_of_autoregressive_parameter"><linktext
>Initial value of autoregressive parameter</linktext>The value from
which the iterative search for the optimal value of rho begins. You
can specify any number less than 1 and greater than -1, although negative
values of rho are uncommon in this procedure.</term>
<term termid="initial_values"><linktext>Initial Values</linktext>Some
of the more complex statistical techniques find parameter estimates
via iterative techniques, in which case the estimation algorithm requires
initial values (or starting values) for the parameter estimates.</term>
<term termid="inner_join"><linktext>Inner Join</linktext>A join used
to link data from two tables, it only includes rows where the related
fields are equal.</term>
<term termid="innovational"><linktext>Innovational</linktext>An outlier
that acts as an addition to the noise term at a particular series
point. For stationary series, an innovational outlier affects several
observations. For nonstationary series, it may affect every observation
starting at a particular series point.</term>
<term termid="input_layer"><linktext>Input Layer</linktext>The input
layer contains the predictors.</term>
<term termid="instrumental"><linktext>Instrumental</linktext>These
are the variables used to compute the predicted values for the endogenous
variables in the first stage of two-stage least squares analysis.
The same variables may appear in both the Explanatory and Instrumental
list boxes. The number of instrumental variables must be at least
as many as the number of explanatory variables. If all explanatory
and instrumental variables listed are the same, the results are the
same as results from the Linear Regression procedure.</term>
<term termid="instrumental_variables"><linktext>Instrumental Variables</linktext
>Variables that are not influenced by other variables in the model
but that do influence those variables. To be effective, instrumental
variables should be highly correlated with the endogenous variables
and not correlated with the error terms.</term>
<term termid="valid_probit_1"><linktext>Instrumental Variables</linktext
>Instruments are secondary predictors which are used to model the
predictors.  Estimates from these models are then used in place of
the actual predictors in order to model the dependent variable.  This
process solves the problem of predictors correlated with the error
term.</term>
<term termid="interaction"><linktext>Interaction</linktext>A measure
of the extent to which the variation in the dependent variable cannot
be considered to be the result of a simple combination of the main
effects of the factors in the analysis. In other words, it may be
concluded that the effect of one variable varies across categories
or combinations of categories of other variables.</term>
<term termid="intercept_2"><linktext>Intercept</linktext>The overall
mean.</term>
<term termid="intercept_6"><linktext>Intercept</linktext>The value
of the response variable when all the predictor variables equal 0.</term>
<term termid="intercept"><linktext>Intercept</linktext>The value of
the dependent variable when the independent variable equals 0 or,
in other words, when the regression line crosses the vertical axis.</term>
<term termid="intercept_multinomial_logistic_regression"><linktext
>Intercept (multinomial logistic regression)</linktext>In the usual
linear regression, the intercept gives the value of the response variable
when all the predictor variables equal 0.  In a multinomial logistic
regression, the intercept for category A gives the value of the logit
for category A versus the baseline category.</term>
<term termid="intercept_probit"><linktext>Intercept (Probit)</linktext
>A separate intercept is fit for each group.</term>
<term termid="intercept_only"><linktext>Intercept Only</linktext>A
model fit with just the intercept term.  If the difference in the
fitted and null model -2 log likelihoods is large, then you can reject
the hypothesis that all the parameter coefficients are 0.</term>
<term termid="inter-item_correlations"><linktext>Inter-Item Correlations</linktext
>Statistics based on the correlation of each pair of items.</term>
<term termid="inter-item_covariances"><linktext>Inter-Item Covariances</linktext
>Statistics based on the covariance of each pair of items.</term>
<term termid="internal_value"><linktext>Internal Value</linktext>The
value used internally to represent a category when recoding a categorical
variable</term>
<term termid="interquartile_range_box_plotsdivexplore"><linktext>Interquartile
Range (Box plots/Explore)</linktext>A measure of the spread of the
data. The distance between the third quartile (75th percentile) and
the first quartile (25th percentile) values.</term>
<term termid="ginterval1"><linktext>Interval Axis</linktext>The axis
from which the bars of a histogram originate. An interval axis differs
from a category axis because it represents a continuous range of values,
rather than discrete categories.</term>
<term termid="interval_start_time"><linktext>Interval Start Time</linktext
>The time period that marks the beginning of the interval.  An interval
extends from the start time up to, but not including, the start time
of the next interval.</term>
<term termid="gintervalvar"><linktext>Interval Variables</linktext
>Quantitative variables measured on a numeric scale in which distances
between the points on the scale can be compared meaningfully. Interval
variables have numeric values, rather than coded values.</term>
<term termid="intervention_onsets"><linktext>intervention onsets</linktext
>Indicates the presence of vertical reference lines at specified points
along the time axis.</term>
<term termid="intraclass_correlation"><linktext>Intraclass Correlation</linktext
>The measure of inter-rater reliability under the given model.</term>
<term termid="invalid"><linktext>Invalid</linktext>Cases not used
in the analysis.</term>
<term termid="inverse_model"><linktext>Inverse Model</linktext>Model
whose equation is Y = b0 + (b1 / t).</term>
<term termid="inverse_of_correlation_matrix_factor_analysis"><linktext
>Inverse of Correlation Matrix (Factor Analysis)</linktext>The inverse
of the matrix of correlation coefficients.</term>
<term termid="item_means"><linktext>Item Means</linktext>Statistics
based on the mean of each item.</term>
<term termid="item_variances"><linktext>Item Variances</linktext>Statistics
based on the variance of each item.</term>
<term termid="items"><linktext>Items</linktext>The variables you have
chosen to make up a scale in reliability analysis.</term>
<term termid="iteration_7"><linktext>Iteration</linktext>The Total
Fit and Difference in Total Fit Between Two Consecutive Iterations
are presented for each iteration of the estimation algorithm</term>
<term termid="iteration"><linktext>Iteration</linktext>Repeated execution
of the estimation algorithm.</term>
<term termid="iteration_4"><linktext>Iteration</linktext>Results are
presented for each iteration of the estimation algorithm.</term>
<term termid="iteration_difference"><linktext>Iteration Difference</linktext
>The difference in Total Fit between two consecutive iterations.</term>
<term termid="iteration_number"><linktext>Iteration Number</linktext
>Dimension containing iteration number.</term>
<term termid="iteration_number_1"><linktext>Iteration Number</linktext
>Displays the major and minor iterations.  The minor iteration determines
the search direction of the algorithm, and the major iteration finds
the best parameter estimates in that direction.</term>
<term termid="jaccard_similarity_measure"><linktext>Jaccard Similarity
Measure</linktext>The similarity ratio for binary data. A matching
coefficient for binary variables in which joint absences are excluded
from both the denominator and the numerator and equal weight is given
to matches and nonmatches. Computed from a fourfold table as a/(a+b+c)
where a represents cases present on both items, and b and c represent
cases present on one item but absent on the other.</term>
<term termid="joint_inclusion_probabilities_file"><linktext>Joint
Inclusion Probabilities File</linktext>In order to use Unequal WOR
estimation for clusters drawn using a PPS WOR method, you need to
specify a separate file containing the joint probabilities. This file
is created by the Sampling Wizard during sampling.</term>
<term termid="jonckheere-terpstra_test"><linktext>Jonckheere-Terpstra
Test</linktext>Tests whether k independent samples defined by a grouping
variable are from the same population. This test is appropriate for
either continuous or ordered categorical data, and is more powerful
than the Kruskal-Wallis test when the k populations are at the ordinal
level of measurement.</term>
<term termid="k_3"><linktext>K</linktext>The order of effects being
tested.</term>
<term termid="k"><linktext>K</linktext>The deviation of the process
mean from the midpoint of the specification limits.</term>
<term termid="k_matrix"><linktext>K matrix</linktext>The K matrix
is the contrast results matrix in the model equation LBM=K.</term>
<term termid="k_matrix_1"><linktext>K matrix</linktext>The K matrix
is the contrast results matrix. This matrix specifies the results
of the linear hypothesis in the equation LB=K.  By default, K is a
zero matrix.</term>
<term termid="kappa"><linktext>Kappa</linktext>Cohen's kappa measures
the agreement between the evaluations of two raters when both are
rating the same object. A value of 1 indicates perfect agreement.
A value of 0 indicates that agreement is no better than chance. Kappa
is based on a square table in which row and column values represent
the same scale. Any cell that has observed values for one variable
but not the other is assigned a count of  0. Kappa is not computed
if the data storage type (string or numeric) is not the same for the
two variables. For string variable, both variables must have the same
defined length.</term>
<term termid="kendalls_tau-b"><linktext>Kendall's tau-b</linktext
>A nonparametric measure of correlation for ordinal or ranked variables
that take ties into account. The sign of the coefficient indicates
the direction of the relationship, and its absolute value indicates
the strength, with larger absolute values indicating stronger relationships.
Possible values range from -1 to 1, but a value of -1 or +1 can be
obtained only from square tables.</term>
<term termid="kendalls_tau-b_1"><linktext>Kendall's tau-b</linktext
>A nonparametric measure of association for ordinal or ranked variables
that take ties into account. The sign of the coefficient indicates
the direction of the relationship, and its absolute value indicates
the strength, with larger absolute values indicating stronger relationships.
Possible values range from -1 to 1, but a value of -1 or +1 can be
obtained only from square tables.</term>
<term termid="kendalls_tau-c"><linktext>Kendall's tau-c</linktext
>A nonparametric measure of association for ordinal variables that
ignores ties. The sign of the coefficient indicates the direction
of the relationship, and its absolute value indicates the strength,
with larger absolute values indicating stronger relationships. Possible
values range from -1 to 1, but a value of -1 or +1 can be obtained
only from square tables.</term>
<term termid="kendalls_w"><linktext>Kendall's W</linktext>A nonparametric
test of the hypothesis that several related samples are from the same
population which measures the agreement of raters. Each case is a
judge or rater and each variable is an item or person being judged.
For each variable, the sum of ranks is computed. Kendall's W ranges
between 0 (no agreement) and 1 (complete agreement).</term>
<term termid="kmo_and_bartletts_test_of_sphericity_factor_analysis"
><linktext>KMO and Bartlett's Test of Sphericity (Factor Analysis)</linktext
>The Kaiser-Meyer-Olkin measure of sampling adequacy tests whether
the partial correlations among variables are small. Bartlett's test
of sphericity tests whether the correlation matrix is an identity
matrix, which would indicate that the factor model is inappropriate.</term>
<term termid="kolmogorov-smirnov"><linktext>Kolmogorov-Smirnov</linktext
>A test of normality based on the absolute value of the maximum difference
between the observed cumulative distribution and that expected based
on the assumption of normality. The Lilliefors correction is applied.</term>
<term termid="kolmogorov-smirnov_lilliefors"><linktext>Kolmogorov-Smirnov
(Lilliefors)</linktext>The Lilliefors test is a modification of the
Kolmogorov-Smirnov test that tests for normality when means and variances
are not known, but must be estimated from the data. The Kolmogorov-Smirnov
test is based on the largest absolute difference between the observed
and the expected cumulative distributions.</term>
<term termid="kolmogorov-smirnov_one-sample_test"><linktext>Kolmogorov-Smirnov
One-Sample Test</linktext>Used to test the hypothesis that a sample
comes from a particular distribution (uniform, normal, or Poisson).
The value of the Kolmogorov-Smirnov Z is based on the largest absolute
difference between the observed and the theoretical cumulative distributions.</term>
<term termid="kolmogorov-smirnov_z"><linktext>Kolmogorov-Smirnov Z</linktext
>A test of whether two samples (groups) come from the same distribution.
It is sensitive to any type of difference in the two distributions--shape,
location, and so on. The test is based on the largest difference between
the two cumulative distributions.</term>
<term termid="kruskal-wallis_h"><linktext>Kruskal-Wallis H</linktext
>A nonparametric equivalent to one-way ANOVA. Tests whether several
independent samples are from the same population. Assumes that the
underlying variable has a continuous distribution, and requires an
ordinal level of measurement.</term>
<term termid="kulczynski_1_similarity_measure"><linktext>Kulczynski
1 Similarity Measure</linktext>A matching coefficient similarity measure
for binary data in which all matches are excluded from the denominator,
joint absences are excluded from the numerator, and equal weight is
given to matches and nonmatches. Computed from a fourfold table as
a/(b+c) where a represents cases present on both items, and b and
c represent cases present on one item but absent on the other. Has
a minimum value of 0 and no upper limit. Undefined when b=0 and c=0.</term>
<term termid="kulczynski_2_similarity_measure"><linktext>Kulczynski
2 Similarity Measure</linktext>Similarity measure for binary data
that yields the average conditional probability that a characteristic
is present in one item, given that the characteristic is present in
the other item. The measure is an average over both items acting as
predictors. Computed from a fourfold table as (a/(a+b)+a/(a+c))/2
where a represents cases present on both items, and b and c represent
cases present on one item but absent on the other. It has a range
of 0 to 1.</term>
<term termid="kurtosis"><linktext>Kurtosis</linktext>A measure of
the extent to which observations cluster around a central point. For
a normal distribution, the value of the kurtosis statistic is zero.
Positive kurtosis indicates that, relative to a normal distribution,
the observations are more clustered about the center of the distribution
and have thinner tails until the extreme values of the distribution,
at which point the tails of the leptokurtic distribution are thicker
relative to a normal distribution.  Negative kurtosis indicates that,
relative to a normal distribution, the observations cluster less and
have thicker tails until the extreme values of the distribution, at
which point the tails of the platykurtic distribution are thinner
relative to a normal distribution.</term>
<term termid="k-way_and_higher_order_effects"><linktext>K-way and
Higher Order Effects</linktext>These are tests of the significance
of K-way and higher order model effects.  A significant (&lt; 0.05)
result means that the effects cannot be explained by chance variation.</term>
<term termid="l_matrix"><linktext>L matrix</linktext>The L matrix
is the contrast coefficients matrix.  This matrix specifies coefficients
of contrasts, which can be used for studying the effects in the model
via the equation LB=K.</term>
<term termid="label"><linktext>Label</linktext>Enter an optional descriptive
variable label for the multiple response set. The label can be up
to 40 characters long.</term>
<term termid="label_20"><linktext>Label</linktext>The value label
for the specified level of a factor.</term>
<term termid="label_13"><linktext>Label</linktext>Value of the variable
used to label cases.</term>
<term termid="b_coefficient_1"><linktext>Label (Display)</linktext
>Descriptive label for a variable or data value.</term>
<term termid="label_cases_by"><linktext>Label Cases by</linktext>Cases
are identified by default by their sequence in the data file. Optionally,
you can label cases with their values for a variable, such as a case
ID variable.</term>
<term termid="label_cases_by_1"><linktext>Label Cases By</linktext
>By default, cases are identified in output by case number. Optionally,
you can use the values of a string variable to identify cases. Select
a string variable.</term>
<term termid="gaxis11"><linktext>Label Orientation</linktext>The angle
and position of labels relative to the axis. Labels can be horizontal,
vertical, diagonal, or staggered.</term>
<term termid="lack_of_fit_1"><linktext>Lack of Fit</linktext>Displays
results for variation due to lack of fit. This is the component of
the error sum of squares due to lack of fit. If the current model
is appropriate, the sum of squares due to lack of fit will be small.</term>
<term termid="lag"><linktext>Lag</linktext>Value of a previous case,
based on the specified lag order. The Order is the number of cases
prior to the current case from which the value is obtained.</term>
<term termid="gacf09"><linktext>Lag (Autocorrelations)</linktext>A
transformation that brings past values of a series into the current
case. The case prior to the current case is a lag of 1; two cases
prior to the current case is a lag of 2; and so on.</term>
<term termid="lag_variable"><linktext>LAG(variable, n)</linktext>LAG(variable[,
n]). Numeric or string. The value of variable in the previous case
or n cases before.  The optional second argument, n, must be a positive
integer; the default is 1. For example, prev4=LAG(gnp,4) returns the
value of gnp for the fourth case before the current one. The first
four cases have system-missing values for prev4.</term>
<term termid="lambda"><linktext>Lambda</linktext>A measure of association
that reflects the proportional reduction in error when values of the
independent variable are used to predict values of the dependent variable.
A value of 1 means that the independent variable perfectly predicts
the dependent variable. A value of 0 means that the independent variable
is no help in predicting the dependent variable.</term>
<term termid="cronbachs_alpha_4"><linktext>Lambda 1</linktext>The
first of Guttman's six measures of reliability that all give lower
bounds for the true reliability of the survey. The first is a simple
estimate that is the basis for computing some of the other lower bounds.</term>
<term termid="lambda_2"><linktext>Lambda 2</linktext>L2 is always
better, in the sense that it is larger than L3 or L1.</term>
<term termid="lambda_3"><linktext>Lambda 3</linktext>L3 is equivalent
to Cronbach's Alpha.</term>
<term termid="lambda_4"><linktext>Lambda 4</linktext>L4 is, in fact,
the Guttman split-half coefficient. Moreover, it is a lower bound
for the true reliability for any split of the test. Therefore, Guttman
suggests finding the split that maximizes L4, comparing it to the
other lower bounds, and choosing the largest.</term>
<term termid="lambda_5"><linktext>Lambda 5</linktext>L5 is better
than L2 when there is one item that has a high covariance with the
other items, which in turn do not have high covariances with each
other. Such a situation may occur on a test that has items that each
pertain to one of several different fields of knowledge, plus one
question that can be answered with knowledge of any of those fields.</term>
<term termid="lambda_6"><linktext>Lambda 6</linktext>L6 is better
than L2 when the inter-item correlations are low compared to the squared
multiple correlation of each item when regressed on the remaining
items. For example, consider a test that covers many different fields
of knowledge and each item covers some small subset of those fields.
Most item pairs will not have overlapping fields, but the fields of
a single item should be well represented given all the remaining items
on the test.</term>
<term termid="lambda_similarity_measure"><linktext>Lambda Similarity
Measure</linktext>Similarity measure for binary data due to Goodman
and Kruskal. Assesses the predictability of the state of a characteristic
on one item (presence or absence) given the state on the other item.
Lambda measures the proportional reduction in error using one item
to predict the other, when the directions of prediction are of equal
importance. Its range is 0 to 1.</term>
<term termid="lance_and_williams_distance_measure"><linktext>Lance
and Williams Distance Measure</linktext>Also known as the Bray-Curtis
nonmetric coefficient; a nonmetric dissimilarity measure for binary
data. Computed from a fourfold table as (b+c)/(2a+b+c) where a represents
cases present on both items, and b and c represent cases present on
one item but absent on the other.</term>
<term termid="last_3"><linktext>Last</linktext>Displays the last data
value encountered in the data file.</term>
<term termid="last_value_aggregate_function"><linktext>Last Value
(Aggregate Function)</linktext>The value of the source variable for
the last case in the break group. </term>
<term termid="layer"><linktext>layer</linktext>If rows give a table
height and columns give it width, then layers give it depth.</term>
<term termid="lcl"><linktext>LCL</linktext>Lower confidence limit
for the forecasted value.</term>
<term termid="ldn_fit"><linktext>LDN Fit</linktext>The Total Fit of
the model to the data.  A larger value indicates a better fit.</term>
<term termid="gaxis09"><linktext>Leading Character</linktext>A character
added to the beginning of all the labels on the scale axis. For example,
a leading character might be a dollar sign at the beginning of each
label.</term>
<term termid="least_significant_difference_1"><linktext>Least Significant
Difference</linktext>Uses t tests to perform all pairwise comparisons
between group means. No adjustment is made to the error rate for multiple
comparisons.</term>
<term termid="least_significant_difference_2"><linktext>Least Significant
Difference</linktext>Uses t tests to perform all pairwise comparisons
between group means. No adjustment is made to the error rate for multiple
comparisons.</term>
<term termid="leave-one-out_classification"><linktext>Leave-one-out
Classification</linktext>Each case in the analysis is classified by
the functions derived from all cases other than that case. It is also
known as the "U-method."</term>
<term termid="left_alignment"><linktext>Left Alignment</linktext>Left
aligns data values in the column.</term>
<term termid="left_outer_join"><linktext>Left Outer Join</linktext
>A join used to link data from two tables, it includes all records
from 'Table 1' and only those records from 'Table 2' where the related
fields are equal. 'Table 1' is either the Primary table, as defined
in the original database, or the table from which you dragged the
join line.</term>
<term termid="length_strexpr"><linktext>LENGTH(strexpr)</linktext
>LENGTH(strexpr). Numeric. Returns the length of strexpr in bytes,
which must be a string expression. For string variables, in Unicode
mode this is the number of bytes in each value, excluding trailing
blanks, but in code page mode this is the defined variable length,
including trailing blanks. To get the length (in bytes) without trailing
blanks in code page mode, use LENGTH(RTRIM(strexpr)). </term>
<term termid="less_than_or_equal_to_the_median"><linktext>Less Than
or Equal to the Median</linktext>Lists the number of observations
that are less than or equal to the median.</term>
<term termid="less_than_varvalue"><linktext>Less Than Varvalue</linktext
>Displays information about the observations that are less than the
cut point value.</term>
<term termid="level_1"><linktext>level</linktext>The values of a factor
are referred to as levels of the factor, or factor levels.</term>
<term termid="level_for_factor_a_b"><linktext>Level for Factor A or
B</linktext>One of two factor levels that always occur together in
the design; indicating that the design is not orthogonal.</term>
<term termid="level_shift"><linktext>Level shift</linktext>An outlier
that shifts all observations by a constant, starting at a particular
series point. A level shift could result from a change in policy.</term>
<term termid="levels"><linktext>Levels</linktext>Each allowed value
of a factor variable is referred to as a level.</term>
<term termid="levenberg-marquardt"><linktext>Levenberg-Marquardt</linktext
>This is the default algorithm for unconstrained models. The Levenberg-Marquardt
method is not available if you specify a constrained model, a user-defined
loss function, or bootstrapping.  You can enter new values for Maximum
iterations, and you can change the selection in the drop-down lists
for Sum-of-squares convergence and Parameter convergence.</term>
<term termid="levene_statistic"><linktext>Levene Statistic</linktext
>A homogeneity-of-variance test that is less dependent on the assumption
of normality than most tests. For each case, it computes the absolute
difference between the value of that case and its cell mean and performs
a one-way analysis of variance on those differences.</term>
<term termid="levene_test"><linktext>Levene Test</linktext>A homogeneity-of-variance
test that is less dependent on the assumption of normality than most
tests. For each case, it computes the absolute difference between
the value of that case and its cell mean and performs a one-way analysis
of variance on those differences.</term>
<term termid="leverage_value"><linktext>Leverage Value</linktext>The
relative influence of each observation on the model's fit.</term>
<term termid="leverage_values_1"><linktext>Leverage Values</linktext
>Uncentered leverage values. The relative influence of each observation
on the model's fit.</term>
<term termid="leverage_values"><linktext>Leverage Values</linktext
>Measures the influence of a point on the fit of the regression. The
centered leverage ranges from 0 (no influence on the fit) to (N-1)/N.</term>
<term termid="lg10_numexpr"><linktext>LG10(numexpr)</linktext>LG10(numexpr).
Numeric. Returns the base-10 logarithm of numexpr, which must be numeric
and greater than 0.</term>
<term termid="likelihood_ratio"><linktext>Likelihood Ratio</linktext
>The likelihood ratio is a test statistic which uses the likelihood
ratio criterion for testing the null hypothesis. It is the ratio of
the maximum value of the likelihood function under the null hypothesis
to the maximum value of the likelihood function when the null hypothesis
is not assumed. If the significance is small (say, less than 0.05),
the null hypothesis is rejected.</term>
<term termid="likelihood_ratio_chi-square"><linktext>Likelihood Ratio
Chi-Square</linktext>A test of independence based on the ratio between
the observed and expected frequencies.</term>
<term termid="likelihood-ratio_chi-square"><linktext>Likelihood-Ratio
Chi-Square</linktext>A goodness-of-fit statistic similar to Pearson's
chi-square. For large sample sizes, the two statistics are equivalent.
The advantage of the likelihood-ratio chi-square is that it can be
subdivided into interpretable parts that add up to the total. For
general purposes, the significance value is more important than the
actual value of the statistic.</term>
<term termid="linear"><linktext>Linear</linktext>Tests the linear
effects across all levels.</term>
<term termid="linear_constraint"><linktext>Linear Constraint</linktext
>In constrained nonlinear regression problems, you can specify linear
constraints on the parameters.  This is the value of the constraint
function for the current estimates of the model parameters.</term>
<term termid="glineareqn"><linktext>Linear Equation</linktext>A linear
equation is one in which each term consists of a variable multiplied
by a constant.</term>
<term termid="linear_model"><linktext>Linear Model</linktext>Model
whose equation is Y = b0 + (b1 * t). The series values are modeled
as a linear function of time.</term>
<term termid="linear_regression_scatterplot_options"><linktext>Linear
Regression (Scatterplot Options)</linktext>Produces a least-squares
linear regression line that best fits the data points on the scatterplot.</term>
<term termid="linear_term"><linktext>Linear Term</linktext>Displays
the results of the 1st-degree polynomial term. A 1st-degree polynomial
term is the term with a variable raised to the 1st power.</term>
<term termid="exsm_tlin"><linktext>Linear Trend (Exponential Smoothing)</linktext
>The mean level of the series increases or decreases linearly (at
a constant rate) with time.</term>
<term termid="linear-by-linear_association_crosstabs"><linktext>Linear-by-Linear
Association (Crosstabs)</linktext>A measure of linear association
between the row and column variables in a crosstabulation. This statistic
should not be used for nominal data. Also known as the Mantel-Haenszel
chi-square test. For general purposes, the significance value is more
important than the actual value of the statistic.</term>
<term termid="linearity"><linktext>Linearity</linktext>The part of
the between groups SS which can be attributed to a linear relationship
between the dependent variable and the levels of the factor variable.
The levels of the factor or grouping variable must have an underlying
order to use this statistic. For example, factor levels may correspond
to dosages of a drug, or number of cars owned, but not to car colors
or religions.</term>
<term termid="link_function_1"><linktext>Link Function</linktext>The
link function is a transformation of the dependent variable that allows
estimation of the model as a linear combination of the predictors.</term>
<term termid="list_of_variables"><linktext>List of Variables</linktext
>The variables are grouped by set, and for each variable the label,
number of categories, and optimal scaling measurement level are displayed.</term>
<term termid="listwise"><linktext>Listwise</linktext>Displays the
means, correlation matrix, and covariance matrix, omitting cases that
have missing values in any variable under consideration (listwise
deletion).</term>
<term termid="listwise_2"><linktext>listwise</linktext>When computing
a measure of association between two variables in a larger set, cases
are included in the computation only when all variables in the set
have nonmissing values.</term>
<term termid="listwise_deletion"><linktext>Listwise Deletion</linktext
>Cases that have missing values for any of the variables named are
omitted from the analysis.</term>
<term termid="ljung-box_statistic"><linktext>Ljung-Box statistic</linktext
>A lack-of-fit statistic used to test the null hypothesis that the
model is correctly specified. Also known as the modified Box-Pierce
statistic.</term>
<term termid="ln_numexpr"><linktext>LN(numexpr)</linktext>LN(numexpr).
Numeric. Returns the base-e logarithm of numexpr, which must be numeric
and greater than 0.</term>
<term termid="lngamma"><linktext>LNGAMMA</linktext>LNGAMMA(numexpr).
Numeric. Returns the logarithm of the complete Gamma function of numexpr,
which must be numeric and greater than 0.</term>
<term termid="local_trend"><linktext>Local trend</linktext>An outlier
that starts a local trend at a particular series point.</term>
<term termid="location"><linktext>location</linktext>The location
parameter for the distribution. For instance, for the normal distribution
this is the parameter 'a' in NORMAL(a,b).</term>
<term termid="def_log"><linktext>Log (output item type)</linktext
>Journal recording the command(s) executed. It also displays the error
message if an error occurred and the procedure was not completed.
It can be activated as a text object.</term>
<term termid="log_determinants"><linktext>Log Determinants</linktext
>In the multigroup model, log determinant values provide an indication
of which groups' covariance matrices differ most. For each group,
its determinant is the product of the eigenvalues of its within group
covariance matrix.</term>
<term termid="log_likelihood"><linktext>Log Likelihood</linktext>A
measure of how well the model fits the data. The larger the value
the better the fit.</term>
<term termid="log_minus_log"><linktext>Log Minus Log</linktext>The
cumulative survival estimate after the ln(-ln) transformation is applied
to the estimate.</term>
<term termid="log_of_determinant_of"><linktext>Log of Determinant
Of</linktext>The natural logarithm of the determinant of a given matrix.</term>
<term termid="log_rank"><linktext>Log Rank</linktext>A test for comparing
the equality of survival distributions. All time points are weighted
equally in this test.</term>
<term termid="log_survival_plot"><linktext>Log Survival Plot</linktext
>Displays the cumulative survival function on a logarithmic scale.</term>
<term termid="log_transform"><linktext>Log Transform</linktext>Many
series are characterized by greater oscillations when series values
are bigger in magnitude than when they are smaller (a form of heteroscedasticity).
A logarithmic transformation, either to base e (the LN function) or
base 10 (the LG10 function), will often make variation constant across
levels of the series.</term>
<term termid="log_transform_cannot_be_done"><linktext>Log transform
cannot be done</linktext>Cases rejected because the value of a covariate
is not a positive number.</term>
<term termid="log_transformation"><linktext>log transformation</linktext
>In order to correct for non-normality, the natural logarithm can
be applied to a positive-valued variable.  This technique is most
effective when the non-normality is due to positive skewness.</term>
<term termid="gacf01"><linktext>Log Transformation (base e)</linktext
>Uses the natural (base e) logarithms of series values instead of
the values themselves. This transformation requires that all values
be positive.</term>
<term termid="logarithmic_model"><linktext>Logarithmic Model</linktext
>Model whose equation is Y = b0 + (b1 * ln(t)).</term>
<term termid="gseaslog"><linktext>Logarithmic Seasonal Adjustment</linktext
>The seasonal adjustments are added to the seasonally adjusted series
in the logarithmic scale. This involves different assumptions about
the error term than the multiplicative model.</term>
<term termid="logistic_model"><linktext>Logistic Model</linktext>Model
whose equation is Y = 1 / (1/u + (b0 * (b1**t))) or ln(1/y-1/u) =
ln (b0) + (ln(b1) * t) where u is the upper boundary value. After
selecting Logistic, specify the upper boundary value to use in the
regression equation. The value must be a positive number that is greater
than the largest dependent variable value.</term>
<term termid="logit"><linktext>Logit</linktext>A logit is simply the
natural log of the odds ratio, p/(1-p).</term>
<term termid="logit_model_1"><linktext>Logit Model</linktext>Similar
to the Bradley-Terry-Luce model but uses the natural log of the utilities
instead of the utilities.</term>
<term termid="logit_model"><linktext>Logit Model</linktext>Applies
the logit (log odds) transformation to the response proportions.</term>
<term termid="logit_residual"><linktext>Logit Residual</linktext>The
residual for the case if it is predicted in the logit scale. The logit
residual is the residual divided by the predicted probability times
1 minus the predicted probability.</term>
<term termid="log-likelihood_2"><linktext>log-likelihood</linktext
>The natural log of the likelihood function.  It can be used as a
rough measure of model fit, where larger values indicate models that
better fit the data.  The log-likelihood tends to favor overly complex
models; the AIC and BIC are "corrections" to the log-likelihood.</term>
<term termid="log-likelihood"><linktext>Log-Likelihood</linktext>Natural
logarithm (base e) of the likelihood function.</term>
<term termid="log-likelihood_convergence"><linktext>Log-likelihood
Convergence</linktext>Convergence is assumed if the relative change
in the log-likelihood function is less than the specified value. The
value must be positive.</term>
<term termid="log-likelihood_function_value"><linktext>Log-likelihood
Function Value</linktext>The value of the model's log-likelihood function
for the estimated coefficients.</term>
<term termid="loglinear_analysis_loglinear"><linktext>Loglinear Analysis
(LOGLINEAR)</linktext>Technique which models the log of the frequency
in a cell as a function of main effect and interaction terms.</term>
<term termid="glongstringvar"><linktext>Long String Variable</linktext
>A string variable of more than 8 bytes. The use of long string variables
is restricted in many procedures. </term>
<term termid="loop_infinite"><linktext>Loop, Infinite</linktext>See
infinite loop.</term>
<term termid="loss_3"><linktext>Loss</linktext>The loss is the amount
of variance unaccounted for by the model.</term>
<term termid="loss_2"><linktext>Loss</linktext>Here, loss represents
the proportion of variation that is not accounted for.</term>
<term termid="loss_function_values_nonlin"><linktext>Loss Function
Values</linktext>This option is available if you specify your own
loss function. The variable name loss_ is assigned to the values of
the loss function.</term>
<term termid="lost"><linktext>Lost</linktext>"Lost" cases are treated
as censored for purposes of analysis.  The designation allows you
to mark these cases as lost to follow-up analysis.</term>
<term termid="lower_bound"><linktext>Lower Bound</linktext>The smallest
value included in the confidence interval. Values less than the lower
bound fall outside the range of the confidence interval.</term>
<term termid="lower_bound_2"><linktext>Lower Bound</linktext>The smallest
value included in the interval. Values less than the lower bound fall
outside the range of the interval.</term>
<term termid="lower_strexpr"><linktext>LOWER(strexpr)</linktext>LOWER(strexpr).
String. Returns strexpr with uppercase letters changed to lowercase
and other characters unchanged. The argument can be a string variable
or a value. For example, LOWER(name1) returns charles if the value
of name1 is Charles.</term>
<term termid="lowerbound_epsilon"><linktext>Lowerbound Epsilon</linktext
>The lowest possible value of epsilon. Epsilon is an adjustment to
the numerator and denominator degrees of freedom that you may make
if the sphericity assumption appears to be violated in a repeated
measures design.</term>
<term termid="lowess_scatterplot_options_fit_line"><linktext>Lowess
(Scatterplot Options: Fit Line)</linktext>Uses an iterative locally
weighted least-squares method to fit a curve to a set of points. At
least 13 data points are needed. You must specify the number of iterations
and the percentage of data points that must be fit.</term>
<term termid="lowest"><linktext>Lowest</linktext>Smallest values.</term>
<term termid="lpad_strexpr_length"><linktext>LPAD(strexpr,length)</linktext
>LPAD(strexpr,length). String. Returns the string strexpr padded on
the left with blanks to extend it to the length given by length, which
must be a positive integer. </term>
<term termid="lpad_strexpr_length_char"><linktext>LPAD(strexpr1,length,strexpr2)</linktext
>LPAD(strexpr1,length[,strexpr2]). String. Left-pads strexpr1 to make
its length the value specified by length using as many complete copies
as will fit of strexpr2 as the padding string. The value of length
represents the number of bytes and must be a positive integer. If
the optional argument strexpr2 is omitted, the value is padded with
blank spaces.</term>
<term termid="least_significant_difference"><linktext>LSD (Least Significant
Difference)</linktext>Uses t tests to perform all pairwise comparisons
between group means. No adjustment is made to the error rate for multiple
comparisons.</term>
<term termid="lsl"><linktext>LSL</linktext>The fixed lower control
limit.</term>
<term termid="ltrim_strexpr"><linktext>LTRIM(strexpr)</linktext>LTRIM(strexpr).
String. Returns the string strexpr trimmed of any leading blanks.</term>
<term termid="ltrim_strexpr_char"><linktext>LTRIM(strexpr,char)</linktext
>LTRIM(strexpr[,char]). String. Returns strexpr with any leading instances
of char removed. If char is not specified, leading blanks are removed.
Char must resolve to a single character.</term>
<term termid="m_matrix"><linktext>M matrix</linktext>The M matrix
is the transformation coefficients matrix, used to transform the dependent
variable(s). This transformation can be used to construct contrasts
among the dependent variables in the model.</term>
<term termid="mean_absolute_error_mae"><linktext>MAE</linktext>Mean
absolute error. Measures how much the series varies from its model-predicted
level. MAE is reported in the original series units.</term>
<term termid="magnitude"><linktext>Magnitude</linktext>The maximum
deviation from the true series value caused by a transient outlier.</term>
<term termid="mahalanobis_distance"><linktext>Mahalanobis Distance</linktext
>A measure of how much a case's values on the independent variables
differ from the average of all cases. A large Mahalanobis distance
identifies a case as having extreme values on one or more of the independent
variables.</term>
<term termid="mahalanobis_distance_1"><linktext>Mahalanobis' Distance</linktext
>A measure of how much a case's values on the independent variables
differ from the average of all cases. For a single independent variable,
it is simply the square of the standardized value of the independent
variable. A large Mahalanobis distance identifies a case as having
extreme values on one or more of the independent variables.</term>
<term termid="mahalanobis_distance_squared"><linktext>Mahalanobis
Distance Squared</linktext>Cases with large values of Mahalanobis
distance from their group mean can be identified as outliers.</term>
<term termid="main_effects"><linktext>Main Effects</linktext>A component
of the total variation in the dependent variable that can be attributed
to a single independent variable or factor. In general, the greater
the differences between the group means of the factor and its overall
mean, the greater the main effect of the variable. The usefulness
of a main effect test of significance depends on whether it is involved
in significant interaction effects with other variables. If there
is no significant interaction, the main effects are of interest.</term>
<term termid="main_effects_with_covariates"><linktext>Main Effects
with Covariates</linktext>Displays ANOVA table information for the
main effects and the covariates.</term>
<term termid="gaxis03"><linktext>Major Increment</linktext>The size
of the labeled intervals. The major increment must be a value that
is evenly divisible into the scale range. The major scale increment
determines the position of axis labels, major tick marks, and major
grid lines for the scale axis.</term>
<term termid="mallows_conditional_mse_of_prediction_cp"><linktext
>Mallow's Conditional MSE of Prediction (Cp)</linktext>A criterion
for model selection. It measures the standardized total mean squared
error of prediction for the observed data.</term>
<term termid="mann-whitney_u"><linktext>Mann-Whitney U</linktext>A
nonparametric equivalent to the t test. Tests whether two independent
samples are from the same population. It is more powerful than the
median test since it uses the ranks of the cases. Requires an ordinal
level of measurement. U is the number of times a value in the first
group precedes a value in the second group, when values are sorted
in ascending order.</term>
<term termid="mantel-haenszel_estimate"><linktext>Mantel-Haenszel
estimate</linktext>Estimates pertaining to the Mantel-Haenszel common
odds ratio.  The common odds ratio assumes that the odds ratios across
covariate patterns are equal, or homogenous.</term>
<term termid="mantel-haenszel_statistic"><linktext>Mantel-Haenszel
statistic</linktext>Mantel-Haenszel's statistic is similar to Cochran's,
but makes corrections for small sample sizes.  Note that the two statistics
are NOT necessarily equivalent for large samples!</term>
<term termid="mean_absolute_percentage_error_mape"><linktext>MAPE</linktext
>Mean Absolute Percentage Error. A measure of how much a dependent
series varies from its model-predicted level. It is independent of
the units used and can therefore be used to compare series with different
units.</term>
<term termid="marginal_frequency"><linktext>Marginal Frequency</linktext
>The number of objects that fall into a given category.</term>
<term termid="marginal_homogeneity_test"><linktext>Marginal Homogeneity
Test</linktext>A nonparametric test for two related ordinal variables.
This test is an extension of the McNemar test from binary response
to multinomial response. It tests for changes in responses using the
chi-square distribution. It is useful for detecting changes in responses
due to experimental intervention in before-and-after designs.</term>
<term termid="marginal_percentage"><linktext>Marginal Percentage</linktext
>The percentage of cases in each group</term>
<term termid="marker_position_subscript"><linktext>Marker Position
Subscript</linktext>Displays footnote markers aligned slightly below
the text in a cell.</term>
<term termid="marker_position_superscript"><linktext>Marker Position
Superscript</linktext>Displays footnote markers aligned slightly above
the text in a cell.</term>
<term termid="marquardt_constant"><linktext>Marquardt Constant</linktext
>The Marquardt constant controls how an algorithm determines step
size at each iteration.  When the constant is large, the step size
is near what would be suggested by the method of steepest descent
(it suggests a big step size).  When the constant approaches 0, the
step size approaches the Inverse Hessian method (it suggests a small
step size, or "slows down" near the target in order to find the best
estimates).</term>
<term termid="mass"><linktext>Mass</linktext>The proportion of the
margin for each category of the total over all cells of the correspondence
table.</term>
<term termid="matrix_file"><linktext>Matrix File</linktext>A data
file created by the OUTFILE subcommand. The matrix file contains a
covariance or correlation matrix of the parameter estimates in the
model. This file can be used in other procedures that call for a matrix
file.</term>
<term termid="matrix_input"><linktext>Matrix Input</linktext>Indicates
that a distance (proximities) matrix was read in directly, and no
new distances were computed. This is usually done if you want to apply
transformations to previously calculated distances.</term>
<term termid="mauchly_sphericity_test"><linktext>Mauchly Sphericity
Test</linktext>A test of the hypothesis that the covariance matrix
of the transformed variables has a constant variance on the diagonal
and zeros off the diagonal. For sufficiently large sample sizes, a
nonsignificant p value means there is insufficient evidence to reject
the sphericity assumption. The test is not very powerful for small
samples. For large samples the test may be significant even when the
impact of the departure on the analysis-of-variance results may be
small.</term>
<term termid="max_value_value_.."><linktext>MAX(value,value[,..])</linktext
>MAX(value,value[,..]). Numeric or string. Returns the maximum value
of its arguments that have valid values. This function requires two
or more arguments. For numeric values, you can specify a minimum number
of valid arguments for this function to be evaluated.</term>
<term termid="maximum_absolute_error_maxae"><linktext>MaxAE</linktext
>Maximum Absolute Error. The largest forecasted error, expressed in
the same units as the dependent series. Like MaxAPE, it is useful
for imagining the worst-case scenario for your forecasts. Maximum
absolute error and maximum absolute percentage error may occur at
different series points--for example, when the absolute error for
a large series value is slightly larger than the absolute error for
a small series value. In that case, the maximum absolute error will
occur at the larger series value and the maximum absolute percentage
error will occur at the smaller series value.</term>
<term termid="maximum_absolute_percentage_error_maxape"><linktext
>MaxAPE</linktext>Maximum Absolute Percentage Error. The largest forecasted
error, expressed as a percentage. This measure is useful for imagining
a worst-case scenario for your forecasts.</term>
<term termid="maximizing_the_smallest_f_ratio_method_of_entry"><linktext
>Maximizing the Smallest F Ratio Method of Entry</linktext>A method
of variable selection in stepwise analysis based on maximizing an
F ratio computed from the Mahalanobis distance between groups.</term>
<term termid="maximum_46"><linktext>Maximum</linktext>The maximum
value across all estimated models.</term>
<term termid="maximum"><linktext>Maximum</linktext>The largest value
of a numeric variable.</term>
<term termid="maximum_43"><linktext>maximum</linktext>The upper bound
'b' for a uniform distribution of the form UNIFORM(a,b).</term>
<term termid="maximum_div_minimum"><linktext>Maximum / Minimum</linktext
>The ratio of the maximum value to the minimum value.</term>
<term termid="maximum_and_minimum"><linktext>Maximum and Minimum</linktext
>Enter the maximum and minimum integer values for the selected variable
in the Factor list.</term>
<term termid="maximum_and_minimum_1"><linktext>Maximum and Minimum</linktext
>Enter values for Minimum and Maximum that correspond to the lowest
and highest categories of the grouping variable. Both values must
be integers, and cases with values outside the bounds are excluded.
Minimum must be less than Maximum.</term>
<term termid="maximum_iterations_15"><linktext>Maximum iterations</linktext
>The maximum number of iterations the estimation algorithm would perform.</term>
<term termid="maximum_iterations_7"><linktext>Maximum iterations</linktext
>The iterative algorithm stops after the number of specified iterations
even if the algorithm has not converged. You can specify a positive
integer in this text box.</term>
<term termid="maximum_iterations_levenberg-marquardt_algorithm"><linktext
>Maximum Iterations (Levenberg-Marquardt Algorithm)</linktext>The
maximum number of iterations allowed, in the Levenberg-Marquardt Algorithm.</term>
<term termid="maximum_iterations_for_convergence_1"><linktext>Maximum
Iterations for Convergence</linktext>By default, a maximum of 25 iterations
is performed for factor rotation. To specify a different maximum,
enter a positive integer.</term>
<term termid="maximum_iterations_for_convergence"><linktext>Maximum
Iterations for Convergence</linktext>By default, a maximum of 25 iterations
is performed for factor extraction. To specify a different maximum,
enter a positive integer.</term>
<term termid="maximum_likelihood"><linktext>Maximum Likelihood</linktext
>Maximum-likelihood methods attempt to estimate the values of the
parameters that would result in the highest likelihood of observing
the data actually observed. These methods often require iterative
solutions.</term>
<term termid="maximum_mos_value"><linktext>Maximum MOS Value</linktext
>An optional value that sets an upper bound on the MOS, overriding
any values found in the MOS variable or computed from the data.</term>
<term termid="maximum_number_of_lags"><linktext>Maximum Number of
Lags</linktext>Specifies the maximum number of lags plotted.</term>
<term termid="maximum_number_of_units_sampled"><linktext>Maximum Number
of Units Sampled</linktext>An optional value that sets an upper bound
on the number of units sampled when the proportion of units to sample
is specified.</term>
<term termid="maximum_possible"><linktext>Maximum Possible</linktext
>When there are tied observations, the Wald-Wolfowitz runs test calculates
both the maximum number of possible runs, and the minimum possible.</term>
<term termid="maximum_tree_depth"><linktext>Maximum tree depth</linktext
>The maximum number of levels below the root node specified for the
model. For CHAID methods, the default is 3. For CRT and QUEST, the
default is 5.</term>
<term termid="maximum_utility_model"><linktext>Maximum Utility Model</linktext
>Determines the probability as the number of respondents predicted
to choose the profile divided by the total number of respondents.
For each respondent, the predicted choice is simply the profile with
the largest total utility.</term>
<term termid="maximum_value_aggregate_function"><linktext>Maximum
Value (Aggregate Function)</linktext>The largest value of the source
variable for all cases in the break group. </term>
<term termid="gbar14"><linktext>Maximum Value (Graph Summary Function)</linktext
>The largest value for the category.</term>
<term termid="maximum-likelihood_method"><linktext>Maximum-Likelihood
Method</linktext>A factor extraction method that produces parameter
estimates that are most likely to have produced the observed correlation
matrix if the sample is from a multivariate normal distribution. The
correlations are weighted by the inverse of the uniqueness of the
variables, and an iterative algorithm is employed.</term>
<term termid="mblen.byte_strexpr_pos"><linktext>MBLEN.BYTE(strexpr,pos)</linktext
>MBLEN.BYTE(strexpr,pos). Numeric.  Returns the number of bytes in
the character at byte position pos of  strexpr.</term>
<term termid="mca"><linktext>MCA</linktext>Multiple Classification
Analysis. A table containing means for each factor level expressed
as deviations from the grand mean. MCA allows you to assess the magnitude
of the effect of each category of a factor. Available only if the
Hierarchical or Experimental method is used.</term>
<term termid="mcfadden_r-square"><linktext>McFadden R-square</linktext
>The McFadden R-square is a generalized coefficient of determination,
used to estimate the proportion of variance in the dependent variable
which is explained by the predictor (independent) variables. The McFadden
R-square is based on the log likelihood for the model compared to
the log likelihood for a baseline model.</term>
<term termid="mcnemar_test"><linktext>McNemar Test</linktext>A nonparametric
test for two related dichotomous variables. Tests for changes in responses
using the chi-square distribution. Useful for detecting changes in
responses due to experimental intervention in "before-and-after" designs.
For larger square tables, the McNemar-Bowker test of symmetry is reported.</term>
<term termid="mean_mi"><linktext>Mean</linktext>The mean value of
the valid cases is reported for scale variables.</term>
<term termid="mean_25"><linktext>Mean</linktext>The mean normalized
raw stress across objects or sources.</term>
<term termid="mean_26"><linktext>Mean</linktext>The result of summing
the ratios and dividing the result by the total number ratios.</term>
<term termid="mean_5"><linktext>Mean</linktext>The observed mean is
the cut point that assigns cases with values less than the cut point
to one group and cases with values greater than or equal to the other
group. You must select at least one cut point, and one test is performed
for each cut point chosen.</term>
<term termid="mean_4"><linktext>Mean</linktext>Lower and upper bounds
(two variables) for the prediction interval of the mean predicted
response.</term>
<term termid="mean"><linktext>Mean</linktext>A measure of central
tendency. The arithmetic average, the sum divided by the number of
cases.</term>
<term termid="mean_29"><linktext>Mean</linktext>A measure of central
tendency that represents the arithmetic average value of the variable.</term>
<term termid="mean_spchart"><linktext>Mean</linktext>The process mean.</term>
<term termid="mean_aggregate_function"><linktext>Mean (Aggregate Function)</linktext
>The arithmetic average of the source variable for all cases in the
break group. </term>
<term termid="mean_cox_regression_plots"><linktext>Mean (cox regression
plots)</linktext>The mean for each variable in the model</term>
<term termid="mean_centered"><linktext>Mean Centered</linktext>The
statistic is computed using the mean as the measure of location and
the standard deviation as the measure of variation.</term>
<term termid="mean_cross-product"><linktext>Mean Cross-Product</linktext
>Equivalent to covariance, except the means are assumed to be 0, since
regression is performed through the origin.</term>
<term termid="mean_difference_i-j_1"><linktext>Mean Difference (I-J)</linktext
>The value of the difference in the observed means for the level specified
by I minus the level specified by J.</term>
<term termid="mean_difference_i-j"><linktext>Mean Difference (I-J)</linktext
>The mean for one group minus the mean for the other group.</term>
<term termid="mean_jonckheere-terpstra_statistic"><linktext>Mean Jonckheere-Terpstra
Statistic</linktext>The mean, or expected value of the Jonckheere-Terpstra
statistic.</term>
<term termid="mean_marginal_homogeneity_statistic"><linktext>Mean
Marginal Homogeneity statistic</linktext>The mean, or expected value
of the marginal homogeneity statistic.</term>
<term termid="gbar06"><linktext>Mean of Values (Graph Summary Function)</linktext
>The arithmetic average for the category.</term>
<term termid="mean_probability"><linktext>Mean Probability</linktext
>The mean probability that the target value falls in a given category.</term><?Pub
Caret1?>
<term termid="mean_rank"><linktext>Mean Rank</linktext>The sum of
ranks divided by the number of cases.</term>
<term termid="mean_rank_1"><linktext>Mean Rank</linktext>Average rank
assigned to tied values. For example, in the sequence 10, 15, 15,
15, 16, all cases with the value 15 would receive a rank of 3.</term>
<term termid="mean_score"><linktext>Mean Score</linktext>The survival
time for each case in each group is compared to the survival times
for all cases in the other groups. Roughly speaking, a case's individual
score is increased for each comparison in which it has the higher
survival time and decreased when it has the lower survival time. The
mean score for a group is the average of the individual scores for
cases in the group.</term>
<term termid="mean_square"><linktext>Mean Square</linktext>The sum
of squares divided by the degrees of freedom.</term>
<term termid="mean_numexpr_numexpr_.."><linktext>MEAN(numexpr,numexpr[,..])</linktext
>MEAN(numexpr,numexpr[,..]). Numeric. Returns the arithmetic mean
of its arguments that have valid, nonmissing values. This function
requires two or more arguments, which must be numeric. You can specify
a minimum number of valid arguments for this function to be evaluated.</term>
<term termid="means_3"><linktext>Means</linktext>Summary statistics
for item means. The smallest, largest, and average item means, the
range and variance of item means, and the ratio of the largest to
the smallest item means are displayed.</term>
<term termid="means"><linktext>Means</linktext>Displays total and
group means, as well as standard deviations for the independent variables.</term>
<term termid="means_and_counts"><linktext>Means and Counts</linktext
>Requests the means and frequency counts for each cell. Available
only if the Hierarchical or Experimental method is used.</term>
<term termid="means_and_standard_deviations"><linktext>Means and Standard
Deviations</linktext>Displays the mean, standard deviation, and number
of nonmissing cases for each variable.</term>
<term termid="measure_3"><linktext>Measure</linktext>Measure names
for a doubly multivariate repeated measures design. In this design,
more than one type of measurement was made on each subject, repeatedly.</term>
<term termid="measure_of_size"><linktext>Measure of Size</linktext
>If units are selected with unequal probability, you need a measure
of size (MOS) that defines the size of each unit.  These sizes can
be explicitly defined in a variable or computed from the data.</term>
<term termid="measure_used"><linktext>measure used</linktext>Indicates
the distance measure used.</term>
<term termid="measurement"><linktext>Measurement</linktext>Labels
the repeated measurements taken for each subject.</term>
<term termid="measurement_level_file_information"><linktext>Measurement
Level (file information)</linktext>Measurement level can be nominal,
ordinal, or scale.</term>
<term termid="median_3"><linktext>Median</linktext>Produces a contingency
table that indicates, for each group, the number of cases with values
greater than the observed median and less than or equal to the median.
A chi-square statistic for the table is computed.</term>
<term termid="median_6"><linktext>Median</linktext>The value such
that number of ratios less than this value and the number of ratios
greater than this value are the same.</term>
<term termid="median"><linktext>Median</linktext>The value above and
below which half of the cases fall, the 50th percentile. If there
is an even number of cases, the median is the average of the two middle
cases when they are sorted in ascending or descending order. The median
is a measure of central tendency not sensitive to outlying values
(unlike the mean, which can be affected by a few extremely high or
low values).</term>
<term termid="median_2"><linktext>Median</linktext>The observed median
is the cut point that assigns cases with values less than the cut
point to one group and cases with values greater than or equal to
the other group. You must select at least one cut point, and one test
is performed for each cut point chosen.</term>
<term termid="median_numexpr_numexpr_.."><linktext>MEDIAN(numexpr,numexpr[,..])</linktext
>MEDIAN(numexpr,numexpr[,..]). Numeric. Returns the median (50th percentile)
of its arguments that have valid, nonmissing values. This function
requires two or more arguments, which must be numeric. You can specify
a minimum number of valid arguments for this function to be evaluated.</term>
<term termid="median_centered"><linktext>Median Centered</linktext
>The statistic is computed using the median as the measure of location
and the root mean squares of deviation from the median as the measure
of variation.</term>
<term termid="gbar07"><linktext>Median of Values (Graph Summary Function)</linktext
>The value below which half the cases fall.</term>
<term termid="median_test"><linktext>Median Test</linktext>Tests whether
two or more independent samples are drawn from populations with the
same median using the chi-square statistic. Should not be used if
any cell has an expected frequency less than 1, or if more than 20%
of the cells have expected frequencies less than 5.</term>
<term termid="mersenne_twister"><linktext>Mersenne Twister</linktext
>A newer random number generator that is more reliable for simulation
purposes. If reproducing randomized results from version 12 or earlier
is not an issue, use this random number generator.</term>
<term termid="m-estimators"><linktext>M-Estimators</linktext>Robust
maximum-likelihood estimators of central tendency that differ in the
weights they apply to the cases. Extreme values receive less weight
than values closer to the center. When the data are from a symmetric
distribution with long tails, or when the data have extreme values,
M-estimators provide better estimates of the location than do the
mean or median. The four estimators are Huber's M-estimator, Andrews'
wave estimator, Hampel's redescending M-estimator, and Tukey's biweight
estimator.</term>
<term termid="method"><linktext>Method</linktext>Allows you to select
the method independent variables are entered into the analysis. Allows
you to construct a variety of regression models from the same set
of variables. To obtain Help on a specific method, select the method
and then click the right mouse button.</term>
<term termid="method_2"><linktext>Method</linktext>Lists the available
extraction methods. Choose one of the methods.</term>
<term termid="method_1"><linktext>Method</linktext>Allows you to select
optional methods for stepwise analysis, to control variable entry
and removal criteria, and to display summary statistics. Use stepwise
method must be selected to choose this option.</term>
<term termid="method_7"><linktext>Method</linktext>Displays the variable
selection method used at each step.</term>
<term termid="method_4"><linktext>Method</linktext>Allows you to select
an alternate clustering method or distance measure, or to transform
values or measures.</term>
<term termid="method_test"><linktext>Method Test</linktext>Variable
selection method based on R-squared and its significance for sets
of independent variables.</term>
<term termid="methods"><linktext>Methods</linktext>Lists the methods
for calculating the percentiles.</term>
<term termid="metric"><linktext>Metric</linktext>The interval or ratio
scale on which factor levels are measured.</term>
<term termid="min_value_value_.."><linktext>MIN(value,value[,..])</linktext
>MIN(value,value[,..]). Numeric or string. Returns the minimum value
of its arguments that have valid, nonmissing values. This function
requires two or more arguments. For numeric values, you can specify
a minimum number of valid arguments for this function to be evaluated.</term>
<term termid="minimize_wilks_lambda"><linktext>Minimize Wilks' Lambda</linktext
>A variable selection method for stepwise discriminant analysis that
chooses variables for entry into the equation on the basis of how
much they lower Wilks' lambda. At each step, the variable that minimizes
the overall Wilks' lambda is entered.</term>
<term termid="minimum_17"><linktext>minimum</linktext>The lower bound
'a' for a uniform distribution of the form UNIFORM(a,b).</term>
<term termid="minimum"><linktext>Minimum</linktext>The smallest value
of a numeric variable.</term>
<term termid="minimum_18"><linktext>Minimum</linktext>The minimum
value across all estimated models.</term>
<term termid="minimum_cases_in_node"><linktext>Minimum cases in node</linktext
>The minimum number of cases for a  node specified for the model.
Nodes with less than the minimum will not be split.</term>
<term termid="minimum_expected_cell_frequency"><linktext>Minimum Expected
Cell Frequency</linktext>The smallest number of cases in a cell that
would be expected to occur if the null hypothesis is true. If any
cell has an expected frequency less than 1, or if more than 20% of
the cells have expected frequencies less than 5, the significance
level of the chi-square statistic may not be accurate. Consider combining
similar categories to obtain a table with fewer cells.</term>
<term termid="minimum_mos_value"><linktext>Minimum MOS Value</linktext
>An optional value that sets a lower bound on the MOS, overriding
any values found in the MOS variable or computed from the data.</term>
<term termid="minimum_number_of_units_sampled"><linktext>Minimum Number
of Units Sampled</linktext>An optional value that sets a lower bound
on the number of units sampled when the proportion of units to sample
is specified.</term>
<term termid="minimum_possible"><linktext>Minimum Possible</linktext
>When there are tied observations, the Wald-Wolfowitz runs test calculates
both the minimum number of possible runs, and the maximum possible.</term>
<term termid="minimum_tolerance"><linktext>Minimum Tolerance</linktext
>The minimum tolerance of all independent variables already in the
model if an independent variable not in the equation is included.</term>
<term termid="minimum_value_aggregate_function"><linktext>Minimum
Value (Aggregate Function)</linktext>The smallest value of the source
variable for all cases in the break group. </term>
<term termid="gbar13"><linktext>Minimum Value (Graph Summary Function)</linktext
>The smallest value within the category.</term>
<term termid="minkowski_distance"><linktext>Minkowski distance</linktext
>Dissimilarity measure for continuous data, in an absolute Minkowski
power metric. The distance between two items is the pth root of the
sum of the absolute differences raised to the pth power between the
values for the items.</term>
<term termid="minkowski_distance_measure"><linktext>Minkowski Distance
Measure</linktext>Dissimilarity measure for continuous data, in an
absolute Minkowski power metric. The distance between two items is
the pth root of the sum of the absolute differences raised to the
pth power between the values for the items. Select a power from the
drop-down list; the same value will be used as the root.</term>
<term termid="gaxis06"><linktext>Minor Increment</linktext>Allow you
to divide the area between major increments. The minor increment must
be evenly divisible into the major increment. The minor scale increment
determines the position of minor tick marks and minor grid lines.</term>
<term termid="missing_18"><linktext>Missing</linktext>Multiple response
analysis considers cases with no responses to any items in a multiple
response set to be missing.</term>
<term termid="missing_12"><linktext>Missing</linktext>The number of
missing values.  System-missing values and values outside the range
defined by you are considered as missing.</term>
<term termid="missing"><linktext>Missing</linktext>Values that are
system-missing or user-missing.</term>
<term termid="missing_number_of_cases_aggregate_function"><linktext
>Missing Number of Cases (Aggregate Function)</linktext>The number
of cases in the break group that have missing data for the source
variable.</term>
<term termid="missing_observations"><linktext>Missing Observations</linktext
>Observations with values that either have been defined as user-missing
values or are unknown and have been assigned the system-missing value,
which is indicated with a period (.).</term>
<term termid="missing_or_out-of-range"><linktext>Missing or out-of-range</linktext
>Indicates the number of cases where both of the group codes are missing
or out-of-range.</term>
<term termid="missing_proximities"><linktext>Missing Proximities</linktext
>The number of missing proximities.</term>
<term termid="missing_values_2"><linktext>missing values</linktext
>Missing values include user-missing, system-missing, and, in the
case of log transformed data, values that are zero or negative. Cases
with missing values are excluded from the analysis.</term>
<term termid="missing_values_possible"><linktext>Missing Values</linktext
>The number of possible values for the model to impute.</term>
<term termid="missing_values_file_information"><linktext>Missing Values
(file information)</linktext>User-missing defined missing values (if
any)</term>
<term termid="missing_weights"><linktext>Missing Weights</linktext
>The number of proximities with missing weights.</term>
<term termid="missing_within_the_set"><linktext>Missing within the
set</linktext>The number of objects (cases) that have missing values
on the variables within the set.</term>
<term termid="missing_variable"><linktext>MISSING(variable)</linktext
>MISSING(variable). Logical. Returns 1 or true if variable has a system-
or user-missing value. The argument should be a variable name in the
active dataset.</term>
<term termid="mixed_effects_models"><linktext>Mixed Effects Model</linktext
>A mixed effects model has one or more fixed effects and one or more
random effects. Since the intercept is usually treated as a fixed
effect, a model including the intercept and one or more random factors
is a mixed effects model.</term>
<term termid="mixed-effect_model"><linktext>Mixed-Effect Model</linktext
>An analysis in which some of the independent variables (factors)
are considered fixed and others random.</term>
<term termid="mod_numexpr_modulus"><linktext>MOD(numexpr,modulus)</linktext
>MOD(numexpr,modulus). Numeric. Returns the remainder when numexpr
is divided by modulus. Both arguments must be numeric, and modulus
must not be 0.</term>
<term termid="mode"><linktext>Mode</linktext>The most frequently occurring
value. If several values share the greatest frequency of occurrence,
each of them is a mode.</term>
<term termid="mode_2"><linktext>Mode</linktext>The observed mode is
the cut point that assigns cases with values less than the cut point
to one group and cases with values greater than or equal to the other
group. You must select at least one cut point, and one test is performed
for each cut point chosen.</term>
<term termid="mode_frequencies"><linktext>Mode (Frequencies)</linktext
>The most frequently occurring value. If several values share the
greatest frequency of occurrence, each of them is a mode. The Frequencies
procedure reports only the smallest of such multiple modes.</term>
<term termid="gbar08"><linktext>Mode of Values (Graph Summary Function)</linktext
>The most frequently occurring value within the category.</term>
<term termid="model_20"><linktext>Model</linktext>The dispersion associated
with the model.</term>
<term termid="model_mi"><linktext>Model</linktext>Shows the univariate
model type and list of effects, including interactions, used to impute
the values for each variable.</term>
<term termid="model_23"><linktext>Model</linktext>Statistics computed
for the model as a whole.</term>
<term termid="model_4"><linktext>Model</linktext>Lists the statistical
information for each model.</term>
<term termid="model_effect"><linktext>Model Effect</linktext>The variation
in the dependent variable explained by the model, which passes through
the origin.</term>
<term termid="model_entropy"><linktext>Model Entropy</linktext>A measure
of the predictive accuracy of the binning input variable with respect
to the guide variable.  Smaller values of model entropy indicate greater
predictive accuracy.</term>
<term termid="model_fit"><linktext>Model Fit</linktext>Provides multiple
R, R-squared, adjusted R-squared, and the standard error. Also, an
ANOVA table displays degrees of freedom, sums of squares, mean squares,
F value, and the observed probability of F.</term>
<term termid="model_fitting_criterion"><linktext>Model Fitting Criterion</linktext
>Statistics that are measures of how well the model fits the data.
 You can use them as a basis of comparison for two different models.</term>
<term termid="model_handle_name"><linktext>Model Handle Name</linktext
>The model handle name that identifies this model. This is the name
assigned to this model on the associated MODEL HANDLE command.</term>
<term termid="model_id"><linktext>Model ID</linktext>Model identifier.
By default, it consists of two components: the dependent variable
being modeled, and an autogenerated name consisting of a prefix (default
of 'Model') followed by an integer suffix.</term>
<term termid="model_name"><linktext>Model Name</linktext>The session-unique
name that identifies this model.</term>
<term termid="model_statistics"><linktext>Model statistics</linktext
>Statistics for the specified model</term>
<term termid="model_summary"><linktext>Model Summary</linktext>This
table displays information about variables added and removed from
the model.</term>
<term termid="model_type"><linktext>Model Type</linktext>The type
of predictive model.</term>
<term termid="model_type_1"><linktext>Model Type</linktext>For exponential
smoothing models, this is the name of the model, such as Holt or Winters'
Additive. Non-seasonal ARIMA models are given by ARIMA(p,d,q) where
p is the order of autoregression, d is the order of differencing,
and q is the moving-average order. Seasonal ARIMA models are given
by ARIMA(p,d,q)(P,D,Q) where P, D, and Q are the seasonal autoregression,
differencing, and moving-average orders respectively.</term>
<term termid="monte_carlo_estimate"><linktext>Monte Carlo Estimate</linktext
>An unbiased estimate of the exact significance level, calculated
by repeatedly sampling from a reference set of tables with the same
dimensions and row and column margins as the observed table. The Monte
Carlo method allows you to estimate exact significance without relying
on the assumptions required for the asymptotic method. This method
is most useful when the data set is too large to compute exact significance,
but the data do not meet the assumptions of the asymptotic method.</term>
<term termid="moses_extreme_reactions"><linktext>Moses Extreme Reactions</linktext
>A nonparametric test designed to test hypotheses in which it is expected
that the experimental variable will affect some subjects in one direction
and other subjects in the opposite direction. Tests for extreme responses
compared to the control group. Requires an ordinal scale of measurement.
This test focuses on the span of the control group, and is a measure
of how much extreme values in the experimental group influence the
span when combined with the control group.</term>
<term termid="most_extreme_differences_kolmogorov-smirnov"><linktext
>Most Extreme Differences (KOLMOGOROV-SMIRNOV)</linktext>The largest
differences between two cumulative distribution functions.</term>
<term termid="most_popular_category"><linktext>Most Popular Category</linktext
>The category to which most cases in the peer group belong.</term>
<term termid="moveit_button"><linktext>Moveit Button</linktext>Moves
selected variables from one list to another. Available only when a
pair of variables has been selected.</term>
<term termid="moving_average_arima"><linktext>Moving Average (ARIMA)</linktext
>q is the order of moving average of the process. Specify a non-negative
integer. Enter 0 for an autoregressive process, 1 for a first-order
moving average, 2 for a second-order moving average, and so on.</term>
<term termid="moving_averages"><linktext>Moving Averages</linktext
>In time series analysis, the average of a series value with surrounding
series values. Moving averages are used to smooth a time series--that
is, reduce noise or fluctuation in the series.</term>
<term termid="moving-average_ma_orders"><linktext>Moving-average (MA)
Orders</linktext>Specifies that the model contains moving-average
orders. The details for each lag (order) are provided.</term>
<term termid="moving-average_ma_seasonal_orders"><linktext>Moving-average
(MA) Seasonal Orders</linktext>Specifies that the model contains seasonal
moving-average orders. The details for each lag (order) are provided.</term>
<term termid="multidimensional_scaling_mds"><linktext>Multidimensional
Scaling (MDS)</linktext>A type of data analysis (exploratory, in the
case of ALSCAL) in which the basic data are dissimilarities, or distance
data. The purpose of MDS is to identify and model the structure and
dimensions of a set of stimuli (objects or events) from the dissimilarity
data observed among the stimuli.</term>
<term termid="multiple_category_coordinates"><linktext>Multiple Category
Coordinates</linktext>For non Multiple Nominal variables, the multiple
category coordinates represent the category coordinates in the object
space before constraints are applied.  For Multiple Nominal variables,
they represent the quantifications of the categories.</term>
<term termid="multiple_category_set"><linktext>Multiple Category Set</linktext
>Includes several categorical variables with the same possible responses.
Multiple responses are coded in the different variables, up to the
limit imposed by the number of variables in the set. A multiple dichotomy
set is a different way of coding the same information.</term>
<term termid="multiple_comparison_dimension"><linktext>Multiple Comparison
Dimension</linktext>The level of the factor being compared.</term>
<term termid="multiple_comparisons"><linktext>Multiple Comparisons</linktext
>Procedures for comparing all possible pairs of group means must adjust
for the fact that many comparisons are being made. When many nonindependent
comparisons are being made, the possibility of finding significant
differences by chance increases as the number of comparisons increases.
Multiple comparison procedures attempt to control the error rate by
imposing more stringent criteria for calling observed differences
significant. The various multiple comparison procedures differ in
how they make this correction.</term>
<term termid="csglm_cslogistic_test_statistics_1"><linktext>Multiple
Correspondence Analysis</linktext>This procedure allows you to examine
the relationship between two or more unordered categorical (nominal)
variables graphically in a multidimensional scatterplot. Multiple
correspondence analysis (also called homogeneity analysis) is similar
to correspondence analysis, but it allows you to examine more than
two variables.</term>
<term termid="multiple_dichotomy_set"><linktext>Multiple Dichotomy
Set</linktext>A set of dichotomous (yes/no) variables, each representing
one of the possible responses to a question. Multiple responses are
coded simply by indicating yes for each of the appropriate dichotomies.
A multiple category set is a different way of coding the same information.</term>
<term termid="multiple_fit"><linktext>Multiple Fit</linktext>This
is the fit resulting from not restricting variables to non- Multiple
Nominal quantifications.  It is equal to single fit plus single loss.</term>
<term termid="multiple_line_chart"><linktext>Multiple Line Chart</linktext
>The chart has two or more lines. Each line connects one point from
each category on the category axis. Lines can represent groups of
cases, separate variables, or individual cases.</term>
<term termid="multiple_nominal_categorical_pca"><linktext>Multiple
Nominal (Categorical PCA)</linktext>The quantifications can be different
for each dimension. When all variables are multiple nominal, categorical
principal components analysis produces the same results as homogeneity
analysis.</term>
<term termid="multiple_nominal_variables"><linktext>Multiple Nominal
Variables</linktext>Variables in which categories are treated as unordered,
and objects (cases) in the same category receive the same quantification.</term>
<term termid="multiple_r"><linktext>Multiple R</linktext>The correlation
coefficient between the observed and predicted values of the dependent
variable. It ranges in value from 0 to 1. A small value indicates
that there is little or no linear relationship between the dependent
variable and the independent variables.</term>
<term termid="multiple_response_definitions"><linktext>Multiple Response
Definitions</linktext>Indicates whether or not the data file contains
defined multiple response sets.</term>
<term termid="multiple_response_set"><linktext>Multiple Response Set</linktext
>A way of coding responses to a set of questions either as multiple
dichotomies or as multiple categories; see those two entries for details.
Sometimes this phrase is a synonym for a multiple category set.</term>
<term termid="multiple_response_sets"><linktext>Multiple Response
Sets</linktext>Lists the multiple response sets you have defined.
You can define up to 20 multiple response sets. Each set must have
a unique name. Use Add, Change, and Remove to modify this list.</term>
<term termid="gseasmult"><linktext>Multiplicative Seasonal Adjustment</linktext
>The seasonal component is a factor by which the seasonally adjusted
series is multiplied to yield the original series. In effect, seasonal
components that are proportional to the overall level of the series.
Observations without seasonal variation have a seasonal component
of 1.</term>
<term termid="exsm_smult"><linktext>Multiplicative Seasonal Component
(Exponential Smoothing)</linktext>The series shows periodic variation
at the periodicity defined for it, and the size of the periodic variation
depends on the overall level of the series.</term>
<term termid="multivariate_tests"><linktext>Multivariate Tests</linktext
>Test statistics for evaluating multivariate differences between groups.</term>
<term termid="n_11"><linktext>N</linktext>The number of values associated
with the given measure.</term>
<term termid="n_3"><linktext>N</linktext>Number of Cases.</term>
<term termid="n"><linktext>N</linktext>The number of cases (observations
or records).</term>
<term termid="n_spchart"><linktext>N</linktext>Total sample size.</term>
<term termid="unfiltered_n"><linktext>N (unfiltered)</linktext>The
total number of cases, ignoring filter status.</term>
<term termid="n_distinct_values"><linktext>N Distinct Values</linktext
>The number of distinct values for each binning input variable (before
binning).</term>
<term termid="n_in_the_anomaly_list"><linktext>N in the Anomaly List</linktext
>Number of cases identified as anomalous.</term>
<term termid="n_levels"><linktext>N levels</linktext>Displays the
number of levels for each effect.</term>
<term termid="n_of_cases_1"><linktext>N of cases</linktext>Number
of cases in each group</term>
<term termid="n_of_cases"><linktext>N of Cases</linktext>The value
graphed represents the number of cases in each category.</term>
<term termid="n_of_cases_file_information"><linktext>N of Cases (file
information)</linktext>Number of cases (rows) in the data file.</term>
<term termid="n_of_cumulative_events"><linktext>N of Cumulative Events</linktext
>The number of cases that have experienced the terminal event from
the start of the table until this time.</term>
<term termid="n_of_defined_variable_elements"><linktext>N of Defined
Variable Elements</linktext>For numeric and short string variables
(8 bytes or less), each variable represents a single variable element.
For long string variables, each 8-byte slot represents a single variable
element.</term>
<term termid="n_of_events"><linktext>N of Events</linktext>The number
of cases that experience the terminal event.</term>
<term termid="n_of_items"><linktext>N of Items</linktext>The number
of items used to calculate the associated statistics.</term>
<term termid="n_of_lines_of_documents"><linktext>N of Lines of Documents</linktext
>Number of lines of descriptive document text.</term>
<term termid="model_fit_statistics_1"><linktext>N of Named Variables</linktext
>Number of variables (columns) in the data file.</term>
<term termid="n_of_remaining_cases"><linktext>N of Remaining Cases</linktext
>The number of cases that, at this time, have yet to experience the
terminal event or be censored.</term>
<term termid="n_parameters"><linktext>N Parameters</linktext>Displays
the number of model parameters accounted for by each effect.</term>
<term termid="n_step-halving"><linktext>N step-halving</linktext>The
number times the algorithm has performed the step-halving procedure.</term>
<term termid="n_subjects"><linktext>N Subjects</linktext>Displays
the number of unique subjects defined by the subject variables for
repeated effects.</term>
<term termid="npct_confidence_interval_drop-down_list"><linktext>N%
Confidence Interval Drop-Down List</linktext>Confidence interval for
the prediction intervals. You can select 90, 95, or 99% from the drop-down
list.</term>
<term termid="npct_confidence_interval_of_the_difference_2"><linktext
>N% Confidence Interval of the Difference</linktext>Enter a value
between 1 and 99 to specify the likelihood that a confidence interval
of the mean difference, a range of values based on the sample mean
difference (between scores for each case), contains the population
mean difference. Higher values will give more conservative intervals
(commonly chosen values are 90, 95 or 99).</term>
<term termid="npct_confidence_interval_of_the_difference_1"><linktext
>N% Confidence Interval of the Difference</linktext>Enter a value
between 1 and 99 to specify the likelihood that a confidence interval
of the difference, a range of values based on the difference between
the sample means, includes the difference between the population means.
Higher values will give more conservative intervals (commonly chosen
values are 90, 95 or 99).</term>
<term termid="npct_confidence_interval_of_the_difference"><linktext
>N% Confidence Interval of the Difference</linktext>Enter a percentage
between 1 and 99 to specify the likelihood that the confidence interval
of the difference, a range of values based on the difference between
the sample mean and the Test Value, includes the difference between
the population mean and the Test Value. Higher values will give more
conservative intervals (commonly chosen values are 90, 95 or 99).</term>
<term termid="nagelkerke_r-square"><linktext>Nagelkerke R-square</linktext
>The Nagelkerke R-square is an adjusted version of the Cox &amp; Snell
R-square. The Cox &amp; Snell R-square has a maximum value of less
than 1, even for a "perfect" model. The Nagelkerke R-square adjusts
the scale of the statistic to cover the full range from 0 to 1.</term>
<term termid="name"><linktext>Name</linktext>Enter up to seven characters
for the name of the multiple response set. The program prefixes a
dollar sign to the name you assign. You cannot refer to multiple response
set names in other procedures.</term>
<term termid="nan"><linktext>NAN</linktext>Abbreviation for "not a
number." Appears in output when data or intermediate calculations
yield values outside the effective range of double precision computation.
You should rarely, if ever, encounter this result.</term>
<term termid="natural_log_2"><linktext>Natural log</linktext>Estimates
pertaining to the natural log of the common odds ratio.</term>
<term termid="natural_log_1"><linktext>Natural Log</linktext>Log transforms
the series before estimation using the natural logarithm (base e).</term>
<term termid="natural_log_3"><linktext>Natural Log</linktext>Specifies
that a natural log transformation was performed on the variable.</term>
<term termid="gnormalpp06"><linktext>Natural Log Transformation</linktext
>Uses the natural (base e) logarithms of data values instead of the
values themselves. This transformation requires that all values be
positive.</term>
<term termid="natural_response"><linktext>Natural Response</linktext
>The natural response rate is the probability of getting a response
with no dose.</term>
<term termid="ncdf.beta_quant_shape1_shape2_nc"><linktext>NCDF.BETA(quant,
shape1, shape2, nc)</linktext>NCDF.BETA(quant, shape1, shape2, nc).
Numeric. Returns the cumulative probability that a value from the
noncentral Beta distribution, with the given shape and noncentrality
parameters, will be less than quant.</term>
<term termid="ncdf.chisq_quant_df_nc"><linktext>NCDF.CHISQ(quant,
df,nc)</linktext>NCDF.CHISQ(quant, df, nc). Numeric. Returns the cumulative
probability that a value from the noncentral chi-square distribution,
with df degrees of freedom and the specified noncentrality parameter,
will be less than quant.</term>
<term termid="ncdf.f_quant_df1_df2_nc"><linktext>NCDF.F(quant, df1,
df2,nc)</linktext>NCDF.F(quant, df1, df2, nc). Numeric. Returns the
cumulative probability that a value from the noncentral F distribution,
with degrees of freedom df1 and df2, and noncentrality nc, will be
less than quant.</term>
<term termid="ncdf.t_quant_df_nc"><linktext>NCDF.T(quant, df,nc)</linktext
>NCDF.T(quant, df, nc). Numeric. Returns the cumulative probability
that a value from the noncentral Student's t distribution, with the
specified degrees of freedom df and noncentrality nc, will be less
than quant.</term>
<term termid="negative"><linktext>Negative</linktext>Cases that are
negative; that is, do not fall into the category of interest.</term>
<term termid="negative_differences"><linktext>Negative Differences</linktext
>Lists the number of negative differences. The number of times variable
1 is less than variable 2.</term>
<term termid="negative_extreme_differences"><linktext>Negative Extreme
Differences</linktext>The largest negative difference between two
cumulative distribution functions.</term>
<term termid="negative_ranks"><linktext>Negative Ranks</linktext>Lists
the number of negative ranks. The number of times the rank of variable
1 is less than the rank of variable 2.</term>
<term termid="negative_zero_log_transform"><linktext>negative zero
log transform</linktext>Number of cases identified as having negative
or zero values. Only applies in the case of a log transform.</term>
<term termid="new_value"><linktext>New Value</linktext>Value into
which one or more old values will be recoded. The value must be the
same data type (numeric or string) as the old value.</term>
<term termid="new_variable_rank_cases"><linktext>New Variable (Rank
Cases)</linktext>The name assigned to the new variable that contains
the ranked values for the source variable.</term>
<term termid="next_stage"><linktext>Next stage</linktext>Stage at
which the cluster formed at this stage is merged with another cluster.</term>
<term termid="nmiss_variable_.."><linktext>NMISS(variable[,..])</linktext
>NMISS(variable[,..]). Numeric. Returns a count of the arguments that
have system- and user-missing values. This function requires one or
more arguments, which should be variable names in the active dataset.</term>
<term termid="no_help"><linktext>No Help</linktext>No help available
for this item.</term>
<term termid="exsm_snone"><linktext>No Seasonal Component (Exponential
Smoothing)</linktext>The series does not show periodic variation at
the periodicity defined for it.</term>
<term termid="no_transformation_1"><linktext>No Transformation</linktext
>Specifies that no transformations were performed on the variable.</term>
<term termid="no_trend_exponential_smoothing"><linktext>No Trend (Exponential
Smoothing)</linktext>The level of the series does not show any overall
linear trend.</term>
<term termid="node"><linktext>Node</linktext>A part of the tree that
represents a subset of cases defined by having certain values of independent
(predictor) variables. Node numbers correspond to node numbers in
the tree diagram.</term>
<term termid="node-by-node"><linktext>Node-by-node</linktext>Results
for individual nodes.</term>
<term termid="noise"><linktext>Noise</linktext>An unpredictable source
of variation in a series. A noise series may or may not be white noise.</term>
<term termid="nominal_4"><linktext>Nominal</linktext>The optimal scaling
level. The nominal optimal scaling level applies no restriction on
the order of the quantifications (optimal scale values) of the categories
of a variable.</term>
<term termid="nominal_5"><linktext>Nominal</linktext>A variable can
be treated as nominal when its values represent categories with no
intrinsic ranking (for example, the department of the company in which
an employee works).  Examples of nominal variables include region,
zip code, and religious affiliation.</term>
<term termid="nominal_by_interval_measures"><linktext>Nominal by Interval
Measures</linktext>Nominal by interval measures assume that one variable
is measured on a nominal scale (for example, job category or company
division), and the other is measured on an interval scale (for example,
salary or sales revenue).</term>
<term termid="nominal_measures"><linktext>Nominal Measures</linktext
>Nominal measures assume that variables have values with no intrinsic
order (for example, job category or company division). One or more
of the following can be selected: contingency coefficient, Phi, Cramer's
V, lambda, uncertainty coefficient.</term>
<term termid="non_mnom_variables"><linktext>Non MNOM Variables</linktext
>This refers to variables that are scaled at a level other than Multiple
Nominal.</term>
<term termid="friedmans_chi-square_1"><linktext>NonAdditivity</linktext
>The residual sums of squares can be further separated in order to
test the non-additivity of the items in the scale.  Additivity holds
under the null hypothesis.  The greater the ratio between the mean
square of non-additivity and the mean square of balance, the less
likely additivity holds.</term>
<term termid="noncentrality_parameter"><linktext>Noncentrality Parameter</linktext
>The estimated noncentrality parameter is used in determining the
observed power under the alternative hypothesis for the F test.</term>
<term termid="none_4"><linktext>None</linktext>Suppresses spread-versus-level
plots and the Levene statistic.</term>
<term termid="none_3"><linktext>None</linktext>Suppresses box plots.</term>
<term termid="none_6"><linktext>None</linktext>Suppresses cluster
membership output.</term>
<term termid="none_2"><linktext>None</linktext>Suppresses all interactions
among factors and includes the corresponding sums of squares in the
error sum of squares.</term>
<term termid="none_1"><linktext>None</linktext>Creates no charts.</term>
<term termid="none"><linktext>None</linktext>Factors are not rotated.
This is the default setting.</term>
<term termid="gspectra08"><linktext>None (Spectral Analysis)</linktext
>No smoothing. If this option is chosen, the spectral density estimate
is the same as the periodogram.</term>
<term termid="non-events"><linktext>Non-Events</linktext>The number
of cases in which the event of interest did not occur.</term>
<term termid="nonlinear_canonical_correlation"><linktext>Nonlinear
Canonical Correlation</linktext>This procedure allows you to analyze
the relationships between two or more sets of variables. It allows
you to examine any mix of categorical (ordered and unordered) and
quantitative variables.</term>
<term termid="nonlinear_principal_components"><linktext>Nonlinear
Principal Components</linktext>This procedure allows you to reduce
an original set of variables into a smaller set of uncorrelated variables
that represent most of the information in the original variable set,
while accounting for as much of the variation as possible in the original
set of variables.</term>
<term termid="nonstationarity"><linktext>Nonstationarity</linktext
>Nonstationary series do not have constant mean and variance over
time. Nonstationary series must be made stationary via a suitable
transformation before you can model them using ARIMA. Forms of nonstationarity
include the presence of a trend, or nonconstant variance across series
values. If a trend is indeed present, then a combination of nonseasonal
and seasonal differencing induces stationarity.</term>
<term termid="normal"><linktext>Normal</linktext>The observed cumulative
distribution function is compared to the normal distribution with
the observed mean and standard deviation as parameters.</term>
<term termid="normal_distribution_1"><linktext>Normal distribution</linktext
>The normal, or Gaussian, distribution is defined by its location
(mean) and scale (standard deviation) parameters.  Its density function
has a bell shape which is symmetric about its mean.  About 68% of
the values of a normal variate will fall within 1 standard deviation
of the mean, 95% within 2 standard deviations, and 99.7% within 3.</term>
<term termid="normal_distribution"><linktext>Normal Distribution</linktext
>A symmetric, bell-shaped distribution which plays an important role
in statistical inference.</term>
<term termid="normal_parameters"><linktext>Normal Parameters</linktext
>The parameters of the normal distribution against which the observed
distribution is being tested.</term>
<term termid="normal_probability_plot"><linktext>Normal Probability
Plot</linktext>Displays a normal probability plot of the standardized
residuals. Used to check for normality. If the variable is normally
distributed, the plotted points form a straight diagonal line.</term>
<term termid="normal_scores_rank_cases"><linktext>Normal Scores (Rank
Cases)</linktext>The z scores corresponding to the estimated cumulative
proportion.</term>
<term termid="normal_variates"><linktext>Normal Variates</linktext
>Error terms are randomly drawn from a distribution with the expected
value 0 and the standard deviation equal to the square root of the
mean squared error term of the regression.</term>
<term termid="normal_stddev"><linktext>NORMAL(stddev)</linktext>NORMAL(stddev).
Numeric. Returns a normally distributed pseudorandom number from a
distribution with mean 0 and standard deviation stddev, which must
be a positive number. You can repeat the sequence of pseudorandom
numbers by setting a seed in the Random Number Seed dialog box before
each sequence.</term>
<term termid="normality_plots_with_tests"><linktext>Normality Plots
with Tests</linktext>Displays normal probability and detrended probability
plots. The Kolmogorov-Smirnov statistic with a Lilliefors significance
level for testing normality is displayed. The Shapiro-Wilk statistic
is calculated if the sample size does not exceed 50.</term>
<term termid="normalize"><linktext>NORMALIZE(strexp).</linktext>NORMALIZE(strexp).
 String.  Returns the normalized version of strexp.  In Unicode mode,
it returns Unicode NFC.  In code page mode, it has no effect and returns
strexp unmodified.  The length of the result may be different from
the length of the input.</term>
<term termid="normalized_bayesian_information_criterion_bic"><linktext
>Normalized  BIC</linktext>Normalized Bayesian Information Criterion.
A general measure of the overall fit of a model that attempts to account
for model complexity. It is a score based upon the mean square error
and includes a penalty for the number of parameters in the model and
the length of the series. The penalty removes the advantage of models
with more parameters, making the statistic easy to compare across
different models for the same series.</term>
<term termid="normalized_importance"><linktext>Normalized importance</linktext
>Measures how strongly a variable acts as a primary or surrogate predictor
relative to the variable having the largest importance value.  Ranges
from 0 to 100, with the variable having the largest importance value
scored as 100.</term>
<term termid="normalized_raw_stress"><linktext>Normalized Raw Stress</linktext
>The procedure attempts to minimize the normalized raw stress of the
solution.</term>
<term termid="notes"><linktext>Notes</linktext>No help is available
for items in the Notes table.</term>
<term termid="def_note"><linktext>Notes (output item type)</linktext
>Information about the execution of the procedure. It includes the
missing value information, weighting or filtering information, and
the syntax command(s). It can be activated as a table object.</term>
<term termid="npdf.beta"><linktext>NPDF.BETA</linktext>NPDF.BETA(quant,
shape1, shape2, nc). Numeric. Returns the probability density of the
noncentral beta distribution, with the given shape and noncentrality
parameters, at quant.</term>
<term termid="npdf.chisq"><linktext>NPDF.CHISQ</linktext>NPDF.CHISQ(quant,
df, nc). Numeric. Returns the probability density of the noncentral
chi-square distribution, with df degrees of freedom and the specified
noncentrality parameter, at quant.</term>
<term termid="npdf.f"><linktext>NPDF.F</linktext>NPDF.F(quant, df1,
df2, nc). Numeric. Returns the probability density of the noncentral
F distribution, with degrees of freedom df1 and df2 and noncentrality
nc, at quant.</term>
<term termid="npdf.t"><linktext>NPDF.T</linktext>NPDF.T(quant, df,
nc). Numeric. Returns the probability density of the noncentral Student's
t distribution, with the specified degrees of freedom df and noncentrality
nc, at quant.</term>
<term termid="ntiles_rank_cases"><linktext>Ntiles (Rank Cases)</linktext
>Ranks are based on percentile groups, with each group containing
approximately the same number of cases. For example, 4 Ntiles would
assign a rank of 1 to cases below the 25th percentile, 2 to cases
between the 25th and 50th percentile, 3 to cases between the 50th
and 75th percentile, and 4 to cases above the 75th percentile.</term>
<term termid="ntrim"><linktext>NTRIM(varname)</linktext>NTRIM(varname).
 Returns the value of varname, without removing trailing blanks. The
value of varname must be a variable name; it cannot be an expression. </term>
<term termid="null"><linktext>Null</linktext>The model fit without
parameters or intercept.  If the difference in the fitted and null
model -2 log likelihoods is large, then you can reject the hypothesis
that all the parameter coefficients are 0.</term>
<term termid="gnullhyp"><linktext>Null hypothesis</linktext>A precise,
testable statement about the population from which a sample of cases
comes. Typically the null hypothesis is the opposite of the real hypothesis
of interest. It might state, for example, that a statistic equals
0 in the population, or that the values of two subgroup statistics
are equal in the population.</term>
<term termid="null_hypothesis_model_test_of_parallel_slopes"><linktext
>Null hypothesis model (test of parallel slopes)</linktext>The null
hypothesized model assumes equal slopes for all categories</term>
<term termid="gbar19"><linktext>Number Above (Graph Summary Function)</linktext
>The number of cases above the specified value.</term>
<term termid="number_at_risk"><linktext>Number at risk</linktext>The
number of subjects in the study during the interval between the listed
event time and the previous event time.</term>
<term termid="gbar20"><linktext>Number Below (Graph Summary Function)</linktext
>The number of cases below the specified value.</term>
<term termid="number_entering_interval"><linktext>Number Entering
Interval</linktext>The number of surviving cases at the beginning
of the interval.</term>
<term termid="number_exposed_to_risk"><linktext>Number Exposed to
Risk</linktext>The number of surviving cases minus one half the censored
cases.  This is intended to account for the effect of the censored
cases.</term>
<term termid="number_format_alphabetic"><linktext>Number Format Alphabetic</linktext
>Displays footnote markers as characters of the alphabet.</term>
<term termid="number_format_numeric"><linktext>Number Format Numeric</linktext
>Displays footnote markers as numerals.</term>
<term termid="number_of_bins"><linktext>Number of Bins</linktext>The
number of bins generated for each binning input variable by the procedure.</term>
<term termid="greport03"><linktext>Number of Cases</linktext>Number
of cases in the break category.</term>
<term termid="number_of_cases_aggregate_function"><linktext>Number
of Cases (Aggregate Function)</linktext>The (weighted) number of cases
in the break group with valid data for the source variable, unless
modified by one of the check boxes below.</term>
<term termid="gbar09"><linktext>Number of Cases (Graph Summary Function)</linktext
>The number of cases having a nonmissing value of the selected variable.
If there are no missing values, this is the same as N of cases in
the previous dialog box.</term>
<term termid="number_of_cases_by_level_of_guide_variable"><linktext
>Number of Cases by Level of Guide Variable</linktext>The cross-classification
of bins by values of the guide variable.</term>
<term termid="number_of_categories_1"><linktext>Number of Categories</linktext
>The number of categories defined for each variable.</term>
<term termid="number_of_clusters_2"><linktext>Number of Clusters</linktext
>The number of clusters in a given solution.</term>
<term termid="number_of_clusters_1"><linktext>Number of clusters</linktext
>Stages are identified by the number of clusters at that stage. At
the starting point (the stage with the largest number) there are as
many clusters as there are cases; at the ending point (stage 1), there
is a single cluster with all cases.</term>
<term termid="number_of_duplicates"><linktext>Number of Duplicates</linktext
>The number of cases in the duplicate identifiers group.</term>
<term termid="number_of_events"><linktext>Number of Events</linktext
>The number of subjects in the study who experience the event of interest
at the event time.</term>
<term termid="number_of_factors"><linktext>Number of Factors</linktext
>Extracts a user-specified number of factors, regardless of their
eigenvalues. Enter a positive number.</term>
<term termid="n_hidden_layer"><linktext>Number of Hidden Layers</linktext
>A multilayer perceptron can have one or two hidden layers.</term>
<term termid="number_of_invalid_cases"><linktext>Number of Invalid
Cases</linktext>The number of cases with mean estimates out of range
for the model's link function.</term>
<term termid="number_of_iterations_1"><linktext>Number of Iterations</linktext
>The number of iterations performed by the algorithm.</term>
<term termid="number_of_levels_1"><linktext>Number of Levels</linktext
>Displays the number of levels of the variable.</term>
<term termid="number_of_measurements_per_subject"><linktext>Number
of Measurements per Subject</linktext>The number of unique combinations
of within-subject effect levels.  This is not necessarily constant
across subjects; for example, different patients may make different
numbers of visits to the hospital during a study.  The minimum and
maximum observed number of measurements among subjects is displayed.</term>
<term termid="number_of_missing_values"><linktext>Number of missing
values</linktext>The number of cases/objects that have values outside
the valid range for a variable. The valid range for a variable is
the range 1 through the specified number of categories.</term>
<term termid="number_of_nodes"><linktext>Number of nodes</linktext
>Number of nodes (including root node) in the final tree model. Since
node counting starts with 0 for the root node, the highest node number
in the tree is one less than the number of nodes reported here.</term>
<term termid="number_of_outliers"><linktext>Number of Outliers</linktext
>The number of outliers included in the estimated model.</term>
<term termid="number_of_predictors"><linktext>Number of Predictors</linktext
>The number of predictors (independent variables) included in the
estimated model. When the Expert Modeler is used, this may be less
than the number of specified predictors, since predictors that do
not have a statistically significant relationship with the dependent
variable are dropped.</term>
<term termid="number_of_predictors_with_missing_values"><linktext
>Number of predictors with missing values</linktext>Lists the number
of predictors with missing values. The missing values are replaces
with the mean.</term>
<term termid="n_of_replaced_missing_values"><linktext>Number of Replaced
Missing Values</linktext>Number of cases for which missing values
were replaced.</term>
<term termid="number_of_responses_2"><linktext>Number of Responses</linktext
>The number of subjects in the specified grouping that received the
specified dosage and experienced the event of interest.</term>
<term termid="number_of_responses_gt_number_of_subjects"><linktext
>Number of Responses &gt; Number of Subjects</linktext>Cases rejected
because the number of responses exceeded the number of subjects.</term>
<term termid="number_of_runs"><linktext>Number of Runs</linktext>The
number of sequences of repetitions of the same outcome.</term>
<term termid="number_of_step-halvings"><linktext>Number of Step-halvings</linktext
>At each iteration, the step size is reduced by a factor of 0.5 until
the log-likelihood increases or maximum step-halving is reached.</term>
<term termid="number_of_subjects"><linktext>Number of Subjects</linktext
>The number of subjects in the specified grouping that received the
specified dosage.</term>
<term termid="number_of_subjects2"><linktext>Number of Subjects</linktext
>The number of unique combinations of subject effect levels.  This
is not necessarily the product of the numbers of levels of subject
effects; for example, while subjects may be defined by the combination
of Hospital ID and Patient ID to avoid problems with non-unique Patient
ID variables across hospitals, Patient ID&#x2019;s are also unlikely
to be exactly duplicated across all hospitals in the study.</term>
<term termid="n_weight_adj"><linktext>Number of Synaptic Weight Adjustments</linktext
>The total number of times a weight's value was updated by the algorithm.</term>
<term termid="number_of_terminal_events"><linktext>Number of Terminal
Events</linktext>The number of cases that experience the terminal
event in this interval.</term>
<term termid="number_of_terminal_nodes"><linktext>Number of terminal
nodes</linktext>The number of nodes at the ends of the branches, which
are not split any further.</term>
<term termid="n_units"><linktext>Number of Units</linktext>There is
one unit for each covariate and each level of each factor.</term>
<term termid="n_units_rbf"><linktext>Number of Units (RBF)</linktext
>In the input and output layers, there is one unit for each covariate
and each level of each factor.  In the hidden layers, the number of
units is user-specified or determined automatically by the procedure.</term>
<term termid="n_units_hidden_layer"><linktext>Number of Units in Hidden
Layer</linktext>The number of units in each hidden layer can be specified
explicitly or determined automatically by the estimation algorithm.</term>
<term termid="number_of_units_sampled"><linktext>Number of Units Sampled</linktext
>The user-specified number of sampling units drawn at this stage.</term>
<term termid="number_of_valid_cases"><linktext>Number of Valid Cases</linktext
>Number of valid cases for the new variable. Depending on the method
used and the position of missing values in the data file, this may
not be equal to the total number of cases in the file.</term>
<term termid="number_of_variables"><linktext>Number of Variables</linktext
>Number of variables in the model.</term>
<term termid="number_of_violations"><linktext>Number of Violations</linktext
>The number of cases whose variable values violate the rule.</term>
<term termid="number_withdrawing_during_interval"><linktext>Number
Withdrawing during Interval</linktext>The number of censored cases
in this interval.</term>
<term termid="number_strexpr_format"><linktext>NUMBER(strexpr,format)</linktext
>NUMBER(strexpr,format). Numeric. Returns the value of the string
expression strexpr as a number. The second argument, format, is the
numeric format used to read strexpr. For example, NUMBER(stringDate,DATE11)
converts strings containing dates of the general format dd-mmm-yyyy
to a numeric number of seconds that represent that date. (To display
the value as a date, use the FORMATS or PRINT FORMATS command.) If
the string cannot be read using the format, this function returns
system-missing.</term>
<term termid="numerator"><linktext>Numerator</linktext>The numerator
variables.</term>
<term termid="numerator_orders"><linktext>Numerator Orders</linktext
>Specifies that the transfer function for the associated independent
variable contains numerator orders. The details for each lag (order)
are provided.</term>
<term termid="numeric_variable"><linktext>Numeric Variable</linktext
>A variable whose values are numbers. Values are displayed in standard
numeric format, using the decimal delimiter specified in the Regional
Setting control panel. The Data Editor accepts numeric values in standard
format; or in scientific notation.</term>
<term termid="gnumericvariable"><linktext>Numeric Variable (Syntax)</linktext
>A variable whose values are numbers. Numeric variables can be displayed
in many different formats, including the number of digits and decimal
places displayed, as well as currency, date, and time formats. None
of these formats affects the way the program represents the data internally.
Numeric variables can be converted to or from string variables using
the STRING and NUMBER functions in the transformation language. In
command syntax you can specify numeric values as integers, decimal
fractions, or in scientific notation.</term>
<term termid="numerical"><linktext>Numerical</linktext>The optimal
scaling level. A numerical optimal scaling level applies a linear
restriction on the quantifications (optimal scale values) of the categories
of a variable.</term>
<term termid="nvalid_variable_.."><linktext>NVALID(variable[,..])</linktext
>NVALID(variable[,..]). Numeric. Returns a count of the arguments
that have valid, nonmissing values. This function requires one or
more arguments, which should be variable names in the active dataset.</term>
<term termid="object_7"><linktext>Object</linktext>The objects in
the analysis.</term>
<term termid="object"><linktext>Object</linktext>Objects are the basic
units of analysis, or cases.</term>
<term termid="object_points_labeled_by"><linktext>Object Points Labeled
by</linktext>The object points on the plot are marked using the category
labels of the indicated variable(s).</term>
<term termid="object_scores"><linktext>Object scores</linktext>Optimal
score assigned to an object (case) in a particular dimension.</term>
<term termid="object_weight"><linktext>Object Weight</linktext>This
is the positive integer weight given to the object.</term>
<term termid="objects"><linktext>Objects</linktext>The number of objects
for which the proximity matrices are defined.</term>
<term termid="objects_labeled_by"><linktext>Objects Labeled by</linktext
>The object on the plot are marked using the category labels of the
indicated variable(s).</term>
<term termid="observed_tree"><linktext>Observed (Tree)</linktext>For
each case, the observed category is the actual category for the dependent
variable in the data file.</term>
<term termid="observed_control_group_span"><linktext>Observed Control
Group Span</linktext>Observations from the control and experimental
groups are combined and ranked. The span is the highest rank of an
observation in the control group minus the lowest rank of an observation
in the control group plus 1. The span is used to determine if the
control and experimental groups come from the same population.</term>
<term termid="observed_count"><linktext>Observed Count</linktext>The
actual number of cases in the cell.</term>
<term termid="observed_frequency"><linktext>Observed Frequency</linktext
>The number or percentage of observed cases that match a particular
covariate pattern.</term>
<term termid="observed_n"><linktext>Observed N</linktext>The number
of observed observations in each category.</term>
<term termid="observed_power"><linktext>Observed Power</linktext>The
observed power gives the probability that the F test will detect the
differences between groups equal to those implied by the sample difference.</term>
<term termid="observed_proportion"><linktext>Observed Proportion</linktext
>The proportion of the cases having a particular value.</term>
<term termid="observed_significance_level"><linktext>Observed Significance
Level</linktext>Often called the p value. The basis for deciding whether
or not to reject the null hypothesis. It is the probability that a
statistical result as extreme as the one observed would occur if the
null hypothesis were true. If the observed significance level is small
enough, usually less than 0.05 or 0.01, the null hypothesis is rejected.</term>
<term termid="ochiai_similarity_measure"><linktext>Ochiai Similarity
Measure</linktext>A binary version of the cosine of the angle between
vectors of values, with a range of 0 to 1. Computed from a fourfold
table as SQRT(a**2/((a+b)(a+c))) where a represents cases present
on both items, and b and c represent cases present on one item but
absent on the other.</term>
<term termid="odds_ratio"><linktext>Odds Ratio</linktext>The ratio
of the odds for the first variable and the odds for the second variable.</term>
<term termid="odds_ratios"><linktext>Odds Ratios</linktext>The ratio
of the odds at each category of the factor to the odds at the specified
reference category.</term>
<term termid="of_dimension_to_inerita_of_point"><linktext>Of dimension
to inertia of point</linktext>The proportion of the inertia of a point
accounted for by each dimension. This measure indicates how well each
point is represented in the solution.</term>
<term termid="of_point_to_inertia_of_dimension"><linktext>Of point
to inertia of dimension</linktext>The proportion of the inertia of
each dimension accounted for by a point. This measure indicates the
importance of each point to each dimension.</term>
<term termid="off-diagonal_cases"><linktext>Off-Diagonal Cases</linktext
>Displays the number of cases that are not on the diagonal.</term>
<term termid="offset_variable"><linktext>Offset</linktext>The offset
term is a "structural" predictor whose coefficient is not estimated
by the model; it is assumed to be 1. Its effect is to shift the value
of the linear predictor of the dependent variable. This is especially
useful in Poisson regression models where each case may have different
levels of exposure to the event of interest; for example, when modeling
accident rates for individual drivers, there is an important difference
between a driver who has been at fault in 1 accident in 3 years of
experience and a driver who has been at fault in 1 accident in 25
years! The number of accidents can be modeled as a Poisson response
if the experience of the driver is included as an offset term.</term>
<term termid="ok_6"><linktext>OK</linktext>Runs the command and closes
the dialog box. The Paired Variables list must contain at least one
pair of variables before you can click OK.</term>
<term termid="ok_5"><linktext>OK</linktext>Runs the command and closes
the dialog box. The Test Variable(s) list must contain at least one
variable and the Grouping Variable must contain a variable with defined
groups before you can click OK.</term>
<term termid="ok_1"><linktext>OK</linktext>Runs the command and closes
the dialog box. The Dependent List must contain at least one variable
before you can click OK.</term>
<term termid="ok_2"><linktext>OK</linktext>Runs the command and closes
the dialog box. The Variable list must contain at least one variable
before you can click OK.</term>
<term termid="ok_3"><linktext>OK</linktext>Runs the command and closes
the dialog box. You must select one dependent variable and at least
one independent variable before you can click OK.</term>
<term termid="ok"><linktext>OK</linktext>Runs the command and closes
the dialog box. The Row(s) list and the Column(s) list must each contain
at least one variable before you can click OK.</term>
<term termid="ok_14"><linktext>OK</linktext>Runs the command and closes
the dialog box. The Test Variables list must contain at least two
variables before you can click OK.</term>
<term termid="ok_42"><linktext>OK</linktext>Runs the command and closes
the dialog box.</term>
<term termid="ok_16"><linktext>OK</linktext>Runs the command and closes
the dialog box. You must select a data file (Browse) and define one
or more variables before you can click on OK.</term>
<term termid="ok_4"><linktext>OK</linktext>Runs the command and closes
the dialog box. The Test Variable(s) list must contain at least one
variable before you can click OK.</term>
<term termid="ok_15"><linktext>OK</linktext>Runs the command and closes
the dialog box. The Independents list must contain at least one variable
and the Grouping Variable list must contain a variable with a defined
range before you can click OK.</term>
<term termid="ok_7"><linktext>OK</linktext>Runs the command and closes
the dialog box. The Dependent list must contain a variable and the
Factor list must contain at least one variable with defined ranges
before you can click OK.</term>
<term termid="ok_13"><linktext>OK</linktext>Runs the command and closes
the dialog box. The Test Pair(s) List must contain at least one pair
of variables before you can click OK.</term>
<term termid="ok_12"><linktext>OK</linktext>Runs the command and closes
the dialog box. The Test Variable List must contain at least one variable
and the Grouping Variable must contain a variable with defined range
before you can click OK.</term>
<term termid="ok_11"><linktext>OK</linktext>Runs the command and closes
the dialog box. The Test Variable List must contain at least one variable
before you can click OK.</term>
<term termid="ok_10"><linktext>OK</linktext>Runs the command and closes
the dialog box. You must select at least one Dependent variable and
an Independent Variable that can be either a variable in the active
dataset or Time before you can click OK.</term>
<term termid="ok_9"><linktext>OK</linktext>Runs the command and closes
the dialog box. The Variables list must contain at least two variables
and the Controlling for variable list must contain at least one variable
before you can click OK.</term>
<term termid="ok_8"><linktext>OK</linktext>Runs the command and closes
the dialog box. The Variables list must contain at least two variables
before you can click OK.</term>
<term termid="olap"><linktext>OLAP</linktext>An OLAP cube is a table
of results summarized across several grouping variables, which can
then be manipulated or rearranged interactively.  For example, you
may have sales figures summarized by geographic region, product type,
customer type, month, and sales indicator (units ordered, revenue,
profit, and so on).</term>
<term termid="omitted_category"><linktext>Omitted Category</linktext
>Category omitted for deviation or simple contrasts.</term>
<term termid="one_minus_survival"><linktext>One Minus Survival</linktext
>Plots one-minus the survival function on a linear scale.</term>
<term termid="one-tailed_asymptotic_significance"><linktext>One-Tailed
Asymptotic Significance</linktext>The probability, based on the asymptotic
distribution of a test statistic and assuming that the data set is
large, of obtaining results as extreme as the one observed, when the
null hypothesis is true. A one-tailed significance level tests a null
hypothesis in which the direction of an effect is specified in advance.</term>
<term termid="one-tailed_significance"><linktext>One-Tailed Significance</linktext
>The probability of obtaining results as extreme as the one observed,
and in the same direction when the null hypothesis is true. Tests
a null hypothesis in which the direction of an effect is specified
in advance.</term>
<term termid="optimal_scaling_level"><linktext>Optimal Scaling Level</linktext
>The optimal scaling level assigned to each variable.</term>
<term termid="optimal_solution_found"><linktext>Optimal Solution Found</linktext
>If the optimal solution is not found, consider changing the algorithm
criteria.</term>
<term termid="optimality_tolerance"><linktext>Optimality Tolerance</linktext
>This can be thought of as the precision (number of significant digits)
in the objective function at the solution; thus if the tolerance is
0.000001, the objective function should have about six significant
digits. The optimality tolerance must be greater than the Function
Precision. The default optimality tolerance is a very small platform-dependent
number.</term>
<term termid="options_2"><linktext>Options</linktext>Allows you to
choose additional descriptive statistics or to specify the order in
which variables are displayed.</term>
<term termid="options_1"><linktext>Options</linktext>Allows you to
specify confidence level for the interval to be displayed and to control
the treatment of missing values.</term>
<term termid="options_3"><linktext>Options</linktext>Allows you to
choose an alternate method of partitioning the sums of squares; to
specify the order of covariate entry; to request optional statistics;
and to limit the order of interactions evaluated among factors.</term>
<term termid="options_4"><linktext>Options</linktext>Allows you to
obtain optional statistics, and change the treatment of missing values.</term>
<term termid="options_5"><linktext>Options</linktext>Allows you to
control the criteria by which variables are chosen for entry or removal
from the regression model; to suppress the constant term; and to control
the handling of missing values.</term>
<term termid="options_6"><linktext>Options</linktext>Allows you to
modify the treatment of missing values.</term>
<term termid="options_7"><linktext>Options</linktext>Allows you to
request additional statistics and to determine the treatment of missing
values.</term>
<term termid="options_8"><linktext>Options</linktext>Allows you to
obtain optional summary statistics and to modify the treatment of
cases with missing values.</term>
<term termid="options"><linktext>Options</linktext>Allows you to modify
the treatment of missing values or sort the display of factor matrices.</term>
<term termid="order_1"><linktext>Order</linktext>Lists the order of
the 5 highest and 5 lowest observations.</term>
<term termid="order_2"><linktext>Order</linktext>Cases are displayed
in order of magnitude of the statistic.</term>
<term termid="ordinal_7"><linktext>Ordinal</linktext>A variable can
be treated as ordinal when its values represent categories with some
intrinsic ranking (for example, levels of service satisfaction from
highly dissatisfied to highly satisfied).  Examples of ordinal variables
include attitude scores representing degree of satisfaction or confidence
and preference rating scores.</term>
<term termid="ordinal_5"><linktext>Ordinal</linktext>The optimal scaling
level. An ordinal optimal scaling level applies an order restriction
on the quantifications (optimal scale values) of the categories of
a variable.</term>
<term termid="ordinal_measures"><linktext>Ordinal Measures</linktext
>Ordinal measures assume that variables have values with some intrinsic
order (for example, low, medium, high; strongly agree, agree, disagree,
strongly disagree). One or more of the following can be selected:
gamma, Somers' d, Kendall's tau-b, Kendall's tau-c. Note: Ordinal
variables can be either numeric codes that represent categories (for
example, 1 = low, 2 = medium, 3 = high) or string values. However,
the alphabetic order of string values is assumed to reflect the true
order of the categories. For example, for a string variable with the
values of low, medium, high, the order of the categories is interpreted
as high, low, medium--which is not the correct order. In general,
it is more reliable to use numeric codes to represent ordinal data.</term>
<term termid="ordinal_optimal_scaling_level"><linktext>Ordinal optimal
scaling level</linktext>An ordinal optimal scaling level applies an
order restriction on the quantifications (optimal scale values) of
the categories.</term>
<term termid="gaxis07"><linktext>Origin Line (Bar Charts)</linktext
>A baseline from which data bars hang or extend.</term>
<term termid="original_value"><linktext>Original Value</linktext>The
value of the original categorical variable before recoding</term>
<term termid="original_value_1"><linktext>Original Value</linktext
>Variable value before recoding.</term>
<term termid="other"><linktext>Other</linktext>Displays one or more
of the following: Pearson chi-square, likelihood-ratio chi-square,
Mantel-Haenszel chi-square, Fisher's exact test, Pearson's r, Spearman's
correlation coefficient, Cohen's kappa, relative risk ratio.</term>
<term termid="def_other"><linktext>Other (output item type)</linktext
>OLE items from other programs inserted in the current Viewer. Objects
that can be inserted into an output document are listed in the Insert
Object dialog box.</term>
<term termid="other_summary_function"><linktext>Other Summary Function</linktext
>The value graphed represents the function displayed for the selected
variable. To specify a different function, click Change Statistic.
The selected variable must be numeric.</term>
<term termid="out_of_range"><linktext>Out of Range</linktext>Cases
rejected because of factor values out of the range specified.</term>
<term termid="outlier"><linktext>outlier</linktext>An outlier is an
observation whose value is distant from the values of the majority
of observations.  It is sometimes more technically defined as a value
whose distance from the nearest quartile is greater than 1.5 times
the interquartile range.  Outliers pull the mean in their direction,
and should always be carefully examined.</term>
<term termid="gbox01"><linktext>Outlier (Box plots/Explore)</linktext
>Cases with values between 1.5 and 3 box lengths from the upper or
lower edge of the box. The box length is the interquartile range.</term>
<term termid="outlier_-1"><linktext>Outlier(-1)</linktext>The identifying
cluster for cases that have been discarded from the analysis as outliers.</term>
<term termid="outliers"><linktext>Outliers</linktext>Displays cases
with the five largest and five smallest values. These are labeled
Extreme Values in the output.</term>
<term termid="outliers_outside_n_standard_deviations"><linktext>Outliers
Outside n Standard Deviations</linktext>After selecting this option,
enter a positive standard deviation value that limits casewise diagnostics
to cases with an absolute standardized residual value greater than
the entered value.</term>
<term termid="outliers_trimmed_from_each_end"><linktext>Outliers Trimmed
from Each End</linktext>5% of the control cases are trimmed from each
end of the control group since outliers can distort the range of the
span.</term>
<term termid="def_outlineview"><linktext>Outline View (Viewer)</linktext
>Displays output in an outline. You can hide or display individual
items, or collapse all the items under one or more headings. You can
also drag items within the outline, move items right or left, or insert
new items. Choose Outlining Toolbar from the View menu to access tools
for manipulating the outline.</term>
<term termid="output_layer"><linktext>Output Layer</linktext>The output
layer contains the target (dependent) variables.</term>
<term termid="output_variables_are_strings"><linktext>Output variables
are strings</linktext>Defines the new, recoded variable as a string
(alphanumeric) variable. The old variable can be numeric or string.</term>
<term termid="overall_4"><linktext>Overall</linktext>The total number
of cases included in the analysis, plus the percentage of total cases
these represent.</term>
<term termid="overall_5"><linktext>Overall</linktext>The comparison
of all groups.</term>
<term termid="overall_score"><linktext>Overall (score)</linktext>Score
statistic and associated tests, used to evaluate model fit.</term>
<term termid="overall_percent"><linktext>Overall percent</linktext
>Overall percentage predicted for each category and overall percentage
of correct predictions.</term>
<term termid="overall_percent_correct"><linktext>Overall Percent Correct</linktext
>The overall percentage of cases correctly classified within each
sample.</term>
<term termid="overall_percentage_1"><linktext>Overall Percentage</linktext
>Percentage of cases correctly classified across all predicted categories</term>
<term termid="overall_percentage"><linktext>Overall Percentage</linktext
>The percentage of cases classified, by predicted category.</term>
<term termid="overview_total"><linktext>Overview total</linktext>The
total proportion of the inertia of a point accounted for by the dimensions
in the analysis.</term>
<term termid="gcontrol03"><linktext>p and np Charts</linktext>Plot
the number or ratio of nonconforming units. p charts plot the ratio
of defective units to the total number inspected within each subgroup
or time interval, while np charts plot the actual number of defective
units. Use p charts rather than np charts if your data contain unequal
sample sizes.</term>
<term termid="p_value"><linktext>p value</linktext>Also known as a
significance value.  Most statistical tests, regardless of the statistic
used in the calculation of the test, report significance values that
lie on a common scale from 0 to 1.  Decreasing significance values
indicate evidence against the null hypothesis.  Generally, values
less than 0.05 mean the data are inconsistent with the null; values
greater than 0.10 mean the data are consistent with the null.</term>
<term termid="p_dgtdorgeqg"><linktext>P(D&gt;d|G=g)</linktext>The
probability of the observed score given membership in the most likely
group.</term>
<term termid="p_geqgordeqd"><linktext>P(G=g|D=d)</linktext>The posterior
probability that a case belongs to the predicted group.</term>
<term termid="paired_differences"><linktext>Paired Differences</linktext
>Displays results for the paired differences.</term>
<term termid="paired_variables"><linktext>Paired Variables</linktext
>Displays the pairs of variables you have chosen for the analysis.</term>
<term termid="paired-samples_t_test"><linktext>Paired-Samples T Test</linktext
>A statistical test of the null hypothesis that two population means
are equal. It is used when the observations for the two groups can
be paired in some way. (For example, when the same person is observed
before and after a treatment.) Pairing is used to make the two groups
as similar as possible. Observed differences between the groups can
then be attributed more readily to the variable of interest.</term>
<term termid="pairs"><linktext>Pairs</linktext>Lists the paired data.</term>
<term termid="pairwise_4"><linktext>pairwise</linktext>When computing
a measure of association between two variables in a larger set, cases
are included in the computation when the two variables have nonmissing
values, irrespective of the values of the other variables in the set.</term>
<term termid="pairwise_5"><linktext>Pairwise</linktext>The comparison
of two groups.</term>
<term termid="pairwise_3"><linktext>Pairwise</linktext>Displays for
each pair of quantitative variables the number of pairwise nonmissing
values, and the pairwise mean, variance, covariance, and correlation.
Each computation is performed using all values for which both variables
have nonmissing values.</term>
<term termid="pairwise_for_each_stratum"><linktext>Pairwise for each
Stratum</linktext>Compares each distinct pair of factor levels for
each stratum. Pairwise trend tests are not available. If you do not
have a stratification variable, the tests are not performed.</term>
<term termid="pairwise_over_strata"><linktext>Pairwise Over Strata</linktext
>Compares each distinct pair of factor levels. Pairwise trend tests
are not available.</term>
<term termid="parallelism_test"><linktext>Parallelism Test</linktext
>A test of the hypothesis that all factor levels have a common slope.</term>
<term termid="parameter"><linktext>Parameter</linktext>Unknown value
estimated by the program.</term>
<term termid="parameter_change_tolerance"><linktext>Parameter change
tolerance</linktext>Iteration stops if no parameter changes by more
than 0.001 from one iteration to the next. You can choose a smaller
or larger value for more or less precision in the parameter estimates.
For greater precision, it may also be necessary to increase the maximum
iterations.</term>
<term termid="parameter_coding"><linktext>Parameter coding</linktext
>Value(s) used to encode the corresponding category</term>
<term termid="parameter_convergence"><linktext>Parameter Convergence</linktext
>The algorithm is assumed to have reach the correct estimates if the
absolute change or relative change in the parameter estimates is less
than this value. The criterion is not used if the value is 0.</term>
<term termid="parameter_convergence_levenberg-marquardt_algorithm"
><linktext>Parameter Convergence (Levenberg-Marquardt Algorithm)</linktext
>If successive iterations fail to change any of the parameter values
by this proportion, the procedure stops. To override the default value,
select an alternate convergence value from the drop-down list or select
disable to disable this criterion.</term>
<term termid="parameter_estimates_2"><linktext>Parameter Estimates</linktext
>Prints estimates of the parameter (model term) effects.</term>
<term termid="parameter_significance"><linktext>Parameter significance</linktext
>A statistical test of whether the parameter is useful in modeling
the dependent variable.  Small values (less than .05 or .1) indicate
that the parameter contributes to the model.</term>
<term termid="parent_node"><linktext>Parent node</linktext>The parent
node one level above the child node in the tree.</term>
<term termid="parsimony"><linktext>Parsimony</linktext>The rule that,
other things equal, simple models are preferable to complicated ones.
The model you use should require the smallest possible number of parameters
that will adequately represent the pattern in the data.</term>
<term termid="part_1_and_2"><linktext>Part 1 and 2</linktext>Statistics
are computed for just the items assigned to this part of the total
scale.</term>
<term termid="part_and_partial_correlations"><linktext>Part and Partial
Correlations</linktext>Displays the zero-order, part, and partial
correlations. Values of a correlation coefficient range from -1 to
1. The sign of the coefficient indicates the direction of the relationship,
and its absolute value indicates the strength, with larger absolute
values indicating stronger relationships.</term>
<term termid="part_correlation"><linktext>Part Correlation</linktext
>The correlation between the dependent variable and an independent
variable when the linear effects of the other independent variables
in the model have been removed from the independent variable. It is
related to the change in R-squared when a variable is added to an
equation. Sometimes called the semipartial correlation.</term>
<term termid="gacf08"><linktext>Partial Autocorrelations</linktext
>Correlates the values of a series with the values lagged by 1 or
more cases, after the effects of correlations at the intervening lags
have been removed.</term>
<term termid="partial_chi-square"><linktext>Partial Chi-Square</linktext
>Partial associations is a test for individual model terms that is
based on the difference in the Pearson chi-square statistics between
a model without the term and a model with the term.</term>
<term termid="partial_correlation"><linktext>Partial Correlation</linktext
>The correlation that remains between two variables after removing
the correlation that is due to their mutual association with the other
variables. The correlation between the dependent variable and an independent
variable when the linear effects of the other independent variables
in the model have been removed from both.</term>
<term termid="partial_correlation_coefficient"><linktext>Partial Correlation
Coefficient</linktext>The correlation that remains between two variables
after removing the correlation that is due to their mutual association
with the other variables. The correlation between the dependent variable
and an independent variable when the linear effects of the other independent
variables in the model have been removed from the independent variable.</term>
<term termid="partial_gammas"><linktext>Partial Gammas</linktext>Conditional
gammas are displayed for 3-way to 10-way tables.</term>
<term termid="partial_residuals_coxreg"><linktext>Partial Residuals</linktext
>You can plot partial residuals against survival time to test the
proportional hazards assumption. One variable is saved for each covariate
in the final model. Parital residuals are available only for models
containing at least one covariate.</term>
<term termid="gspectra05"><linktext>Parzen Window (Spectral Plots)</linktext
>The weights are Wk = 1/p(2 + cos(2 pi fk)) (F[p/2] (2 pi fk))**2,
for k= 0, ... p, where p is the integer part of half the span and
F[p/2] is the Fejer kernel of order p/2.</term>
<term termid="paste_2"><linktext>Paste</linktext>Generates command
syntax from the dialog box selections and pastes the syntax into the
designated syntax window. You can customize the commands with additional
features not available from dialog boxes. The Dependent List must
contain at least one variable before you can click Paste.</term>
<term termid="paste_3"><linktext>Paste</linktext>Generates command
syntax from the dialog box selections and pastes the syntax into the
designated syntax window. You can customize the commands with additional
features not available from dialog boxes. The Test Variable(s) list
must contain at least one variable before you can click Paste.</term>
<term termid="paste_8"><linktext>Paste</linktext>Generates command
syntax from the dialog box selections and pastes the syntax into the
designated syntax window. You can customize the commands with additional
features not available from dialog boxes. The Variables list must
contain at least two variables and the Controlling for variable list
must contain at least one variable before you can click Paste.</term>
<term termid="paste_1"><linktext>Paste</linktext>Generates command
syntax from the dialog box selections and pastes the syntax into the
designated syntax window. You can customize the commands with additional
features not available from dialog boxes. The Variable list must contain
at least one variable before you can click Paste.</term>
<term termid="paste_4"><linktext>Paste</linktext>Generates command
syntax from the dialog box selections and pastes the syntax into the
designated syntax window. You can customize the commands with additional
features not available from dialog boxes. The Test Variable(s) list
must contain at least one variable and the Grouping Variable must
contain a variable with defined groups before you can click Paste.</term>
<term termid="paste_5"><linktext>Paste</linktext>Generates command
syntax from the dialog box selections and pastes the syntax into the
designated syntax window. You can customize the commands with additional
features not available from dialog boxes. The Paired Variables list
must contain at least one pair of variables before you can click Paste.</term>
<term termid="paste_6"><linktext>Paste</linktext>Generates command
syntax from the dialog box selections and pastes the syntax into the
designated syntax window. You can customize the commands with additional
features not available from dialog boxes. The Dependent list must
contain at least one variable and the Factor list must contain a variable
with defined ranges before you can click Paste.</term>
<term termid="paste"><linktext>Paste</linktext>Generates command syntax
from the dialog box selections and pastes the syntax into the designated
syntax window. You can customize the commands with additional features
not available from dialog boxes. You must select one dependent variable
and at least one independent variable before you can click Paste.</term>
<term termid="paste_7"><linktext>Paste</linktext>Generates command
syntax from the dialog box selections and pastes the syntax into the
designated syntax window. You can customize the commands with additional
features not available from dialog boxes. The Variables list must
contain at least two variables before you can click Paste.</term>
<term termid="paste_13"><linktext>Paste</linktext>Generates command
syntax from the dialog box selections and pastes the syntax into the
designated syntax window. You can customize the commands with additional
features not available from dialog boxes. The Test Variables list
must contain at least two variables before you can click Paste.</term>
<term termid="paste_9"><linktext>Paste</linktext>Generates command
syntax from the dialog box selections and pastes the syntax into the
designated syntax window. You can customize the commands with additional
features not available from dialog boxes. You must select at least
one Dependent variable and an Independent Variable that can be either
a variable in the active dataset or Time before you can click Paste.</term>
<term termid="paste_15"><linktext>Paste</linktext>Generates command
syntax from the dialog box selections and pastes the syntax into the
designated syntax window. You can customize the commands with additional
features not available from dialog boxes. The Independents list must
contain at least one variable and the Grouping Variable list must
contain a variable with a defined range before you can click Paste.</term>
<term termid="paste_10"><linktext>Paste</linktext>Generates command
syntax from the dialog box selections and pastes the syntax into the
designated syntax window. You can customize the commands with additional
features not available from dialog boxes. The Test Variable List must
contain at least one variable before you can click Paste.</term>
<term termid="paste_12"><linktext>Paste</linktext>Generates command
syntax from the dialog box selections and pastes the syntax into the
designated syntax window. You can customize the commands with additional
features not available from dialog boxes. The Test Pair(s) List must
contain at least one pair of variables before you can click Paste.</term>
<term termid="paste_14"><linktext>Paste</linktext>Generates command
syntax from the dialog box selections and pastes the syntax into the
designated syntax window. You can customize the commands with additional
features not available from dialog boxes. The Row(s) list and the
Column(s) list must each contain at least one variable before you
can click Paste.</term>
<term termid="pattern"><linktext>Pattern</linktext>The pattern of
predictor values for each requested plot. Each plot is generated using
the predictor values shown in the column corresponding to the plot.</term>
<term termid="pattern_difference_measure"><linktext>Pattern Difference
Measure</linktext>Difference measure for binary data that ranges from
0 to 1. Computed from a fourfold table as bc/(a+b+c+d)**2 where a
represents cases present on both items, b and c represent cases present
on one item but absent on the other, and d represents cases absent
on both items.</term>
<term termid="pattern_fill_charts"><linktext>Pattern Fill (Charts)</linktext
>Pattern fill. When you click Apply, the current fill color is applied
along with the pattern.</term>
<term termid="pctcombined"><linktext>pctCombined</linktext>The frequency
of cases in a cluster, expressed as a percentage of the total number
of cases assigned to clusters (including the outlier cluster).</term>
<term termid="pcttotal"><linktext>pctTotal</linktext>The number of
cases in a given category, expressed as a percentage of the total
number of cases, including those excluding from the analysis due to
missing values.</term>
<term termid="pdf.bernoulli"><linktext>PDF.BERNOULLI</linktext>PDF.BERNOULLI(quant,
prob).  Numeric. Returns the probability that a value from the Bernoulli
distribution, with the given probability parameter, will be equal
to quant.</term>
<term termid="pdf.beta"><linktext>PDF.BETA</linktext>PDF.BETA(quant,
shape1, shape2). Numeric. Returns the probability density of the beta
distribution, with the given shape parameters, at quant.</term>
<term termid="pdf.binom"><linktext>PDF.BINOM</linktext>PDF.BINOM(quant,
n, prob). Numeric. Returns the probability that the number of successes
in n trials, with probability prob of success in each, will be equal
to quant. When n is 1, this is the same as PDF.BERNOULLI.</term>
<term termid="pdf.bvnor"><linktext>PDF.BVNOR</linktext>PDF.BVNOR(quant1,
quant2, corr). Numeric. Returns the probability density of the standard
bivariate normal distribution, with the given correlation parameter,
at quant1, quant2.</term>
<term termid="pdf.cauchy"><linktext>PDF.CAUCHY</linktext>PDF.CAUCHY(quant,
loc, scale). Numeric. Returns the probability density of the Cauchy
distribution, with the given location and scale parameters, at quant.</term>
<term termid="pdf.chisq"><linktext>PDF.CHISQ</linktext>PDF.CHISQ(quant,
df). Numeric. Returns the probability density of the chi-square distribution,
with df degrees of freedom, at quant.</term>
<term termid="pdf.exp"><linktext>PDF.EXP</linktext>PDF.EXP(quant,
shape). Numeric. Returns the probability density of the exponential
distribution, with the given shape parameter, at quant.</term>
<term termid="pdf.f"><linktext>PDF.F</linktext>PDF.F(quant, df1, df2).
Numeric. Returns the probability density of the F distribution, with
degrees of freedom df1 and df2, at quant.</term>
<term termid="pdf.gamma"><linktext>PDF.GAMMA</linktext>PDF.GAMMA(quant,
shape, scale). Numeric. Returns the probability density of the gamma
distribution, with the given shape and scale parameters, at quant.</term>
<term termid="pdf.geom"><linktext>PDF.GEOM</linktext>PDF.GEOM(quant,
prob). Numeric. Returns the probability that the number of trials
to obtain a success, when the probability of success is given by prob,
will be equal to quant.</term>
<term termid="pdf.halfnrm"><linktext>PDF.HALFNRM</linktext>PDF.HALFNRM(quant,
mean, stddev). Numeric. Returns the probability density of the half
normal distribution, with specified mean and standard deviation, at
quant.</term>
<term termid="pdf.hyper"><linktext>PDF.HYPER</linktext>PDF.HYPER(quant,
total, sample, hits). Numeric. Returns the probability that the number
of objects with a specified characteristic, when sample objects are
randomly selected from a universe of size total in which hits have
the specified characteristic, will be equal to quant.</term>
<term termid="pdf.igauss"><linktext>PDF.IGAUSS</linktext>PDF.IGAUSS(quant,
loc, scale). Numeric. Returns the probability density of the inverse
Gaussian distribution, with the given location and scale parameters,
at quant.</term>
<term termid="pdf.laplace"><linktext>PDF.LAPLACE</linktext>PDF.LAPLACE(quant,
mean, scale). Numeric. Returns the probability density of the Laplace
distribution, with the specified mean and scale parameters, at quant.</term>
<term termid="pdf.lnormal"><linktext>PDF.LNORMAL</linktext>PDF.LNORMAL(quant,
a, b). Numeric. Returns the probability density of the log-normal
distribution, with the specified parameters, at quant.</term>
<term termid="pdf.logistic"><linktext>PDF.LOGISTIC</linktext>PDF.LOGISTIC(quant,
mean, scale). Numeric. Returns the probability density of the logistic
distribution, with the specified mean and scale parameters, at quant.</term>
<term termid="pdf.negbin"><linktext>PDF.NEGBIN</linktext>PDF.NEGBIN(quant,
thresh, prob). Numeric. Returns the probability that the number of
trials to obtain a success, when the threshold parameter is thresh
and the probability of success is given by prob, will be equal to
quant.</term>
<term termid="pdf.normal"><linktext>PDF.NORMAL</linktext>PDF.NORMAL(quant,
mean, stddev). Numeric. Returns the probability density of the normal
distribution, with specified mean and standard deviation, at quant.</term>
<term termid="pdf.pareto"><linktext>PDF.PARETO</linktext>PDF.PARETO(quant,
threshold, shape). Numeric. Returns the probability density of the
Pareto distribution, with the specified threshold and shape parameters,
at quant.</term>
<term termid="pdf.poisson"><linktext>PDF.POISSON</linktext>PDF.POISSON(quant,
mean). Numeric. Returns the probability that a value from the Poisson
distribution, with the specified mean or rate parameter, will be equal
to quant.</term>
<term termid="pdf.t"><linktext>PDF.T</linktext>PDF.T(quant, df). Numeric.
Returns the probability density of Student's t distribution, with
the specified degrees of freedom df, at quant.</term>
<term termid="pdf.uniform"><linktext>PDF.UNIFORM</linktext>PDF.UNIFORM(quant,
min, max). Numeric. Returns the probability density of the uniform
distribution, with the specified minimum and maximum, at quant.</term>
<term termid="pdf.weibull"><linktext>PDF.WEIBULL</linktext>PDF.WEIBULL(quant,
a, b). Numeric. Returns the probability density of the Weibull distribution,
with the specified parameters, at quant.</term>
<term termid="pearson"><linktext>Pearson</linktext>A measure of linear
association between two variables. Values of the correlation coefficient
range from -1 to 1. The sign of the coefficient indicates the direction
of the relationship, and its absolute value indicates the strength,
with larger absolute values indicating stronger relationships.</term>
<term termid="pearson_chi-square_npar_tests"><linktext>Pearson Chi-Square
(Npar Tests)</linktext>A measure of how well the selected model fits
the data. When the significance level is small, the expected counts
are too far from the observed counts, even adjusted for their size.</term>
<term termid="pearson_chi-square_goodnes_of_fit"><linktext>Pearson
Chi-square Goodness of Fit</linktext>Goodness of fit statistic based
on observed and expected frequencies. This statistic is sensitive
to small expected frequencies. If many expected frequencies are less
than 5 (for example if you have continuous covariates), this statistic
does not follow the chi-square distribution, and conclusions should
not be made based on the calculated significance level.</term>
<term termid="pearson_correlation"><linktext>Pearson correlation</linktext
>A measure of linear association between two variables. Values of
the correlation coefficient range  from -1 to 1. The sign of the coefficient
indicates the direction of the relationship, and its  absolute value
indicates the strength, with larger absolute values indicating stronger
relationships.</term>
<term termid="pearson_correlation_coefficient"><linktext>Pearson Correlation
Coefficient</linktext>A measure of linear association between two
variables. Values of the correlation coefficient range from -1 to
1. The sign of the coefficient indicates the direction of the relationship,
and its absolute value indicates the strength, with larger absolute
values indicating stronger relationships.</term>
<term
termid="pearson_correlation_similarity_measure_cluster_analysisdivdistance_and_proximity_measures"
><linktext>Pearson Correlation Similarity Measure (Cluster Analysis/Distance
and Proximity Measures)</linktext>A similarity measure for continuous
data that measures correlations between vectors of values.</term>
<term termid="pearson_residual"><linktext>Pearson Residual</linktext
>The residual for a linear regression is the actual value of the response
variable minus the value predicted by the regression equation.  Here,
for each covariate pattern, the Pearson residual is the difference
between the observed and predicted frequencies, rescaled by the expected
variation in the frequency.</term>
<term termid="pearsons_r"><linktext>Pearson's r</linktext>A measure
of linear association between two variables. Values of the correlation
coefficient range from -1 to 1. The sign of the coefficient indicates
the direction of the relationship, and its absolute value indicates
the strength, with larger absolute values indicating stronger relationships.</term>
<term termid="peer_id"><linktext>Peer ID</linktext>Identifies the
group, based on similarities between values of the input variables,
to which a case belongs.</term>
<term termid="peer_size"><linktext>Peer Size</linktext>The number
of cases in the peer group.</term>
<term termid="peer_size_percent"><linktext>Peer Size Percent</linktext
>The percent of valid cases in the peer group.</term>
<term termid="penalized_stress"><linktext>Penalized Stress</linktext
>The product of Kruskal's Stress I and a penalty term; the algorithm
attempts to find a solution that minimizes the penalized stress.</term>
<term termid="percent_2"><linktext>Percent</linktext>The percentage
of cases in the group.</term>
<term termid="percent_14"><linktext>Percent</linktext>The percentage
of cases having a particular value.</term>
<term termid="percent"><linktext>Percent</linktext>The percentage
of cases having a particular value. The raw percentage is calculated
by dividing the frequency of the value by the total number of cases
in the sample including missing values. The raw percentage may differ
from the valid percentage, which is computed with missing values excluded
from the percentage base.</term>
<term termid="percent_12"><linktext>Percent</linktext>The percentage
of cases in each category</term>
<term termid="percent_between"><linktext>Percent between</linktext
>This reports the percentage of ratios between the low and high values
of the interval.</term>
<term termid="percent_censored"><linktext>Percent Censored</linktext
>The percentage of cases withdrawn from observation before experiencing
the terminal event.</term>
<term termid="percent_correct_tree"><linktext>Percent correct</linktext
>The percentage of cases in which the predicted category is the same
as the observed category.</term>
<term termid="percent_correct"><linktext>Percent Correct</linktext
>Percentage of cases correctly classified, by observed category.</term>
<term termid="percent_of_cases"><linktext>Percent of Cases</linktext
>The number of responses to each item expressed as a percentage of
the number of cases.  Note that because there can be more responses
than cases, the total can be more than 100%.</term>
<term termid="percent_of_n_in"><linktext>Percent of N in</linktext
>Percentage of the number of cases for the specified grouping variable
within categories of other grouping variables. If you only have one
grouping variable, this value is identical to percentage of total
number of cases.</term>
<term termid="percent_of_sum_in"><linktext>Percent of sum in</linktext
>Percentage of the sum for the specified grouping variable within
categories of other grouping variables. If you only have one grouping
variable, this value is identical to percentage of total sum.</term>
<term termid="percent_of_total_n"><linktext>Percent of total N</linktext
>Percentage of the total number of cases in each category.</term>
<term termid="percent_of_total_sum"><linktext>Percent of total sum</linktext
>Percentage of the total sum in each category.</term>
<term termid="percent_of_variance"><linktext>Percent of Variance</linktext
>This is the proportion of the total variance in the data that is
explained.  It sums across variables within a dimension, but not across
dimensions.</term>
<term termid="percentage"><linktext>Percentage</linktext>The percentage
assigned to each response, with each covariate pattern summing to
100%.</term>
<term termid="greport04"><linktext>Percentage Above</linktext>The
percentage of cases falling above the specified value.</term>
<term termid="percentage_above_aggregate_function"><linktext>Percentage
Above (Aggregate Function)</linktext>The percentage of the cases in
the break group whose value for the source variable is greater than
the specified value.</term>
<term termid="gbar16"><linktext>Percentage Above (Graph Summary Function)</linktext
>The percentage of cases above the specified value.</term>
<term termid="greport05"><linktext>Percentage Below</linktext>The
percentage of cases falling below the specified value.</term>
<term termid="percentage_below_aggregate_function"><linktext>Percentage
Below (Aggregate Function)</linktext>The percentage of the cases in
the break group whose value for the source variable is less than the
specified value.</term>
<term termid="gbar17"><linktext>Percentage Below (Graph Summary Function)</linktext
>The percentage of cases below the specified value.</term>
<term termid="greport06"><linktext>Percentage Inside</linktext>The
percentage of cases of the summary variable falling inside the specified
interval. The specified high and low values are included in the range.</term>
<term termid="percentage_inside_aggregate_function"><linktext>Percentage
Inside (Aggregate Function)</linktext>The percentage of the cases
in the break group whose value for the source variable is greater
than or equal to the specified Low value, and less than or equal to
the specified High value.</term>
<term termid="percentage_outside_aggregate_function"><linktext>Percentage
Outside (Aggregate Function)</linktext>The percentage of the cases
in the break group whose value for the source variable is less than
the specified Low value or greater than the specified High value.</term>
<term termid="percentages"><linktext>Percentages</linktext>Displays
bar charts of the percentage of cases in each category.</term>
<term termid="percentile_2"><linktext>percentile</linktext>The data
value below which the specified percentage of values fall. The 25th,
 50th, and 75th  percentiles are also known as the quartiles, Q1,
Q2, and Q3.  The quartiles, plus the minimum and maximum, are known
collectively as the five number summary. The 50th percentile is equivalent
to the median.</term>
<term termid="percentile"><linktext>Percentile</linktext>Values below
which certain percentages of cases fall.</term>
<term termid="gbar18"><linktext>Percentile (Graph Summary Function)</linktext
>The data value below which the specified percentage of values fall.</term>
<term termid="percentiles"><linktext>Percentiles</linktext>Values
that divide cases according to values below which certain percentages
of cases fall. For example, the median is the 50% percentile, the
value below which 50% of the cases fall.</term>
<term termid="percentiles_1"><linktext>Percentiles</linktext>Values
below which a designated percentage of the cases fall. Displays the
following percentiles: 5, 10, 25, 50, 75, 90, 95. Also displays Tukey's
hinges (which define the quartiles of the distribution).</term>
<term termid="percentiles_list_box"><linktext>Percentiles List Box</linktext
>Displays the selected percentile values in sorted order.</term>
<term termid="percentiles_s"><linktext>Percentiles(s)</linktext>For
each specified percentile, displays the value below which that percentage
of the cases fall. For example the 50th percentile is the same as
the median, the value below which 50% of the cases fall.</term>
<term termid="performance_indices"><linktext>Performance Indices</linktext
>Measures of the performance of the process.  Most of these statistics
are based upon the estimated standard deviation of the process and/or
the specification limits.</term>
<term termid="gacf10"><linktext>Periodicity</linktext>Repetitive cyclical
variation of units within a period. For example, months have a periodicity
of 12 within a year, and days have a periodicity of 7 within a week.</term>
<term termid="gspectra01"><linktext>Periodogram</linktext>Unsmoothed
plot of spectral amplitude (plotted on a logarithmic scale) against
either frequency or period. Low-frequency variation characterizes
a smooth series. Variation spread evenly across all frequencies indicates
"white noise."</term>
<term termid="permutations"><linktext>Permutations</linktext>The original
crosstabulation reordered according to the scores of the rows and
columns for each dimension.</term>
<term termid="phase"><linktext>Phase</linktext>Phase of the analysis
for which results are reported</term>
<term termid="phase_spectrum"><linktext>Phase spectrum</linktext>A
measure of the extent to which each frequency component of one series
leads or lags the other.</term>
<term termid="phi_3"><linktext>Phi</linktext>A chi-square based measure
of association that involves dividing the chi-square statistic by
the sample size and taking the square root of the result. For 2x2
tables only, the phi coefficient is equal to the Pearson correlation
coefficient, so phi ranges from -1 to +1. For tables in which one
dimension is greater than 2, phi need not be bounded by 0 and 1.</term>
<term termid="phi"><linktext>Phi</linktext>A chi-square based measure
of association. For 2x2 tables only, the phi coefficient is equal
to the Pearson correlation coefficient, so phi ranges from -1 to +1.
For tables in which one dimension is greater than 2, the value of
Phi can exceed an absolute value of 1. For general purposes, if either
variable has more than two categories, the significance value is more
important that the actual value of the statistic.</term>
<term termid="phi_4-point_correlation_distance_and_proximity_measures"
><linktext>Phi 4-Point Correlation (Distance and Proximity Measures)</linktext
>A similarity measure for binary data that is the Pearson product-moment
correlation coefficient for the binary case, with a range of 0 to
1. Computed from a fourfold table as (ad-bc)/SQRT((a+b)(a+c)(b+d)(c+d))
where a represents cases present on both items, b and c represent
cases present on one item but absent on the other, and d represents
cases absent on both items.</term>
<term termid="phi_and_cramers_v"><linktext>Phi and Cramer's V</linktext
>Phi is a chi-square-based measure of association that involves dividing
the chi-square statistic by the sample size and taking the square
root of the result. Cramer's V is a measure of association based on
chi-square.</term>
<term termid="phi-square_distance_measure"><linktext>Phi-Square Distance
Measure</linktext>Dissimilarity measure for frequency count data that
attempts to take sample size into account to decrease the effect of
actual observed frequencies on the value of the measure. This is the
chi-square measure normalized by the square root of the combined frequency.
Therefore, its value does not depend on the total frequencies of the
two items whose distance is computed.</term>
<term termid="pie_chart"><linktext>Pie Chart</linktext>A chart that
divides a circle into wedge-shaped slices. The size of each slice
indicates the value of that category relative to the whole.</term>
<term termid="pillais_trace"><linktext>Pillai's Trace</linktext>A
multivariate test of significance. It can be transformed to approximate
an F statistic.</term>
<term termid="plan_file"><linktext>Plan File</linktext>A plan file
contains complex sample specifications.</term>
<term termid="plot_models"><linktext>Plot Models</linktext>Plots the
values of the dependent variable and each selected model against the
independent variable. A separate chart is produced for each dependent
variable.</term>
<term termid="plots"><linktext>Plots</linktext>Requests optional plots,
including box plots, histograms, normality plots and tests, and spread-versus-level
plots with the Levene statistic. You must choose to display either
Plots or Both before you can request optional plots.</term>
<term termid="plots_2"><linktext>Plots</linktext>Requests optional
plots, including scatterplots, histograms, normal probability plots
and outlier plots.</term>
<term termid="plots_checkbox"><linktext>Plots Checkbox</linktext>Displays
plots. To suppress plots, deselect this item.</term>
<term termid="plots_pushbutton"><linktext>Plots Pushbutton</linktext
>Allows you to display a dendrogram or control the format of icicle
plots.</term>
<term termid="plots_radio_button"><linktext>Plots Radio Button</linktext
>Displays plots only, and suppresses all statistics. Box plots and
stem-and-leaf plots for each variable are displayed by default.</term>
<term termid="pmml_data_type"><linktext>PMML Data Type</linktext>The
possible values are string and double.</term>
<term termid="pmml_display_name"><linktext>PMML Display Name</linktext
>Variable label if provided.</term>
<term termid="pmml_measurement_level"><linktext>PMML Measurement Level</linktext
>Referred to as the optype in PMML. The possible values are categorical,
ordinal, and continuous.</term>
<term termid="point_probability"><linktext>Point Probability</linktext
>The probability of the observed statistic under the null hypothesis.</term>
<term termid="poisson"><linktext>Poisson</linktext>The observed cumulative
distribution function is compared to the Poisson distribution with
the observed mean as a parameter.</term>
<term termid="poisson_2"><linktext>Poisson</linktext>A discrete distribution
useful for describing the outcomes of a random variable. The outcomes
are any non-negative integers. The mean and the variance of a Poisson
distribution are equal. The Poisson model assumes that the observations
are independent Poisson random variables with different means. Choose
this distribution if the total sample size is not fixed and the cells
are independent.</term>
<term termid="poisson_distribution"><linktext>Poisson Distribution</linktext
>A skewed distribution appropriate and useful for phenomena that have
a very small probability of occurring on any particular trial, but
for which an extremely large number of trials are available (while
the product of the two numbers is moderate). The mean and the variance
of a Poisson distribution are equal.</term>
<term termid="poisson_parameters"><linktext>Poisson Parameters</linktext
>The parameters of the Poisson distribution against which the observed
distribution is being tested.</term>
<term termid="polychotomous"><linktext>polychotomous</linktext>A term
for a variable that has a discrete number of values; a categorical
variable.</term>
<term termid="polynomial_and_degree"><linktext>Polynomial and Degree</linktext
>Allows you to partition the between-groups sum of squares into polynomial
trend components. After selecting Polynomial, select the highest degree
of the polynomial you wish to model. You can choose a degree up to
5.</term>
<term termid="polynomial_contrast"><linktext>Polynomial Contrast</linktext
>Compares the linear effect, quadratic effect, cubic effect, and so
on. The first degree of freedom contains the linear effect across
all categories; the second degree of freedom, the quadratic effect;
and so on. These contrasts are often used to estimate polynomial trends.</term>
<term termid="polynomial_contrast_metric"><linktext>Polynomial Contrast
Metric</linktext>The metric of a polynomial contrast indicates the
spacing of the levels. The default (1,2,3...) indicates equal spacing.
The relative differences between the terms of the metric indicate
the spacing of the levels. For example, for dosages of 20mg, 30mg,
50mg, and 80mg, the metric could be specified as (1,2,4,7), whereas
for 20mg, 30mg, 40mg, and 50mg, the metric would be (1,2,3,4).</term>
<term termid="polynomial_contrasts"><linktext>Polynomial Contrasts</linktext
>In a polynomial contrast, the first degree of freedom contains the
linear effect across the categories of the predictor variable or factor;
the second contains the quadratic effect, and so on. Categories are
assumed to be evenly spaced.</term>
<term termid="pooled_over_strata"><linktext>Pooled Over Strata</linktext
>Compares all factor levels in a single test to test the equality
of survival curves.</term>
<term termid="pooled_variance_estimate"><linktext>Pooled Variance
Estimate</linktext>The test of the hypothesis that two population
means are equal computed under the assumption that the two groups
come from populations with the same variance.</term>
<term termid="population_size"><linktext>Population Size</linktext
>The number of units in the population at this stage.</term>
<term termid="population_subject_size"><linktext>Population Subject
Size</linktext>The number of sujects in the entire population.</term>
<term termid="portmanteau_test"><linktext>Portmanteau Test</linktext
>A test statistic that tests the null hypothesis that a set of autocorrelations
are associated with a random series. This is referred to as a portmanteau
lack-of-fit test.</term>
<term termid="position_3"><linktext>Position</linktext>Position of
the variable in the data file (first variable is 1, second variable
is 2, etc.)</term>
<term termid="positive"><linktext>Positive</linktext>Cases that are
positive; that is, fall into the category of interest.</term>
<term termid="positive_differences"><linktext>Positive Differences</linktext
>Lists the number of positive differences. The number of times variable
1 is greater than variable 2.</term>
<term termid="positive_extreme_differences"><linktext>Positive Extreme
Differences</linktext>The largest positive difference between two
cumulative distribution functions.</term>
<term termid="positive_if_greater_than_cutoff"><linktext>Positive
if greater than cutoff</linktext>The value of the Test Variable is
considered "Positive" if it is greater than BUT NOT equal to the cutoff.</term>
<term termid="positive_if_greater_than_or_equal_to_cutoff"><linktext
>Positive if greater than or equal to cutoff</linktext>The value of
the Test Variable is considered "Positive" if it is greater than OR
equal to the cutoff.</term>
<term termid="positive_if_less_than_cutoff"><linktext>Positive if
less than cutoff</linktext>The value of the Test Variable is considered
"Positive" if it is less than BUT NOT equal to the cutoff.</term>
<term termid="positive_if_less_than_or_equal_to_cutoff"><linktext
>Positive if less than or equal to cutoff</linktext>The value of the
Test Variable is considered "Positive" if it is less than OR equal
to the cutoff.</term>
<term termid="positive_ranks"><linktext>Positive Ranks</linktext>List
the number of positive ranks. The number of times the rank of variable
1 is greater than the rank of variable 2.</term>
<term termid="post_hoc"><linktext>Post Hoc</linktext>Allows you to
choose post hoc tests for pairwise multiple comparisons between means
or range tests.</term>
<term termid="power_4"><linktext>Power</linktext>The power is used
in conjunction with the source variable to compute weights.  The weights
are equal to 1/(weight variable)**power.</term>
<term termid="power_estimation"><linktext>Power Estimation</linktext
>Plots the natural logs of the interquartile ranges against the natural
logs of the medians for each group, as well as an estimate of the
power transformation for achieving equal variances in the cells. This
method can aid in the determination of an appropriate transformation
for your data.</term>
<term termid="power_model"><linktext>Power Model</linktext>Model whose
equation is Y = b0 * (t**b1) or ln(Y) = ln(b0) + (b1 * ln(t)).</term>
<term termid="power_range_wls"><linktext>Power Range</linktext>This
is used in conjunction with the weight variable to compute weights.
Several regression equations will be fit, one for each value in the
power range. The values entered in the Power range test box and the
through text box must be between -6.5 and 7.5, inclusive. The power
values range from the low to high value, in increments determined
by the value specified. The total number of values in the power range
is limited to 150.</term>
<term termid="pp"><linktext>PP</linktext>Performance of the process,
which is the ratio of the difference between the upper specification
limit and the lower specification limit to six times the estimated
standard deviation of the process.</term>
<term termid="ppk"><linktext>PpK</linktext>Performance of process
related to both dispersion and centeredness.  It is the minimum of
PpU and PpL.</term>
<term termid="ppl"><linktext>PpL</linktext>The distance between the
process mean and the lower specification limit scaled by process standard
deviation.</term>
<term termid="ppm"><linktext>PpM</linktext>An index relating process
variance and the difference between the process mean and the target
value.  A target value must be specified to obtain this statistic.</term>
<term termid="ppu"><linktext>PpU</linktext>The distance between the
process mean and the upper specification limit scaled by process standard
deviation.</term>
<term termid="pr"><linktext>PR</linktext>The reciprocal of the process
performance.</term>
<term termid="practical_significance"><linktext>practical significance</linktext
>A statistical test will answer the question, "Is there a difference
between two groups?" but not the follow-up "Is that difference large
enough for me to care?"  It is up to you to determine whether test
results are useful to your situation.</term>
<term termid="prais-winsten_method"><linktext>Prais-Winsten</linktext
>A generalized least-squares method for estimating a regression equation
whose errors follow a first-order autoregressive process. It cannot
be used when a series contains embedded missing values. Generally,
the Prais-Winsten method is preferable to the Cochrane-Orcutt method.</term>
<term termid="prd"><linktext>PRD</linktext>The price-related differential,
also known as the index of regressivity, is the result of dividing
the mean by the weighted mean.</term>
<term termid="predict_from_estimation_period_through_last_case_1"
><linktext>Predict from estimation period through last case</linktext
>Predicts values for all cases from the estimation period through
the end of the file but does not create new cases. If you are analyzing
a range of cases that start after the beginning of the file, cases
prior to that range are not predicted. If no estimation period has
been defined, all cases are used to predict values.</term>
<term termid="predict_from_estimation_period_through_last_case"><linktext
>Predict from estimation period through last case</linktext>Predicts
values for all cases in the file, based on the cases in the estimation
period. The estimation period is displayed at the bottom of the dialog
box. Available only if Predicted values is selected.</term>
<term termid="predict_through"><linktext>Predict through</linktext
>After selecting this option, enter the observation number through
which you would like to predict values. This can be used to forecast
values beyond the last case in the time series. Available only if
you selected Time as the independent variable and Predicted values
is selected.</term>
<term termid="predict_through_1"><linktext>Predict through</linktext
>Predicts values through the specified date, time, or observation
number, based on the cases in the estimation period. This can be used
to forecast values beyond the last case in the time series. The text
boxes that are available for specifying the end of the prediction
period depend on the currently defined date variables.</term>
<term termid="predicted_category_tree"><linktext>Predicted category</linktext
>For each case, the predicted category is the category of the dependent
variable predicted by the model.</term>
<term termid="predicted_frequency"><linktext>Predicted Frequency</linktext
>The number or percentage of cases that are predicted to match a particular
covariate pattern.</term>
<term termid="predicted_group"><linktext>Predicted group</linktext
>The predicted group membership</term>
<term termid="predicted_group_membership"><linktext>Predicted Group
Membership</linktext>The group with the largest posterior probability,
based on discriminant scores. The group the model predicts the case
belongs to.</term>
<term termid="predicted_mean_1"><linktext>Predicted mean</linktext
>Predicted mean value of the dependent variable.   Frequency and influence
weights, if any, are involved in its calculation.  (In contrast, the
column labeled Mean is calculated using only the frequency weights,
if any.)</term>
<term termid="predicted_mean"><linktext>Predicted Mean</linktext>Displays
mean for each factor level expressed as deviations from the grand
mean. These allow you to assess the magnitude of the effect of each
category of a factor.</term>
<term termid="predicted_probabilities_1"><linktext>Predicted probabilities</linktext
>Probability that the predicted value is correct.</term>
<term termid="predicted_value_general_loglinear"><linktext>Predicted
Value (General Loglinear)</linktext>Also known as expected cell count,
this is the number of observations in that cell estimated from the
sample under a specified model.</term>
<term termid="predicted_values"><linktext>Predicted Values</linktext
>The value the model predicts for the dependent variable.</term>
<term termid="predicted_values_2"><linktext>Predicted values</linktext
>Values predicted by the model.</term>
<term termid="predicted_values_nonlin"><linktext>Predicted Values</linktext
>Saves predicted values with the variable name pred_.</term>
<term termid="prediction_interval_for_individual"><linktext>Prediction
Interval for Individual</linktext>Lower and upper bounds (two variables)
for the prediction interval of the dependent variable for a single
case.</term>
<term termid="prediction_interval_for_mean"><linktext>Prediction Interval
for Mean</linktext>Lower and upper bounds (two variables) for the
prediction interval of the mean of the dependent variable.</term>
<term termid="prediction_intervals"><linktext>Prediction Intervals</linktext
>Saves the upper and lower bounds (two variables) for the prediction
interval of each case for each selected model.</term>
<term termid="predictor_added"><linktext>Predictor Added</linktext
>The predictor added to each subsequent subset is the one that increases
the average log-likelihood the most.</term>
<term termid="predictor_importance"><linktext>Predictor Importance</linktext
>Measures how strongly a variable acts as a primary or surrogate predictor.
 Equals the weighted sum across all nodes in the tree of the improvements
that the variable has when it is used as a primary or surrogate predictor.</term>
<term termid="predictor_variables"><linktext>Predictor Variables</linktext
>Independent variables. Variables that are used to predict the values
of another variable in a model.</term>
<term termid="previous-next-block"><linktext>Previous-Next-Block</linktext
>Allows you to specify different entry methods for different subsets
of variables. To add a second block of variables to the regression
model, click Next. To move back and forth between blocks of independent
variables, use Previous and Next.</term>
<term termid="prev_next_block_logreg"><linktext>Previous-Next-Block
(Cox Regression/Logistic Regression)</linktext>Allows you to specify
different entry methods for different blocks of covariates. To add
a second block of covariates to the regression model, click Next.
To move back and forth between blocks of covariates, use Previous
and Next.</term>
<term termid="previous-next-contrast"><linktext>Previous-Next-Contrast</linktext
>Allows you to specify up to 10 sets of contrasts. Use Next and Previous
to move between sets of contrasts.</term>
<term termid="previous-next-layer"><linktext>Previous-Next-Layer</linktext
>Selected variables divide the crosstabulation into sub-groups. For
example, a crosstabulation of region and profitability could be further
sub-divided by product line. Selected variables should be categorical
(variables with a few distinct values).</term>
<term termid="previous-next-scatter"><linktext>Previous-Next-Scatter</linktext
>Allows you to obtain up to nine different scatterplots. Select one
X variable and one Y variable. To request additional plots, click
Next and repeat this process. Previous displays the variables for
the scatterplot in the previous layer.</term>
<term termid="primary_predictor"><linktext>Primary predictor</linktext
>The independent variable (predictor)  used to split the parent node.</term>
<term termid="principal"><linktext>Principal</linktext>The distances
between row points and column points are approximations of the selected
distance measure. Use this method if you want to examine differences
between categories of either or both variables instead of differences
between the two variables.</term>
<term termid="principal_axis_factoring"><linktext>Principal Axis Factoring</linktext
>A method of extracting factors from the original correlation matrix,
with squared multiple correlation coefficients placed in the diagonal
as initial estimates of the communalities. These factor loadings are
used to estimate new communalities that replace the old communality
estimates in the diagonal. Iterations continue until the changes in
the communalities from one iteration to the next satisfy the convergence
criterion for extraction.</term>
<term termid="principal_components_analysis"><linktext>Principal Components
Analysis</linktext>A factor extraction method used to form uncorrelated
linear combinations of the observed variables. The first component
has maximum variance. Successive components explain progressively
smaller portions of the variance and are all uncorrelated with each
other. Principal components analysis is used to obtain the initial
factor solution. It can be used when a correlation matrix is singular.</term>
<term termid="print_all_layers"><linktext>Print All Layers</linktext
>Prints each layer as a separate table.</term>
<term termid="prior_probability_1"><linktext>Prior probability</linktext
>Estimates of the overall relative frequency for each target category
of the dependent variable prior to knowing anything about the values
of the independent (predictor) variables. Using prior probabilities
helps correct any tree growth caused by data in the sample that is
not representative of the entire population. User-specified values
are normalized to proportions that sum to 1.</term>
<term termid="prior_probability"><linktext>Prior Probability</linktext
>An estimate of the likelihood that a case belongs to a particular
group when no other information about it is available. Unless you
specify otherwise, it is assumed that a case is equally likely to
be a member of any group. That is, all prior probabilities are equal.</term>
<term termid="predicted_probabilities"><linktext>Probabilities</linktext
>For each case, saves the predicted probability of occurrence of the
event. A table in the output displays name and contents of any new
variables.</term>
<term termid="probabilities_of_group_membership"><linktext>Probabilities
of Group Membership</linktext>Creates as many variables as there are
groups. The first variable contains the posterior probability of membership
in the first group, the second new variable contains the probability
of membership in the second group, and so on.</term>
<term termid="probability"><linktext>Probability</linktext>The model-estimated
probability of response used to calculate the number of expected responses.</term>
<term termid="probability_density"><linktext>Probability Density</linktext
>An estimate of the probability of experiencing the terminal event
during the interval.</term>
<term termid="probability_for_stepwise"><linktext>Probability for
Stepwise</linktext>A variable is entered into the model if the probability
of its score statistic is less than the Entry value and is removed
if the probability is greater than the Removal value. To override
the default settings, enter positive values for Entry and Removal.
Entry must be less than Removal.</term>
<term termid="probability_tables"><linktext>Probability Tables</linktext
>Statistical tables that contain critical values which can be used
to assess the significance level of observed results. Frequently used
tables are often found in basic statistics books.</term>
<term termid="probit_model"><linktext>Probit Model</linktext>Applies
the probit transformation (the inverse of the cumulative standard
normal distribution function) to the response proportions.</term>
<term termid="probit_prob"><linktext>PROBIT(prob)</linktext>PROBIT(prob).
Numeric. Returns the value in a standard normal distribution having
a cumulative probability equal to prob. The argument prob is a probability
greater than 0 and less than 1.</term>
<term termid="gcontrol07"><linktext>Process Line (Control Charts)</linktext
>Plots the quality characteristic under consideration. The process
is either measured over time, or over a number of samples. If a process
is in control, the process line should be random, centered around
the center line, and the points should fall within the control limits.</term>
<term termid="processed"><linktext>Processed</linktext>The number
of cases evaluated. The number of cases used is the number processed
minus the number excluded.</term>
<term termid="produce_all_partial_plots"><linktext>Produce All Partial
Plots</linktext>Produces a scatterplot (for each independent variable)
of residuals of the dependent variable and an independent variable
when both variables are regressed separately on the rest of the independent
variables. At least two independent variables must be in the equation
to produce a partial plot.</term>
<term termid="profiles"><linktext>Profiles</linktext>Row and column
profiles are the row and column proportions for each cell, based on
the marginal totals.</term>
<term termid="profit_tree"><linktext>Profit (Tree)</linktext>The average
profit value for the node.  For each dependent variable category,
multiply the proportion of cases in the category by the profit value
assigned to the category. Then sum across the categories to get the
average profit value for the node.</term>
<term termid="projected_centroids"><linktext>Projected Centroids</linktext
>The projected centroids are the category centroid values mapped to
a straight line.</term>
<term termid="promax_rotation"><linktext>Promax Rotation</linktext
>An oblique rotation, which allows factors to be correlated. This
rotation can be calculated more quickly than a direct oblimin rotation,
so it is useful for large datasets.</term>
<term termid="properties_dimension"><linktext>Properties Dimension</linktext
>The dimension (row, column, or layer) of the table into which are
placed the properties assigned to the levels of the factors.</term>
<term termid="proportion_estimates_rank_cases"><linktext>Proportion
Estimates (Rank Cases)</linktext>Estimates of the cumulative proportion
of the distribution corresponding to a particular rank.</term>
<term termid="proportion_explained_prefscal"><linktext>Proportion
Explained (prefscal)</linktext>The proportion of the total inertia
explained by the dimension.</term>
<term termid="proportion_of_inertia"><linktext>Proportion of inertia</linktext
>The proportion of the total inertia for each dimension.</term>
<term termid="proportion_of_units_sampled"><linktext>Proportion of
Units Sampled</linktext>The user-specified proportion of sampling
units drawn at this stage.</term>
<term termid="proportion_surviving"><linktext>Proportion Surviving</linktext
>One minus the proportion terminating.</term>
<term termid="proportion_terminating"><linktext>Proportion Terminating</linktext
>The ratio of terminal events to the number exposed to risk.</term>
<term termid="proportional_reduction_in_error"><linktext>Proportional
Reduction in Error</linktext>A criterion used to measure the association
between two variables. It is the proportional change in error when
predicting the values of the dependent variable based only on its
distribution, and when predicting it based on knowledge of the independent
variable.</term>
<term termid="proximites_in_single_column"><linktext>The proximites
are stacked in a single column</linktext>The proximity matrices are
collapsed into a single column, or variable.  Three additional variables,
identifying the row, column, and source for each cell, are necessary.
This leads to the Proximities in One Column dialog box.</term>
<term termid="proximities_in_a_matrix_across_columns"><linktext>The
proximities are in a matrix across columns</linktext>The proximity
matrix is spread across a number of columns equal to the number of
objects. This leads to the Proximities in Matrices across Columns
dialog box.</term>
<term termid="proximities_in_a_single_column"><linktext>The proximities
are in a single column</linktext>The proximity matrix is collapsed
into a single column, or variable. Two additional variables, identifying
the row and column for each cell, are necessary. This leads to the
Proximities in One Column dialog box.</term>
<term termid="proximities_in_columns_one_source_per_column"><linktext
>The proximities are in columns, one source per column</linktext>The
proximity matrices are collapsed into multiple columns, or variables.
Two additional variables, identifying the row and column for each
cell, are necessary. This leads to the Proximities in Columns dialog
box.</term>
<term termid="proximities_in_stacked_matrices_across_columns"><linktext
>The proximities are in stacked matrices across columns</linktext
>The proximity matrices are spread across a number of columns equal
to the number of objects and are stacked above one another across
a number of rows equal to the number of objects times the number of
sources. This leads to the Proximities in Matrices across Columns
dialog box.</term>
<term termid="pseudo_-2_log-likelihood"><linktext>Pseudo -2 Log-Likelihood</linktext
>In complex sampling situations, the pseudo-likelihood is a sample
estimate of the population log-likelihood, and parameter estimates
are derived by maximizing the pseudo-likelihood.  The pseudo -2 log-likelihood
is a measure of how well the model fits the data. The smaller the
value, the better the fit.</term>
<term termid="pseudo_r-square"><linktext>Pseudo R-Square</linktext
>An approximation to the R-square goodness-of-fit measure of a linear
model.  It ranges in value from 0 to 1. Small values indicate that
the model does not fit the data well.</term>
<term termid="pseudo-bic"><linktext>Pseudo-BIC</linktext>A "best"
subset criterion based on the average log-likelihood of the training
data.  Smaller values indicate better subsets.</term>
<term termid="ptbl_adjusted_ssq_arima"><linktext>PTBL adjusted SSQ
(ARIMA)</linktext>Adjusted sum of squares of the model residuals.</term>
<term termid="ptbl_ar_arima"><linktext>PTBL AR (ARIMA)</linktext>The
number of autoregressive parameters in the model. Each parameter measures
the independent effect of values with a specified lag. Thus, an autoregressive
order of 2 means that a series value is affected by the preceding
two values (independently of one another).</term>
<term termid="ptbl_autocorrelation"><linktext>PTBL autocorrelation</linktext
>Correlations of a series with lagged values of itself; also known
as serial correlation.</term>
<term termid="ptbl_acf_box-ljung_value"><linktext>PTBL Box-Ljung Value
(ACF)</linktext>Value of the Box-Ljung statistic.</term>
<term termid="ptbl_cases_skipped_arima"><linktext>PTBL cases skipped
(ARIMA)</linktext>Number of cases skipped due to missing values at
the beginning or end of the series.</term>
<term termid="ptbl_cases_skipped_begin_arima"><linktext>PTBL cases
skipped begin (ARIMA)</linktext>Number of cases skipped due to missing
values at the beginning of the series.</term>
<term termid="ptbl_cases_skipped_end_arima"><linktext>PTBL cases skipped
end (ARIMA)</linktext>Number of cases skipped due to missing values
at the end of the series.</term>
<term termid="ptbl_cases_skipped_in_series_arima"><linktext>PTBL cases
skipped in series (ARIMA)</linktext>Number of cases skipped due to
missing values imbedded within the series.</term>
<term termid="ptbl_command_names_tdisplay"><linktext>PTBL command
names  (TDISPLAY)</linktext>Name of procedure associated with model.</term>
<term termid="ptbl_computable_first_lags"><linktext>PTBL computable
first lags</linktext>Number of 1st order lags that can be computed,
based on the number of valid cases.</term>
<term termid="ptbl_computable_first_lags_differencing"><linktext>PTBL
computable first lags differencing</linktext>Number of 1st order lags
that can be computed taking into account number of valid cases and
degree of differencing.</term>
<term termid="ptbl_computable_zero_corr_differencing"><linktext>PTBL
computable zero corr differencing</linktext>Number of zero order correlations
that can be computed taking into account number of valid cases and
degree of differencing.</term>
<term termid="ptbl_computing_method_season"><linktext>PTBL computing
method  (SEASON)</linktext>Method in effect when computing moving
averages.</term>
<term termid="ptbl_confidence_interval_tshow"><linktext>PTBL confidence
interval (TSHOW)</linktext>Shows the setting of CIN for the TSET command.
This is the setting for confidence intervals given as a percentage.</term>
<term termid="ptbl_constant_arima"><linktext>PTBL constant (ARIMA)</linktext
>In MA models, the constant is the mean level of the series. In AR(1)
models, the constant is a trend parameter. When a series has been
differenced, these interpretations apply to the differences.</term>
<term termid="ptbl_constant_tshow"><linktext>PTBL constant (TSHOW)</linktext
>Displays the setting for whether a constant term is used in analyses.</term>
<term termid="ptbl_convergence_tshow"><linktext>PTBL convergence (TSHOW)</linktext
>Shows the setting of CNVERGE for the TSET command. If successive
iterations fail to change the sum of squares by this proportion, the
procedure stops.</term>
<term termid="ptbl_cross_correlations"><linktext>PTBL cross correlations</linktext
>The correlations between the two series at the specified lags and
leads.</term>
<term termid="ptbl_date_id_vars_tshow"><linktext>PTBL date id vars
(TSHOW)</linktext>Variables generated by the DATE command. Date identification
variables are used to label plots and other output, establish periodicity,
and distinguish between historical, validation, and forecasting periods.</term>
<term termid="ptbl_dependent_arima"><linktext>PTBL dependent (ARIMA)</linktext
>Variable label (or variable name) of dependent variable.</term>
<term termid="ptbl_df_residuals_arima"><linktext>PTBL df residuals
(ARIMA)</linktext>Degrees of freedom associated with the model residuals.</term>
<term termid="ptbl_display_and_plot"><linktext>PTBL Display and Plot</linktext
>Specifies whether correlations are displayed at all lags or only
at periodic lags (applies only to time series with a defined periodicity).</term>
<term termid="ptbl_forecasted_cases_arima"><linktext>PTBL forecasted
cases (ARIMA)</linktext>Number of cases forecast beyond the range
of the current data.</term>
<term termid="ptbl_from_ccf"><linktext>PTBL From (CCF)</linktext>Maximum
negative lag (second series leading).</term>
<term termid="ptbl_from_tshow"><linktext>PTBL from (TSHOW)</linktext
>Designates the beginning of a range of cases.</term>
<term termid="ptbl_id_tshow"><linktext>PTBL ID (TSHOW)</linktext>Shows
the setting of ID for the TSET command. This specifies a variable
whose values are used to label observations in plots. If ID is not
specified, the DATE_ variable is used to label observations.</term>
<term termid="ptbl_independent_arima"><linktext>PTBL independent (ARIMA)</linktext
>Variable labels (or variable names) of any independent variables.</term>
<term termid="ptbl_level_exsmooth"><linktext>PTBL level (EXSMOOTH)</linktext
>Series level. Applies to models without damped trend.</term>
<term termid="ptbl_level_phi_exsmooth"><linktext>PTBL level phi (EXSMOOTH)</linktext
>Initial series level for the specific Phi value in the presence of
a damped trend component.</term>
<term termid="ptbl_ma_arima"><linktext>PTBL MA (ARIMA)</linktext>The
number of moving average orders in the model. A moving average is
the average of a series value with surrounding series values. Moving
averages are used to smooth a time series--that is, reduce noise or
fluctuation in the series.</term>
<term termid="ptbl_marquardt_termination_arima"><linktext>PTBL marquardt
termination (ARIMA)</linktext>Iteration terminates when the maximum
Marquardt constant is larger than this value.</term>
<term termid="ptbl_max_auto_lags_tshow"><linktext>PTBL max auto lags
(TSHOW)</linktext>Shows the setting of MXAUTO for the TSET command.
This is the maximum number of lags used in autocorrelation and partial
autocorrelation plots.</term>
<term termid="ptbl_max_cross_lags_tshow"><linktext>PTBL max cross
lags (TSHOW)</linktext>Shows the setting of MXCROSS for the TSET command.
This is the maximum number of lags used in cross-correlation plots.</term>
<term termid="ptbl_max_iterations_arima"><linktext>PTBL max iterations
(ARIMA)</linktext>Maximum number of iterations.</term>
<term termid="ptbl_max_new_cases_tshow"><linktext>PTBL max new cases
(TSHOW)</linktext>Shows the setting of MXPREDICT for the TSET command.
This is the maximum number of new cases that can be added to the active
dataset per procedure when the PREDICT command is used.</term>
<term termid="ptbl_max_new_vars_tshow"><linktext>PTBL max new vars
(TSHOW)</linktext>Shows the setting of MXNEWVARS for the TSET command.
This is the maximum number of new variables that can be generated
by a procedure.</term>
<term termid="ptbl_model_exsmooth"><linktext>PTBL model (EXSMOOTH)</linktext
>Designates a custom exponential smoothing model.</term>
<term termid="ptbl_model_input_file_name"><linktext>PTBL model input
file name</linktext>Path to model file.</term>
<term termid="ptbl_model_label_tdisplay"><linktext>PTBL model label
(TDISPLAY)</linktext>User-specified model label.</term>
<term termid="ptbl_model_missing_handling"><linktext>PTBL model missing
handling</linktext>Specifies the treatment of missing values, encountered
during the scoring process, for the predictor variables defined in
the model. SYSMIS means return the system-missing value. SUBSTITUTE
means use value substitution (if supported).</term>
<term termid="ptbl_model_rank_exsmooth"><linktext>PTBL model rank
(EXSMOOTH)</linktext>Ranking given to model based on sum of squared
errors. The model with the lowest sum of squared errors is given the
rank of 1.</term>
<term termid="ptbl_model_std._error_arima"><linktext>PTBL model std.
Error (ARIMA)</linktext>The standard error of the model estimates.</term>
<term termid="ptbl_model_type_season"><linktext>PTBL model type (SEASON)</linktext
>The type of model, either multiplicative or additive. For additive
seasonality, seasonal adjustments are added to the seasonally adjusted
series to obtain the original series. For multiplicative seasonality,
the seasonal component is a factor by which the seasonally adjusted
series is multiplied to yield the original series.</term>
<term termid="ptbl_model_validation_handling"><linktext>PTBL model
validation handling</linktext>Specifies whether data validation checking
is enabled (default is enabled). Validation checking includes checking
that data is of the correct type as well as checking that the data
values are in the set of allowed values defined in the model.</term>
<term termid="ptbl_model_variable_label"><linktext>PTBL model variable
label</linktext>Variable label from model file.</term>
<term termid="ptbl_model_variable_measurement"><linktext>PTBL model
variable measurement</linktext>Level of measurement of variable from
model file.</term>
<term termid="ptbl_model_variable_name"><linktext>PTBL model variable
name</linktext>Variable name from model file.</term>
<term termid="ptbl_model_variable_type"><linktext>PTBL model variable
type</linktext>Variable type from model file.</term>
<term termid="ptbl_model_variable_width"><linktext>PTBL model variable
width</linktext>Variable width from model file.</term>
<term termid="ptbl_model_variables"><linktext>PTBL model variables</linktext
>Properties of variables from model file.</term>
<term termid="ptbl_new_cases"><linktext>PTBL new cases</linktext>Number
of new cases added to the data file as a result of forecasting beyond
the end of the current data.</term>
<term termid="ptbl_new_variables_tshow"><linktext>PTBL new variables
(TSHOW)</linktext>Shows the setting of NEWVAR for the TSET command.
NONE means no new variables are created. ALL creates new variables
without replacing existing ones. For CURRENT, the new series are saved
as temporary variables in the active dataset and any existing temporary
variables created by Forecasting commands are dropped.</term>
<term termid="ptbl_non-seasonal_differencing"><linktext>PTBL Non-seasonal
differencing</linktext>Degree of non-seasonal differencing specified
for the analysis.</term>
<term termid="ptbl_non-seasonal_lags_arima"><linktext>PTBL non-seasonal
lags (ARIMA)</linktext>Values for non-seasonal autoregressive and
moving-average orders.</term>
<term termid="ptbl_number_parameters_arima"><linktext>PTBL number
parameters (ARIMA)</linktext>Number of autoregressive and moving-average
parameters.</term>
<term termid="ptbl_number_residuals_arima"><linktext>PTBL number residuals
(ARIMA)</linktext>Number of model residuals. This number is reduced
by including differencing transformations.</term>
<term termid="ptbl_original_minus_moving_average_season"><linktext
>PTBL original minus moving average (SEASON)</linktext>For additive
seasonality, this is the difference between the original series and
the moving average series.</term>
<term termid="ptbl_original_series"><linktext>PTBL original series</linktext
>Case by case listing of the original series.</term>
<term termid="ptbl_output_amount_tshow"><linktext>PTBL output amount
(TSHOW)</linktext>Shows the setting of the PRINT subcommand of the
TSET command. For procedures with multiple iterations, BRIEF generally
means that final statistics are displayed with no iteration history.
DEFAULT provides a one-line summary at each iteration in addition
to the final statistics. DETAILED provides a complete summary of each
iteration (where necessary) plus the final statistics.</term>
<term termid="ptbl_parameter_change_arima"><linktext>PTBL parameter
change (ARIMA)</linktext>Iteration stops if no parameter changes by
more than this value from one iteration to the next.</term>
<term termid="ptbl_period_heading_season"><linktext>PTBL period heading
 (SEASON)</linktext>Number indexing the seasonal factors from 1 to
the specified periodicity.</term>
<term termid="ptbl_predict_tshow"><linktext>PTBL predict (TSHOW)</linktext
>Specifies the observations that mark the beginning and end of the
forecast period. The range is set with the PREDICT command.</term>
<term termid="ptbl_range_of_lags_ccf"><linktext>PTBL Range of Lags
(CCF)</linktext>Range of positive and negative lags for which output
is produced.</term>
<term termid="ptbl_ratio_original_to_moving_average_season"><linktext
>PTBL ratio original to moving average (SEASON)</linktext>For multiplicative
seasonality, this is the ratio of the original series to the moving
average series, expressed as a percentage.</term>
<term termid="ptbl_regression_coeff_arima"><linktext>PTBL regression
coeff (ARIMA)</linktext>Regression coefficients, if any, for any independent
variables.</term>
<term termid="ptbl_regression_tolerance_tshow"><linktext>PTBL regression
tolerance (TSHOW)</linktext>Shows the setting of TOLER for the TSET
command. Tolerance is the proportion of the variance of a variable
that is not accounted for by other independent variables in the equation.</term>
<term termid="ptbl_rejected_log_transform_arima"><linktext>PTBL rejected
log transform (ARIMA)</linktext>Number of cases rejected when applying
a log transform to the data. Cases with negative values and values
of zero cannot be log-transformed.</term>
<term termid="ptbl_residual_ssq_arima"><linktext>PTBL residual SSQ
(ARIMA)</linktext>Sum of squares of the model residuals.</term>
<term termid="ptbl_residual_variance_arima"><linktext>PTBL residual
variance (ARIMA)</linktext>Variance of the model residuals.</term>
<term termid="ptbl_rho_areg"><linktext>PTBL rho (AREG)</linktext>Value
of the first-order autoregressive parameter.</term>
<term termid="ptbl_season_seasonal_pct"><linktext>PTBL season seasonal
%</linktext>Seasonal factors for multiplicative seasonality. The seasonal
component is a factor by which the seasonally adjusted series is multiplied
to yield the original series. Observations without seasonal variation
have a seasonal component of 1.</term>
<term termid="ptbl_seasonal_ar_arima"><linktext>PTBL seasonal AR (ARIMA)</linktext
>The number of seasonal autoregressive parameters in the model. Seasonal
components work just like their nonseasonal counterparts, but they
"skip over" the seasonal interval. For example, a seasonal order 1
AR process would model a given value based on the value lagged by
the seasonal interval.</term>
<term termid="ptbl_seasonal_differencing"><linktext>PTBL seasonal
differencing</linktext>Degree of seasonal differencing specified for
analysis.</term>
<term termid="ptbl_seasonal_differencing_arima"><linktext>PTBL seasonal
differencing (ARIMA)</linktext>Degree of seasonal differencing specified
for the analysis. Seasonal components work just like their nonseasonal
counterparts, but they "skip over" the seasonal interval.</term>
<term termid="ptbl_seasonal_factor"><linktext>PTBL seasonal factor</linktext
>Seasonal factors for additive seasonality. The seasonal adjustments
are added to the seasonally adjusted series to obtain the observed
values. Observations without seasonal variation have a seasonal component
of 0.</term>
<term termid="ptbl_seasonal_indices_exsmooth"><linktext>PTBL seasonal
indices (EXSMOOTH)</linktext>The set of seasonal factors. The number
of seasonal factors is equal to the length of the seasonal period.</term>
<term termid="ptbl_seasonal_lags_arima"><linktext>PTBL seasonal lags
(ARIMA)</linktext>Values for seasonal autoregressive and moving-average
orders.</term>
<term termid="ptbl_seasonal_ma_arima"><linktext>PTBL seasonal MA (ARIMA)</linktext
>The number of seasonal moving average orders in the model. Seasonal
components work just like their nonseasonal counterparts, but they
"skip over" the seasonal interval.</term>
<term termid="ptbl_seasonality_exsmooth"><linktext>PTBL seasonality
(EXSMOOTH)</linktext>Type of seasonality associated with selected
model. For additive seasonality, seasonal adjustments are added to
the seasonally adjusted series to obtain the original series. For
multiplicative seasonality, the seasonal component is a factor by
which the seasonally adjusted series is multiplied to yield the original
series.</term>
<term termid="ptbl_series_length"><linktext>PTBL Series Length</linktext
>Number of cases in series. Excludes any cases that have been filtered
out.</term>
<term termid="ptbl_series_name"><linktext>PTBL Series Name</linktext
>Variable label (or variable name) of variable used in analysis.</term>
<term termid="ptbl_series_pair_ccf"><linktext>PTBL series pair (CCF)</linktext
>The two series whose cross-correlations are shown.</term>
<term termid="ptbl_simple_seasonal_exsmooth"><linktext>PTBL simple
seasonal (EXSMOOTH)</linktext>A model with no trend component and
an additive seasonal component.</term>
<term termid="ptbl_spss_variables"><linktext>PTBL Variables</linktext
>Variable in active dataset associated with variable from model file.</term>
<term termid="ptbl_sse_exsmooth"><linktext>PTBL SSE (EXSMOOTH)</linktext
>Sum of the squared differences between the predicted and observed
series values.</term>
<term termid="ptbl_ssq_termination_arima"><linktext>PTBL SSQ termination
(ARIMA)</linktext>A relative change in the adjusted sum of squares
by less than this amount causes termination.</term>
<term termid="ptbl_standard_error_method"><linktext>PTBL Standard
Error Method</linktext>Method used for calculating the standard errors
of the autocorrelations.</term>
<term termid="ptbl_std_errors_autocorr_tshow"><linktext>PTBL std errors
autocorr (TSHOW)</linktext>Shows the setting of ACFSE for the TSET
command. Specifies the method of calculating the standard errors for
autocorrelations. The IND (independence model) method assumes the
underlying process is white noise. The MA method is based on Bartlett's
approximation. With this method, appropriate where the true MA order
of the process is k-1, standard errors grow at increased lags.</term>
<term termid="ptbl_system_missing"><linktext>PTBL system missing</linktext
>Number of cases identified with system-missing values.</term>
<term termid="ptbl_time_series_settings_tshow"><linktext>PTBL time
series settings (TSHOW)</linktext>Global parameters to be used by
procedures that analyze time series and sequence variables. These
are set with the TSET command.</term>
<term termid="ptbl_time_series_transformation"><linktext>PTBL Time
Series Transformation</linktext>Transformation, if any, applied to
time series.</term>
<term termid="ptbl_to_ccf"><linktext>PTBL To (CCF)</linktext>Maximum
positive lag (first series leading).</term>
<term termid="ptbl_to_tshow"><linktext>PTBL to (TSHOW)</linktext>Designates
the end of a range of cases.</term>
<term termid="ptbl_trend_exsmooth"><linktext>PTBL trend (EXSMOOTH)</linktext
>Trend coefficient. Applies to models with linear or exponential trend.</term>
<term termid="ptbl_trend_phi_exsmooth"><linktext>PTBL trend phi (EXSMOOTH)</linktext
>Initial series trend for the specific Phi value in the presence of
a damped trend component.</term>
<term termid="ptbl_use_tshow"><linktext>PTBL use (TSHOW)</linktext
>Designates the range of observations used. The range is set with
the USE command.</term>
<term termid="ptbl_user_missing"><linktext>PTBL user missing</linktext
>Number of cases identified with user-missing values.</term>
<term termid="ptbl_user-missing_tshow"><linktext>PTBL user-missing
(TSHOW)</linktext>Shows the setting of MISSING for the TSET command.
This controls the treatment of user-missing values. INCLUDE indicates
that observations with user-missing values should be treated as valid
values and included in analyses. EXCLUDE indicates that observations
with user-missing values should be excluded from analyses.</term>
<term termid="ptbl_valid_values"><linktext>PTBL Valid Values</linktext
>Series length minus number of cases with missing values.</term>
<term termid="ptbl_values_lost_to_differencing"><linktext>PTBL values
lost to differencing</linktext>Number of values lost due to seasonal
and non-seasonal differencing.</term>
<term termid="ptbl_winters_additive_exsmooth"><linktext>PTBL Winter's
Additive (EXSMOOTH)</linktext>A model that assumes that the series
has a linear trend and an additive seasonal variation.</term>
<term termid="ptbl_with_ccf"><linktext>PTBL With (CCF)</linktext>Every
series listed before WITH is paired with every series associated with
the WITH label.</term>
<term termid="pure_error"><linktext>Pure Error</linktext>Displays
results for variation due to pure error. This is the component of
the error sum of squares due to pure error. If the current model is
appropriate, the sum of squares due to pure error will be large.</term>
<term termid="pzl"><linktext>PZL</linktext>The number of standard
deviations between the process mean and the lower specification limit.</term>
<term termid="pzmax"><linktext>PZMAX</linktext>The maximum number
of standard deviations between the process mean and the specification
limits.</term>
<term termid="pzmin"><linktext>PZMIN</linktext>The minimum number
of standard deviations between the process mean and the specification
limits.</term>
<term termid="pzout"><linktext>PZOUT</linktext>The estimated percentage
outside the specification limits.  The standard normal approximation
is based on the Z-upper and Z-lower.</term>
<term termid="pzlout"><linktext>PZLOUT</linktext>The estimated percentage
outside the lower specification limit. The standard normal approximation
is based on Z-lower.</term>
<term termid="pzuout"><linktext>PZUOUT</linktext>The estimated percentage
outside the upper specification limit. The standard normal approximation
is based on Z-upper.</term>
<term termid="pzu"><linktext>PZU</linktext>The number of standard
deviations between the process mean and the upper specification limit.</term>
<term termid="quadratic"><linktext>Quadratic</linktext>Tests the quadratic
effects across all levels.</term>
<term termid="quadratic_model"><linktext>Quadratic Model</linktext
>Model whose equation is Y = b0 + (b1 * t) + (b2 * t**2). The quadratic
model can be used to model a series that "takes off" or a series that
dampens.</term>
<term termid="quadratic_regression_scatterplot_options"><linktext
>Quadratic Regression (Scatterplot Options)</linktext>A least-squares
regression curve, including a squared term, that best fits the data
points on the scatterplot.</term>
<term termid="quadratic_term_1"><linktext>Quadratic Term</linktext
>The fixed effects that are involved in the quadratic term in the
expected mean square. The term is quadratic in the estimable function
of an effect.</term>
<term termid="quadratic_term"><linktext>Quadratic Term</linktext>Displays
the results of the 2nd-degree polynomial term. A 2nd-degree polynomial
term is the term with a variable raised to the 2nd power.</term>
<term termid="quadrature_spectrum"><linktext>Quadrature spectrum</linktext
>The imaginary part of the cross-periodogram, which is a measure of
the correlation of the out-of-phase frequency components of two time
series.  The components are out of phase by pi/2 radians.</term>
<term termid="gquantile"><linktext>Quantiles</linktext>Values that
divide the cases into some number of equal-sized groups.</term>
<term termid="quartiles"><linktext>Quartiles</linktext>Displays values
corresponding to the 25th, 50th, and 75th percentiles.</term>
<term termid="quartimax_method"><linktext>Quartimax Method</linktext
>A rotation method that minimizes the number of factors needed to
explain each variable. This method simplifies the interpretation of
the observed variables.</term>
<term termid="quasi_likelihood_under_independence_model"><linktext
>Quasi Likelihood under Independence Model</linktext>The linear model
parameter estimates, computed under the hypothesized working correlation
structure, are used to compute the value of a "quasi-likelihood" function
that assumes an independent correlation structure and known scale
parameter.</term>
<term termid="quasi_likelihood_under_independence_model_criterion"
><linktext>Quasi Likelihood under Independence Model Criterion (QIC)</linktext
>A measure for choosing between two correlation structures, given
a set of model terms. Smaller values indicate better models.</term>
<term termid="quest"><linktext>QUEST</linktext>Quick, Unbiased, Efficient
Statistical Tree.  A method that is fast and avoids other methods'
bias in favor of predictors with many categories. QUEST can be specified
only if the dependent variable is nominal.</term>
<term termid="r"><linktext>R</linktext>The multiple correlation coefficient
is the linear correlation between the observed and model-predicted
values of the dependent variable. It ranges in value from -1 to +1.</term>
<term termid="r_statistic"><linktext>R statistic</linktext>A measure
of linear association between two variables. Values of R range between
-1 (a perfect negative relationship in which all points fall on a
line with negative slope) and +1 (a perfect positive relationship
in which all points fall on a line with positive slope). A value of
0 indicates no linear relationship.</term>
<term termid="random_effects"><linktext>Random Effects</linktext>Model
effects for which the factor levels are considered to be a random
sample of possible levels about which conclusions are desired.</term>
<term termid="random_effects_model"><linktext>Random Effects Model</linktext
>A model in which the factor levels are considered to be a random
sample of possible levels about which conclusions are desired. Also
called a variance component model. The standard error and 95% confidence
interval for the mean are computed differently in a random effects
model. If you have a warning that the estimate of the variance under
the random effects model is negative, this means that your within-groups
sum of squares is larger than your between-groups sum of squares.</term>
<term termid="random_start"><linktext>Random Start</linktext>The seed
and normalized raw stress are displayed for each random start.  The
start with the lowest stress is used as the initial configuration.</term>
<term termid="range_3"><linktext>Range</linktext>Inclusive range of
values. Not available for string variables. Any user-missing values
within the range are included.</term>
<term termid="range"><linktext>Range</linktext>The difference between
the largest and smallest values of a numeric variable, the maximum
minus the minimum.</term>
<term termid="range_bar_chart"><linktext>Range Bar Chart</linktext
>Displays bars that "float" to show the range between high and low
values. In a simple range bar chart, there is one bar for each category
or case on the category axis. The high and low values are defined
by a variable with two categories, or by separate summary variables.</term>
<term termid="range_of_solutions"><linktext>Range of Solutions</linktext
>Displays cluster memberships for a range of cluster solutions. Enter
values corresponding to the lowest and highest cluster solutions.
Both values must be integers greater than 1, and the first value must
be less than the second value.</term>
<term termid="range_test_lo_hi_lo_hi_.."><linktext>RANGE(test,lo,hi[,lo,hi,..])</linktext
>RANGE(test,lo,hi[,lo,hi,..]). Logical. Returns 1 or true if test
is within any of the inclusive range(s) defined by the pairs lo, hi.
Arguments must be all numeric or all strings of the same length, and
each of the lo, hi pairs must be ordered with lo &lt;= hi. Note: For
string values, results can vary by locale even for the same set of
characters, since the national collating sequence is used. Language
order, not ASCII order, determines where certain characters fall in
the sequence.</term>
<term termid="rank_3"><linktext>Rank</linktext>The row rank or column
rank. The maximum number of linearly independent rows or columns.</term>
<term termid="rank_5"><linktext>Rank</linktext>The rank according
to the specified "best" criterion.  The "best" has rank 1, the second
"best" has rank 2, and so on.</term>
<term termid="rank"><linktext>Rank</linktext>The location of a case
when the values of all cases are arranged from smallest to largest.</term>
<term termid="rank_rank_cases"><linktext>Rank (Rank Cases)</linktext
>Simple rank. The value of the new variable equals its rank.</term>
<term termid="rank_assigned_to_ties"><linktext>Rank Assigned to Ties</linktext
>Specifies the method used to handle tied values.</term>
<term termid="rankit_probability_plot"><linktext>Rankit (Probability
Plot)</linktext>Calculates the expected normal distribution using
the formula (r-1/2) / n, where n is the number of observations, and
r is the rank, ranging from 1 to n.</term>
<term termid="rankit_rank_cases"><linktext>Rankit (Rank Cases)</linktext
>Uses the formula (r-1/2) / w, where w is the number of observations
and r is the rank, ranging from 1 to w.</term>
<term termid="ranks"><linktext>Ranks</linktext>This dimension lists
the negative and positive ranks, along with the number of tied ranks,
and the total number of pairs.</term>
<term termid="raos_v"><linktext>Rao's V</linktext>A measure of the
differences between group means. Also called the Lawley-Hotelling
trace. At each step, the variable that maximizes the increase in Rao's
V is entered.</term>
<term termid="raos_v_discriminant_analysis"><linktext>Rao's V (Discriminant
Analysis)</linktext>A measure of the differences between group means.
Also called the Lawley-Hotelling trace. At each step, the variable
that maximizes the increase in Rao's V is entered. After selecting
this option, enter the minimum value a variable must have to enter
the analysis.</term>
<term termid="ratio_estimate"><linktext>Ratio Estimate</linktext>The
estimate of the ratio of the numerator to the denominator.</term>
<term termid="ratio_of_aic_changes"><linktext>Ratio of AIC changes</linktext
>The ratio of the current AIC change to the AIC change for going from
1 to 2 clusters.</term>
<term termid="ratio_of_bic_changes"><linktext>Ratio of BIC changes</linktext
>The ratio of the current BIC change to the BIC change for going from
1 to 2 clusters.</term>
<term termid="ratio_of_d"><linktext>Ratio of d</linktext>The ratio
of the distance measures (Likelihood or Euclidean) for the next smaller
model to the distance measure for the given model.  Thus, the ratio
of distance measures for 4 clusters is the ratio of the distance measure
of the model with 3 clusters to the distance measure of the model
with 4 clusters.</term>
<term termid="gratiovar"><linktext>Ratio Variables</linktext>Quantitative
variables measured on a numeric scale in which distances between the
points on the scale can be compared meaningfully, and which have a
true (non-arbitrary) zero point.</term>
<term termid="raw"><linktext>Raw</linktext>Results for the original
data scaling when analyzing a covariance matrix</term>
<term termid="reason"><linktext>Reason</linktext>Variable(s) that
contribute the most to a case's classification as an anomaly.</term>
<term termid="recoded_value"><linktext>Recoded Value</linktext>Variable
value after recoding.</term>
<term termid="reference_category_2"><linktext>Reference Category</linktext
>The category to which other levels are compared.</term>
<term termid="gaxis12"><linktext>Reference Line</linktext>An optional
line drawn on a bar chart, line chart or scatterplot to emphasize
a particular value on a scale or category axis.</term>
<term termid="reference_pattern"><linktext>Reference Pattern</linktext
>The baseline hazard and survival function are computed for "typical"
values of the predictors, in addition to any patterns you may have
specified.</term>
<term termid="regression_5"><linktext>Regression</linktext>Displays
means, correlation matrix, and covariance matrix, computed from estimates
of missing values derived from a regression algorithm.</term>
<term termid="regression_anova"><linktext>regression (ANOVA)</linktext
>The variation in the dependent variable that is explained by the
model.</term>
<term termid="regression_analysis"><linktext>Regression Analysis</linktext
>Estimation of the linear relationship between a dependent variable
and one or more independent variables or covariates.</term>
<term termid="regression_coefficients"><linktext>Regression Coefficients</linktext
>Regression coefficients used to compute the regression equation.</term>
<term termid="regression_method"><linktext>Regression Method</linktext
>A method for estimating factor score coefficients. The scores that
are produced have a mean of 0 and a variance equal to the squared
multiple correlation between the estimated factor scores and the true
factor values. The scores may be correlated even when factors are
orthogonal.</term>
<term termid="regression_ss"><linktext>Regression SS</linktext>The
sum of squares information is displayed for the model.</term>
<term termid="r-e-g-w_f_post_hoc"><linktext>R-E-G-W F (Post Hoc)</linktext
>Ryan-Einot-Gabriel-Welsch multiple stepdown procedure based on an
F test.</term>
<term termid="r-e-g-w_q_post_hoc"><linktext>R-E-G-W Q (Post Hoc)</linktext
>Ryan-Einot-Gabriel-Welsch multiple stepdown procedure based on the
Studentized range.</term>
<term termid="relation_to_ranks_or_scores"><linktext>Relation to Ranks
or Scores</linktext>Specifies the expected relationship between each
factor and the rankings or scores. DISCRETE means no assumption is
made. LINEAR means that a linear relationship is expected. IDEAL and
ANTIIDEAL mean that a quadratic relationship is expected.</term>
<term termid="relative_median_potency"><linktext>Relative Median Potency</linktext
>Displays the ratio of median potencies for each pair of factor levels.
Also shows 95% confidence limits for each relative median potency.
Relative median potencies are not available if you do not have a factor
variable or if you have more than one covariate.</term>
<term termid="relative_risk"><linktext>Relative Risk</linktext>For
2 x 2 tables, a measure of the strength of the association between
the presence of a factor and the occurrence of an event. If the confidence
interval for the statistic includes a value of 1, you cannot assume
that the factor is associated with the event. The odds ratio can be
used as an estimate or relative risk when the occurrence of the factor
is rare.</term>
<term termid="reliability_analysis"><linktext>Reliability Analysis</linktext
>Procedure for evaluating multiple-item additive scales. The procedure
provides a large number of reliability coefficients for multiple-item
scales. Its subcommands encompass many different approaches to reliability
definition and estimation. In general, the concept of reliability
refers to how accurate, on the average, the estimate of the true score
is in a population of objects to be measured.</term>
<term termid="reliability_of_scale"><linktext>Reliability of Scale</linktext
>The estimated reliability of the item scale under the model (parallel
or strict parallel) assumption.</term>
<term termid="reliability_of_scale_unbiased"><linktext>Reliability
of Scale (Unbiased)</linktext>An unbiased estimate of the reliability
of the item scale under the model (parallel or strict parallel) assumption.</term>
<term termid="remove"><linktext>Remove</linktext>A procedure for variable
selection in which all variables in a block are removed in a single
step.</term>
<term termid="remove_1"><linktext>Remove</linktext>Deletes the selected
value from the list.</term>
<term termid="removed"><linktext>Removed</linktext>Variable removed
at a step.</term>
<term termid="repeated_contrast"><linktext>Repeated Contrast</linktext
>Compares the mean of each level (except the last) to the mean of
the subsequent level.</term>
<term termid="repeated_contrasts"><linktext>Repeated Contrasts</linktext
>A repeated contrast compares adjacent categories. Each category of
the predictor variable or factor except the first category is compared
to the category that precedes it.</term>
<term termid="repeated_effects"><linktext>Repeated Effects</linktext
>Model effects associated with the repeated measurements.</term>
<term termid="repeated_measures"><linktext>Repeated Measures</linktext
>Model parameters associated with the repeated effects.</term>
<term termid="replace_working_data_file_1"><linktext>Replace active
dataset</linktext>Replaces the active dataset with the aggregated
data file.  The file includes the break variables that define the
aggregated cases and all aggregate variables defined by aggregate
functions. This does not affect the original data file. The aggregated
data file is not saved unless you explicitly save the data file (File
menu, Save, or Save As).</term>
<term termid="replace_existing"><linktext>Replace existing</linktext
>The new series are saved as temporary variables in the active dataset,
and any existing temporary variables created by Forecasting commands
are dropped. Variable names are formed from a three-letter prefix,
a pound sign (#), and a number.</term>
<term termid="replace_existing_1"><linktext>Replace existing</linktext
>The new series created by Seasonal Decomposition are saved as temporary
variables in your active dataset. At the same time, any existing temporary
variables created by the Forecasting procedures are dropped. Variable
names are formed from a three-letter prefix, a pound sign (#), and
a number.</term>
<term termid="replace_missing_values_with_mean"><linktext>Replace
Missing Values With Mean</linktext>During classification, means are
substituted for missing values for predictor variables, and cases
with missing values are classified.</term>
<term termid="replace_with_mean"><linktext>Replace with Mean</linktext
>Replaces missing values with the variable mean.</term>
<term termid="replace_a1_a2_a3_a4"><linktext>REPLACE(a1,a2,a3,a4)</linktext
>REPLACE(a1, a2, a3[, a4]). String. In a1, instances of a2 are replaced
with a3. The optional argument a4 specifies the number of occurrences
to replace; if a4 is omitted, all occurrences are replaced. Arguments
a1, a2, and a3 must resolve to string values (literal strings enclosed
in quotes or string variables), and the optional argument a4 must
resolve to a non-negative integer. For example, REPLACE("abcabc",
"a", "x") returns a value of "xbcxbc" and REPLACE("abcabc", "a", "x",
1) returns a value of "xbcabc".</term>
<term termid="report_values"><linktext>Report Values</linktext>Treats
missing values for factor variables as a separate category. All output
is produced for this additional category. Frequency tables include
categories for missing values.</term>
<term termid="reproduced_correlation"><linktext>Reproduced Correlation</linktext
>The predicted correlation based on the factor model</term>
<term termid="reproduced_correlation_matrix_factor_analysis"><linktext
>Reproduced Correlation Matrix (Factor Analysis)</linktext>The estimated
correlation matrix from the factor solution. Residuals (difference
between estimated and observed correlations) are also displayed.</term>
<term termid="reproduced_covariance"><linktext>Reproduced Covariance</linktext
>The predicted covariance estimate based on the factor model</term>
<term termid="requested"><linktext>Requested</linktext>The number
or proportion of units requested during sampling.</term>
<term termid="rescale_long_table"><linktext>Rescale Long Table</linktext
>Shrinks a long table, keeping the aspect ratio, to fit the length
specifications in Page Setup, for printing only.</term>
<term termid="rescale_wide_table"><linktext>Rescale Wide Table</linktext
>Shrinks a wide table, keeping the aspect ratio, to fit the width
specifications in Page Setup, for printing only.</term>
<term termid="rescaled"><linktext>Rescaled</linktext>Results for the
rescaled (standardized) variables</term>
<term termid="rescaling"><linktext>Rescaling method</linktext>Rescaling
scale variables improves network training.  All rescaling is based
on the training data.  The standardized rescaling method subtracts
the mean and divides by the standard deviation; the normalized method
subtracts the minimum and divides by the range so that values fall
between 0 and 1; the adjusted normalized method "stretches and recenters"
the normalized method so values fall between -1 and 1.</term>
<term termid="reserved_attributes"><linktext>Reserved Attributes</linktext
>Custom variable or dataset attributes with names that start with
a dollar sign ($). These attributes are reserved for internal use.</term>
<term termid="reset"><linktext>Reset</linktext>Resets all settings
to the system defaults and removes all variable assignments from the
dialog box.</term>
<term termid="residual_8"><linktext>Residual</linktext>The difference
between the observed correlation or covariance and the value predicted
by the factor model</term>
<term termid="residual_10"><linktext>Residual</linktext>The variability
associated with the error term.</term>
<term termid="residual_7"><linktext>Residual</linktext>The difference
between an observed value and the expected value. Large absolute values
for the residuals indicate that the observed values are very different
from the expected values.</term>
<term termid="blank_5"><linktext>Residual</linktext>The dispersion
in the data that is not accounted for by the model.</term>
<term termid="residual_3"><linktext>Residual</linktext>The difference
between the observed and expected frequencies for a cell. A positive
value indicates that the cell frequency is higher than you would expect
if the two variables were unrelated.</term>
<term termid="residual_2"><linktext>Residual</linktext>The portion
of the total variation of the dependent variable that cannot be explained
by the main effects of the independent variables and their interactions.</term>
<term termid="residual_11"><linktext>Residual</linktext>The part of
the within-people variability that cannot be explained by differences
between items.</term>
<term termid="residual"><linktext>Residual</linktext>The actual value
of the dependent variable minus the value predicted by the regression
equation.</term>
<term termid="residual_anova"><linktext>residual (ANOVA)</linktext
>Statistics related to the variation in the dependent variable that
is not explained by the model.</term>
<term termid="residual_general_loglinear"><linktext>Residual (General
Loglinear)</linktext>Also called the simple or raw residual, it is
the difference between the observed cell count and its expected count.</term>
<term termid="residual_error_season_1"><linktext>residual error  (SEASON)</linktext
>The remainder of removing the seasonal, trend, and cycle components
from the original series.</term>
<term termid="residual_ss"><linktext>Residual SS</linktext>The sum
of squared deviations between the observed values and those predicted
from the model.</term>
<term termid="residual_sscp_glm"><linktext>Residual SSCP (GLM)</linktext
>Sums of squares and cross products of residuals. The dimension of
the RSSCP matrix is the same as the number of dependent variables
in the model. For the residual covariance matrix, the residual SSCP
matrix is divided by the degrees of freedom of the residual. The residual
correlation matrix is the standardized form of the residual covariance
matrix.</term>
<term termid="residual_sum_of_squares"><linktext>Residual Sum of Squares</linktext
>The default loss function is the sum of the squared residuals under
the current estimates of the model parameters.</term>
<term termid="residuals_5"><linktext>Residuals</linktext>Error terms
are chosen randomly from the observed residuals of complete cases
to be added to the regression estimates.</term>
<term termid="residuals_nonlin"><linktext>Residuals</linktext>Saves
residuals with the variable name resid.</term>
<term termid="residuals"><linktext>Residuals</linktext>The observed
value of the dependent variable minus the value predicted by the regression
equation, for each case. Large absolute values for the residuals indicate
that the observed values are very different from the predicted values.</term>
<term termid="gcurvefit13"><linktext>Residuals (Time Series)</linktext
>The actual value of the dependent variable minus the value predicted
by the model. This is the error component of the original series,
apart from the trend-cycle component and seasonal factors. The variable
names begin with err.</term>
<term termid="respondents_tables_statistic"><linktext>Respondents
(Tables Statistic)</linktext>The (weighted) number of cases in the
cell. (This can be less than Responses.) Available for multiple response
sets and their totals.</term>
<term termid="response_tree"><linktext>Response</linktext>The percentage
of cases in the node in the specified target category.</term>
<term termid="response_col_pct_tables_statistic"><linktext>Response
Col % (Tables Statistic)</linktext>The percentage of all the responses
in the column that are in the cell. Available for multiple-response
sets and their totals.</term>
<term termid="response_layer_pct_tables_statistic"><linktext>Response
Layer % (Tables Statistic)</linktext>The percentage of all the responses
in the layer that are in the cell. Available for multiple-response
sets and their totals.</term>
<term termid="response_percentage"><linktext>Response Percentage</linktext
>The cell count of responses as a percentage of the total.</term>
<term termid="response_row_pct_tables_statistic"><linktext>Response
Row % (Tables Statistic)</linktext>The percentage of all the responses
in the row that are in the cell. Available for multiple-response sets
and their totals.</term>
<term termid="response_subtable_pct_tables_statistic"><linktext>Response
Subtable % (Tables Statistic)</linktext>The percentage of all the
responses in the smallest subtable containing the cell that are actually
in the cell. Available for multiple-response sets and their totals.</term>
<term termid="response_table_pct_tables_statistic"><linktext>Response
Table % (Tables Statistic)</linktext>The percentage of all the responses
in the table that are in the cell. Available for multiple-response
sets and their totals.</term>
<term termid="responses_2"><linktext>Responses</linktext>Statistics
related to the responses to each item in the multiple response set.
 Note that the reported percentages are in terms of the total number
of responses, thus they add to 100%.</term>
<term termid="responses"><linktext>Responses</linktext>Count of responses
for a group variable.</term>
<term termid="responses_tables_statistic"><linktext>Responses (Tables
Statistic)</linktext>The number of responses in the cell. (This can
be more than Respondents, the number of cases.) Available for multiple-response
sets and their totals.</term>
<term termid="result_variable"><linktext>Result Variable</linktext
>Variables that contain the original nonmissing values and computed
values for missing values.</term>
<term termid="results"><linktext>Results</linktext>Summary of results
for the final tree, including independent variables included in final
tree and number of nodes and levels.</term>
<term termid="results_at_each_step"><linktext>Results at Each Step</linktext
>Displays step-by-step output for stepwise methods. Statistics include
Wilks' lambda, equivalent F, degrees of freedom, and significance
of F for each step. Tolerance and the value of the statistic used
for variable selection are reported for all variables. F-to-remove,
F-to-enter, and minimum tolerance are also displayed.</term>
<term termid="revenue_expense_and_profit"><linktext>Revenue, Expense,
and Profit</linktext>Profit values assigned to dependent variable
categories prior to building the model are computed as user-specified
revenue minus user-specified expense. Profit values affect average
profit and ROI (return on investment) values in gains tables. They
do not affect the basic tree model structure.</term>
<term termid="reversals"><linktext>Reversals</linktext>A reversal
occurs when a subject shows preferences for a given factor that are
opposite to expectations; for instance, showing higher preference
for higher price.</term>
<term termid="right_outer_join"><linktext>Right Outer Join</linktext
>A join used to link data from two tables, it includes all records
from 'Table 2' and only those records from 'Table 1' where the related
fields are equal. 'Table 2' is the table to which you dragged the
join line.</term>
<term termid="rindex_haystack_needle"><linktext>RINDEX(haystack,needle)</linktext
>RINDEX(haystack,needle). Numeric. Returns an integer that indicates
the starting position of the last occurrence of the string needle
in the string haystack. Returns 0 if needle does not occur within
haystack.</term>
<term termid="rindex_haystack_needle_divisor"><linktext>RINDEX(haystack,needle,divisor)</linktext
>RINDEX(haystack,needle[,divisor]). Numeric.  Returns an integer that
indicates the starting byte position of the last occurrence of the
string needle in the string haystack.  The optional third argument,
divisor, is the number of bytes used to divide needle into separate
strings. Divisor must be a positive integer and must divide evenly
into the length of needle. If needle is not found, the value 0 is
returned.</term>
<term termid="risk_difference"><linktext>Risk Difference</linktext
>The difference between the risk of an event in the presence of the
factor and the risk of the event in the absence of the factor.</term>
<term termid="root_mean_square_error_rmse"><linktext>RMSE</linktext
>Root Mean Square Error. The square root of mean square error. A measure
of how much a dependent series varies from its model-predicted level,
expressed in the same units as the dependent series.</term>
<term termid="rnd_numexpr"><linktext>RND(numexpr,mult,fuzzbits)</linktext
>RND(numexpr[,mult,fuzzbits]). Numeric. With a single argument, returns
the integer nearest to that argument. Numbers ending in .5 exactly
are rounded away from 0. For example, RND(-4.5) rounds to -5. The
optional second argument, mult, specifies that the result is an integer
multiple of this value&#x2014;for example, RND(-4.57,0.1) = -4.6.
The value must be numeric but cannot be 0. The default is 1. The optional
third argument, fuzzbits, is the number of least-significant bits
by which numexpr may fall short of the threshold for rounding up (e.g.,
0.5 when rounding to an integer) but still be rounded up. If omitted,
the system setting of FUZZBITS (set to 6 at install time) is used.</term>
<term termid="robust_tests_statistic"><linktext>Robust Tests Statistic</linktext
>The value of the Welch or Brown-Forsythe statistic.</term>
<term termid="rogers_and_tanimoto_similarity_measure"><linktext>Rogers
and Tanimoto Similarity Measure</linktext>Similarity measure for binary
data. A matching coefficient in which joint absences are included
in the numerator and nonmatches are given double weight in the denominator.
Computed from a fourfold table as (a+d)/(a+d+2(b+c)) where a represents
cases present on both items, d represents cases absent on both items,
and b and c represent cases present on one item but absent on the
other.</term>
<term termid="roi"><linktext>ROI</linktext>Return on investment. The
average profit divided by the average expense for the node, expressed
as a percent.</term>
<term termid="role_in_imputation"><linktext>Role in Imputation</linktext
>Marks whether the variable was used as a dependent, predictor, or
both, during imputation.</term>
<term termid="def_root"><linktext>Root (output item type)</linktext
>The first item in the outline of the Viewer. By default, it is labeled
OUTPUT. The user can select the root item and cut, copy, paste or
delete every item in the Viewer, and can expand or collapse it. A
root item exists only in the outline. The user can change its label
text but cannot activate it for further editing.</term>
<term termid="rotated_solution"><linktext>Rotated Solution</linktext
>A rotation method must be selected to obtain a rotated solution.
For orthogonal rotations, the rotated pattern matrix and factor transformation
matrix are displayed. For oblique rotations, the pattern, structure,
and factor correlation matrices are displayed.</term>
<term termid="role"><linktext>Role</linktext>Role can be Input, Target,
Both, None, Partition, or Split. Used to pre-select fields in dialogs
that support predefined roles.</term>
<term termid="rotation"><linktext>Rotation</linktext>Allows you to
specify a rotation method or obtain factor loading plots. Rotation
is a general method for making a factor solution easier to interpret.
There are several different methods for rotation.</term>
<term termid="rotation_sums_of_squared_loadings"><linktext>Rotation
Sums of Squared Loadings</linktext>Sums of Squared Loadings for the
rotated factor solution</term>
<term termid="round"><linktext>Round</linktext>The pth percentile
is estimated by the case number closest to np.</term>
<term termid="round_case_weights"><linktext>Round case weights</linktext
>Case weights are rounded before use.</term>
<term termid="round_cell_counts"><linktext>Round cell counts</linktext
>Case weights are used as is but the accumulated weights in the cells
are rounded before computing any statistics.</term>
<term termid="row_10"><linktext>Row</linktext>Dimension containing
the category numbers or category value labels of the row variable.</term>
<term termid="row_dimension_labels_in_corner"><linktext>Row Dimension
Labels in Corner</linktext>Displays the dimension labels in the corner
of the table.</term>
<term termid="row_dimension_labels_nested"><linktext>Row Dimension
Labels Nested</linktext>Displays the dimension label in a row before
the first category label for the dimension.</term>
<term termid="row_objects"><linktext>Row Objects</linktext>The row
objects are represented by cases, or rows, in the dataset.</term>
<term termid="row_percentage_crosstabs"><linktext>Row Percentage (Crosstabs)</linktext
>The percentage of all the cases in a row that fall into a particular
cell.</term>
<term termid="row_principal"><linktext>Row Principal</linktext>Distances
between the row points are approximations of the selected distance
measure. This method maximizes distances between row points. Use this
method if you want to examine differences or similarities between
categories of the row variable.</term>
<term termid="row_s_variable_list"><linktext>Row(s) Variable List</linktext
>Selected variables are displayed in rows in the crosstabulation.
Selected variables should be categorical (variables with a limited
number of distinct values). A crosstabulation is produced for each
combination of row and column variables.</term>
<term termid="rows"><linktext>Rows</linktext>Variables on this list
are displayed as rows.</term>
<term termid="roys_largest_root"><linktext>Roy's Largest Root</linktext
>A multivariate test of the significance of each effect in the model.
It is the largest eigenvalue.</term>
<term termid="rpad_strexpr_length"><linktext>RPAD(strexpr,length)</linktext
>RPAD(strexpr,length). String. Returns the string strexpr padded on
the right with blanks to extend it to the length given by length.
The value of length represents the number of bytes and must be a positive
integer. </term>
<term termid="rpad_strexpr_length_char"><linktext>RPAD(strexpr1,length,strexpr2)</linktext
>RPAD(strexpr1,length[,strexpr2]). String. Right-pads strexpr1 with
strexpr2 to extend it to the length given by length using as many
complete copies as will fit of strexpr2 as the padding string. The
value of length represents the number of bytes and must be a positive
integer. The optional third argument strexpr2 is a quoted string or
an expression that resolves to a string. If strepxr2 is omitted, the
value is padded with blanks.</term>
<term termid="r_squared"><linktext>R-Squared</linktext>Goodness-of-fit
measure of a linear model, sometimes called the coefficient of determination.
It is the proportion of variation in the dependent variable explained
by the regression model. It ranges in value from 0 to 1. Small values
indicate that the model does not fit the data well.</term>
<term termid="r-squared"><linktext>R-squared</linktext>An estimate
of the proportion of the total variation in the series that is explained
by the model. This measure is most useful when the series is stationary.
R-squared can be negative with a range of negative infinity to 1.
Negative values mean that the model under consideration is worse than
the baseline model. Positive values mean that the model under consideration
is better than the baseline model.</term>
<term termid="r_squared_change"><linktext>R-Squared Change</linktext
>The change in the R-squared statistic that is produced by adding
or deleting an independent variable. If the R-squared change associated
with a variable is large, that means that the variable is a good predictor
of the dependent variable.</term>
<term termid="rtrim_strexpr"><linktext>RTRIM(strexpr)</linktext>RTRIM(strexpr).
String. Returns the string strexpr trimmed of any trailing blanks.
This function is normally used within a larger expression, since strings
are padded with trailing blanks upon being assigned to variables.</term>
<term termid="rtrim_strexpr_char"><linktext>RTRIM(strexpr,char)</linktext
>RTRIM(strexpr[,char]). String. Trims trailing instances of char within
strexpr. The optional second argument char is a single quoted character
or an expression that yields a single character. If char is omitted,
trailing blanks are trimmed.</term>
<term termid="rule_1"><linktext>Rule</linktext>A rule is used to determine
whether a case is valid. Single-variable rules consist of a fixed
set of checks that apply to a single variable.  Cross-variable rules
are user-defined rules that can be applied to a single variable or
a combination of variables.</term>
<term termid="rule_expression"><linktext>Rule Expression</linktext
>The computational expression that defines the rule.</term>
<term termid="rules_file"><linktext>Rules file</linktext>The text
file that contains the case selection or scoring rules for the model.</term>
<term termid="runs"><linktext>Runs</linktext>Any sequence of like
observations in an ordered group of values. The likeness may be the
same value, the same sign, or being from the same sample.</term>
<term termid="runs_test"><linktext>Runs Test</linktext>A one-sample
nonparametric test for randomness in a dichotomous variable. A run
is any sequence of cases having the same value. The total number of
runs in a sample is a measure of randomness in the order of the cases
in the sample. Too many or too few runs can suggest a nonrandom (dependent)
ordering. The runs test is only appropriate when the order of cases
is meaningful.</term>
<term termid="russell_and_rao_similarity_measure"><linktext>Russell
and Rao Similarity Measure</linktext>Similarity measure for binary
data. The binary dot product, computed from a fourfold table as a/(a+b+c+d)
where a represents cases present on both items, d represents cases
absent on both items, and b and c represent cases present on one item
but absent on the other.</term>
<term termid="rv.bernoulli_prob"><linktext>RV.BERNOULLI(prob)</linktext
>RV.BERNOULLI(prob). Numeric. Returns a random value from a Bernoulli
distribution with the specified probability parameter prob.</term>
<term termid="rv.beta_shape1_shape2"><linktext>RV.BETA(shape1, shape2)</linktext
>RV.BETA(shape1, shape2). Numeric. Returns a random value from a Beta
distribution with specified shape parameters.</term>
<term termid="rv.binom_n_prob"><linktext>RV.BINOM(n, prob)</linktext
>RV.BINOM(n, prob). Numeric. Returns a random value from a binomial
distribution with specified number of trials and probability parameter.</term>
<term termid="rv.cauchy_loc_scale"><linktext>RV.CAUCHY(loc, scale)</linktext
>RV.CAUCHY(loc, scale). Numeric. Returns a random value from a Cauchy
distribution with specified location and scale parameters.</term>
<term termid="rv.chisq_df"><linktext>RV.CHISQ(df)</linktext>RV.CHISQ(df).
Numeric. Returns a random value from a chi-square distribution with
specified degrees of freedom df.</term>
<term termid="rv.exp_shape"><linktext>RV.EXP(shape)</linktext>RV.EXP(scale).
Numeric. Returns a random value from an exponential distribution with
specified scale parameter.</term>
<term termid="rv.f_df1_df2"><linktext>RV.F(df1, df2)</linktext>RV.F(df1,
df2). Numeric. Returns a random value from an F distribution with
specified degrees of freedom, df1 and df2.</term>
<term termid="rv.gamma_shape_scale"><linktext>RV.GAMMA(shape, scale)</linktext
>RV.GAMMA(shape, scale). Numeric. Returns a random value from a Gamma
distribution with specified shape and scale parameters.</term>
<term termid="rv.geom_prob"><linktext>RV.GEOM(prob)</linktext>RV.GEOM(prob).
Numeric. Returns a random value from a geometric distribution with
specified probability parameter.</term>
<term termid="rv.halfnrm"><linktext>RV.HALFNRM</linktext>RV.HALFNRM(mean,
stddev). Numeric. Returns a random value from a half normal distribution
with the specified mean and standard deviation.</term>
<term termid="rv.hyper_total_sample_hits"><linktext>RV.HYPER(total,
sample, hits)</linktext>RV.HYPER(total, sample, hits). Numeric. Returns
a random value from a hypergeometric distribution with specified parameters.</term>
<term termid="rv.igauss"><linktext>RV.IGAUSS</linktext>RV.IGAUSS(loc,
scale). Numeric. Returns a random value from an inverse Gaussian distribution
with the specified location and scale parameters.</term>
<term termid="rv.laplace_mean_scale"><linktext>RV.LAPLACE(mean, scale)</linktext
>RV.LAPLACE(mean, scale). Numeric. Returns a random value from a Laplace
distribution with specified mean and scale parameters.</term>
<term termid="rv.lnormal_a_b"><linktext>RV.LNORMAL(a, b)</linktext
>RV.LNORMAL(a, b). Numeric. Returns a random value from a log-normal
distribution with specified parameters.</term>
<term termid="rv.logistic_mean_scale"><linktext>RV.LOGISTIC(mean,
scale)</linktext>RV.LOGISTIC(mean, scale). Numeric. Returns a random
value from a logistic distribution with specified mean and scale parameters.</term>
<term termid="rv.negbin_threshold_prob"><linktext>RV.NEGBIN(threshold,
prob)</linktext>RV.NEGBIN(threshold, prob). Numeric. Returns a random
value from a negative binomial distribution with specified threshold
and probability parameters.</term>
<term termid="rv.normal_mean_stddev"><linktext>RV.NORMAL(mean, stddev)</linktext
>RV.NORMAL(mean, stddev). Numeric. Returns a random value from a normal
distribution with specified mean and standard deviation.</term>
<term termid="rv.pareto_threshold_shape"><linktext>RV.PARETO(threshold,
shape)</linktext>RV.PARETO(threshold, shape). Numeric. Returns a random
value from a Pareto distribution with specified threshold and shape
parameters.</term>
<term termid="rv.poisson_mean"><linktext>RV.POISSON(mean)</linktext
>RV.POISSON(mean). Numeric. Returns a random value from a Poisson
distribution with specified mean/rate parameter.</term>
<term termid="rv.t_df"><linktext>RV.T(df)</linktext>RV.T(df). Numeric.
Returns a random value from a Student's t distribution with specified
degrees of freedom df.</term>
<term termid="rv.uniform_min_max"><linktext>RV.UNIFORM(min, max)</linktext
>RV.UNIFORM(min, max). Numeric. Returns a random value from a uniform
distribution with specified minimum and maximum. See also the UNIFORM
function.</term>
<term termid="rv.weibull_a_b"><linktext>RV.WEIBULL(a, b)</linktext
>RV.WEIBULL(a, b). Numeric. Returns a random value from a Weibull
distribution with specified parameters.</term>
<term termid="s.e._of_mean_predicted_value"><linktext>S.E. of Mean
Predicted Value</linktext>An estimate of the standard deviation of
the average value of the dependent variable for cases that have the
same values of the independent variables.</term>
<term termid="s.e._of_mean_predictions"><linktext>S.E. of Mean Predictions</linktext
>Standard errors of the predicted values. An estimate of the standard
deviation of the average value of the dependent variable for cases
that have the same values of the independent variables.</term>
<term termid="sample"><linktext>Sample</linktext>Selects a random
sample based on an approximate percentage or an exact number of cases.</term>
<term termid="sample_information"><linktext>Sample Information</linktext
>These fields provide further details of the sampling design.</term>
<term termid="sampling_design_degrees_of_freedom"><linktext>Sampling
Design Degrees of Freedom</linktext>The degrees of freedom used in
computing p-values for all test statistics.  The default value is
the difference between the number of primary sampling units and the
number of strata in the first stage of sampling.</term>
<term termid="sampling_zeros"><linktext>Sampling Zeros</linktext>Cells
that have no observed values.</term>
<term termid="savage_score_rank_cases"><linktext>Savage Score (Rank
Cases)</linktext>The new variable contains Savage scores based on
an exponential distribution. </term>
<term termid="save_2"><linktext>Save</linktext>Allows you to save
classification information or discriminant scores for each case.</term>
<term termid="save_4"><linktext>Save</linktext>Allows you to save
cluster memberships.</term>
<term termid="save"><linktext>Save</linktext>Allows you to save predicted
values, residuals, and related measures as new variables which are
added to the active dataset. A table in the output shows the name
of each new variable and its contents.</term>
<term termid="save_as"><linktext>Save As</linktext>Saves the current
table properties to a specified file and directory.</term>
<term termid="save_as_variables"><linktext>Save As Variables</linktext
>Saves factor scores as variables. One variable is created for each
factor in the solution. A table in the output shows the name of each
new variable and a variable label indicating the method used to calculate
factor scores.</term>
<term termid="save_look"><linktext>Save Look</linktext>Saves the current
table properties to the currently selected TableLook file.</term>
<term termid="save_standardized_values_as_variables"><linktext>Save
Standardized Values As Variables</linktext>Creates and saves one Z-score
variable for each selected variable. New variable names are created
by prefixing the letter z to the first seven characters of original
variable names.</term>
<term termid="scale_2"><linktext>Scale</linktext>The scale of the
results</term>
<term termid="scale_10"><linktext>Scale</linktext>A variable can be
treated as scale (continuous) when its values represent ordered categories
with a  meaningful metric, so that distance comparisons between values
are appropriate.  Examples of scale variables include age in years
and income in thousands of dollars. </term>
<term termid="scale_11"><linktext>scale</linktext>The scale parameter
for the distribution. For instance, for the normal distribution this
is the parameter 'b' in NORMAL(a,b).</term>
<term termid="gscale1"><linktext>Scale Axis</linktext>An axis that
displays numerical values to scale. (A category axis, in contrast,
displays individual values separately and not necessarily to scale.)
Bar charts and line charts usually have at least one scale axis, plus
one category axis. Scatterplots have at least two scale axes but no
category axis. Histograms have a scale axis and an interval axis.</term>
<term termid="scale_mean_if_item_deleted"><linktext>Scale Mean if
Item Deleted</linktext>A measure for examining the relationship between
individual items and the total scale, this is the mean value of the
item sum if the given item is not included in the scale.</term>
<term termid="scale_parameter"><linktext>Scale parameter</linktext
>The scale parameter is an estimated model parameter related to the
variance of the response.</term>
<term termid="gaxis08"><linktext>Scale Range</linktext>Determines
the maximum and minimum values along the scale axis.</term>
<term termid="scale_variance_if_item_deleted"><linktext>Scale Variance
if Item Deleted</linktext>A measure for examining the relationship
between individual items and the total scale, this is the variance
of the item sum if the given item is not included in the scale.</term>
<term termid="scale_weight"><linktext>Scale Weight</linktext>The scale,
or omega, weights are "known" values that determine each case's relative
contribution to the estimate of the scale parameter. Cases with scale
weight values that are less than or equal to zero, or missing, are
not used in the analysis.</term>
<term termid="scaled_deviance"><linktext>Scaled Deviance</linktext
>The ratio of the deviance statistic to the scale parameter.</term>
<term termid="scaled_pearson_chi-square"><linktext>Scaled Pearson
Chi-Square</linktext>The ratio of the Pearson chi-square statistic
to the scale parameter.</term>
<term termid="gaxis14"><linktext>Scaling Factor</linktext>A factor
by which the axis labels have been divided for display purposes. If
the scaling factor is 1000, axis labels are 1000 times smaller than
the corresponding data values. If the scaling factor is 0.01, axis
labels are 100 times larger than the corresponding data values.</term>
<term termid="scatterplot_matrix"><linktext>Scatterplot Matrix</linktext
>Plots a square matrix of simple (bivariate) scatterplots, plotting
all possible pairs of the selected variables against one another.</term>
<term termid="scheffe"><linktext>Scheffe</linktext>Performs simultaneous
joint pairwise comparisons for all possible pairwise combinations
of means. Uses the F sampling distribution. Can be used to examine
all possible linear combinations of group means, not just pairwise
comparisons.</term>
<term termid="scheffe_test"><linktext>Scheffe test</linktext>Performs
simultaneous joint pairwise comparisons for all possible pairwise
combinations of means. Uses the F sampling distribution. This test
can be used to examine all possible linear combinations of group means,
not just pairwise comparisons.</term>
<term termid="schwarz_bayesian_criterion_sbc"><linktext>Schwarz Bayesian
Criterion (BIC)</linktext>A statistic which helps you decide the order
of a model. BIC takes into account both how well the model fits the
observed series, and the number of parameters used in the model. Look
for that model which adequately describes the series and has the minimum
BIC. The BIC is based on Bayesian (maximum-likelihood) considerations.</term>
<term termid="schwarzs_bayesian_criterion"><linktext>Schwarz's Bayesian
Criterion</linktext>A statistic that helps you choose between models.
The BIC takes into account both how well the model fits the observed
series, and the number of parameters used in the fit. Smaller values
of the BIC indicate better models.</term>
<term termid="scientific_notation_variable"><linktext>Scientific Notation
Variable</linktext>Defines a numeric variable whose values are displayed
with an imbedded E and a signed power-of-ten exponent. The Data Editor
accepts numeric values for such variables with or without an exponent.
The exponent can be preceded either by E or D with an optional sign,
or by the sign alone. For example, 123, 1.23E2, 1.23D2, 1.23E+2, and
even 1.23+2.</term>
<term termid="score"><linktext>score</linktext>Custom scores assigned
to each category of the dependent variable. Scores define the order
of and distance between categories of the dependent variable. You
can use scores to increase/decrease the relative distance between
ordinal values or change the order of the values.</term>
<term termid="score_3"><linktext>Score</linktext>The value of the
derivative for the estimated parameters.</term>
<term termid="score_4"><linktext>Score</linktext>The total utility
for the simulation case.</term>
<term termid="score_in_dimension"><linktext>Score in dimension</linktext
>Optimal quantification assigned to a category in a particular dimension.</term>
<term termid="score_statistic"><linktext>Score statistic</linktext
>Test statistic to determine whether the coefficient is different
from zero, based on the change in log-likelihood associated with the
effect</term>
<term termid="scores_1"><linktext>Scores</linktext>The marginal proportions
and scores for each row and column.</term>
<term termid="scores"><linktext>Scores</linktext>Allows you to save
scores for use in other analyses.</term>
<term termid="scree"><linktext>Scree</linktext>A plot of the variance
that is associated with each factor. This plot is used to determine
how many factors should be kept. Typically the plot shows a distinct
break between the steep slope of the large factors and the gradual
trailing of the rest (the scree).</term>
<term termid="s-curve_model"><linktext>S-Curve Model</linktext>Model
whose equation is Y = e**(b0 + (b1/t)) or ln(Y) = b0 + (b1/t).</term>
<term termid="sd_numexpr_numexpr_.."><linktext>SD(numexpr,numexpr[,..])</linktext
>SD(numexpr,numexpr[,..]). Numeric. Returns the standard deviation
of its arguments that have valid, nonmissing values. This function
requires two or more arguments, which must be numeric. You can specify
a minimum number of valid arguments for this function to be evaluated.</term>
<term termid="seasonal"><linktext>Seasonal</linktext>The seasonal
column contains text boxes in which you can specify the corresponding
parameters (sp, sd, and sq) of the process at seasonal lags. These
values can be 0 or a positive integer, usually 1.</term>
<term termid="seasonal_additive"><linktext>Seasonal additive</linktext
>An outlier that affects a particular observation and all subsequent
observations separated from it by one or more seasonal periods. All
such observations are affected equally. A seasonal additive outlier
might occur if, beginning in a certain year, sales are higher every
January.</term>
<term termid="seasonal_autoregressive_ar_orders"><linktext>Seasonal
Autoregressive (AR) Orders</linktext>Specifies that the model contains
seasonal autoregressive orders. The details for each lag (order) are
provided.</term>
<term termid="gexsmooth07"><linktext>Seasonal Delta Value (Exponential
Smoothing)</linktext>Exponential smoothing parameter that controls
the relative weight given to recent observations in estimating the
present seasonality. It ranges from 0 to 1, with values near 1 giving
higher weight to recent values. Delta is used for all exponential
smoothing models with a seasonal component. It is not used for the
simple or Holt models.</term>
<term termid="seasonal_denominator_orders"><linktext>Seasonal Denominator
Orders</linktext>Specifies that the transfer function for the associated
independent variable contains seasonal denominator orders. The details
for each lag (order) are provided.</term>
<term termid="seasonal_difference_1"><linktext>Seasonal Difference</linktext
>Specifies the number of seasonal differencing (integration) orders
for the associated dependent or independent variable.</term>
<term termid="gx11arima02"><linktext>Seasonal Factors</linktext>Seasonal
components of the original variables. For models with seasonal components,
you can optionally move a variable containing seasonal factors into
the Seasonal Factors box. The Seasonal Decomposition procedure creates
such variables. The adjusted seasonal averages indicate the net effect
of each period on the level of the series. The variable names begin
with saf.</term>
<term termid="seasonal_numerator_orders"><linktext>Seasonal Numerator
Orders</linktext>Specifies that the transfer function for the associated
independent variable contains seasonal numerator orders. The details
for each lag (order) are provided.</term>
<term termid="seasonally_adjusted_series"><linktext>seasonally adjusted
series</linktext>The original series with seasonal variations removed.</term>
<term termid="gx11arima01"><linktext>Seasonally Adjusted Variables</linktext
>Variables with the seasonal variation removed. Seasonal adjustment
serves to highlight trend, thus enabling you to assess whether the
series values have increased or decreased net of the seasonal effect.
The saved variable names begin with sas.</term>
<term termid="gacf03"><linktext>Seasonally Difference</linktext>Transforms
time series data by calculating the difference between series values
a constant span apart. Enter a positive integer to specify the degree
of differencing (the number of time periods used to calculate the
difference). This transformation is only available if the periodicity
of the series has been defined (using Define Dates on the Data menu).</term>
<term termid="second_highest_group"><linktext>Second highest group</linktext
>Statistics concerning the second most strongly predicted group membership
for each case, based on the discriminant function(s)</term>
<term termid="second-order_controls"><linktext>Second-order Controls</linktext
>The second factor specified.  The analysis is stratified on levels
of this factor.</term>
<term termid="seed"><linktext>Seed</linktext>The seed used to generate
each random start.</term>
<term termid="select"><linktext>Select</linktext>Allows you to limit
the analysis to a subset of cases having a particular value for a
variable. After selecting this option choose a selection variable
and enter a set value for the case-selection variable.</term>
<term termid="selected_8"><linktext>Selected</linktext>Variables selected
by the procedure.</term>
<term termid="selected"><linktext>Selected</linktext>Displays information
for cases selected by the selection variable.</term>
<term termid="selected_data_file"><linktext>Selected Data File</linktext
>ASCII text data file to be defined.</term>
<term termid="selection_4"><linktext>Selection</linktext>Selection
status (selected or unselected) of cases</term>
<term termid="selection_criteria"><linktext>Selection Criteria</linktext
>Displays the different selection criteria for each model.</term>
<term termid="selection_method"><linktext>Selection Method</linktext
>The method used to extract the sample. Methods differ in several
ways, including whether units are selected with equal or unequal probability,
whether the selection is systematic or random, and whether units are
selected with or without replacement.</term>
<term termid="selection_variable"><linktext>Selection Variable</linktext
>Choose a selection variable to limit the analysis to a subset of
cases having a particular value for this variable.</term>
<term termid="sensitivity"><linktext>Sensitivity</linktext>A measure
of the usefulness of a classification scheme.  Sensitivity is the
probability that a "positive" case is correctly classified, and is
plotted on the y-axis in an ROC curve.  1-sensitivity is the false
negative rate.</term>
<term termid="separate_variance_estimate"><linktext>Separate Variance
Estimate</linktext>The test of the null hypothesis that two population
means are equal must use separate-variance estimates when the variances
for the two populations are not assumed to be equal.</term>
<term termid="separate-groups"><linktext>Separate-Groups</linktext
>Separate-groups covariance matrices are used for classification.
Because classification is based on the discriminant functions (not
based on the original variables), this option is not always equivalent
to quadratic discrimination.</term>
<term termid="separate-groups_covariance"><linktext>Separate-Groups
Covariance</linktext>Displays separate covariance matrices for each
group.</term>
<term termid="separate-groups_plots"><linktext>Separate-Groups Plots</linktext
>Creates separate-group scatterplots of the first two discriminant
function values. If there is only one function, histograms are displayed
instead.</term>
<term termid="sequential_bonferroni"><linktext>Sequential Bonferroni</linktext
>This is a sequentially step-down rejective Bonferroni procedure that
is much less conservative in terms of rejecting individual hypotheses
but maintains the same overall significance level.</term>
<term termid="sequential_quadratic_programming"><linktext>Sequential
Quadratic Programming</linktext>This method is available for constrained
and unconstrained models. Sequential quadratic programming is used
automatically if you specify a constrained model, a user-defined loss
function, or bootstrapping.  You can enter new values for Maximum
iterations and Step limit, and you can change the selection in the
drop-down lists for Optimality tolerance, Function precision, and
Infinite step size.</term>
<term termid="sequential_sidak"><linktext>Sequential Sidak</linktext
>This is a sequentially step-down rejective Sidak procedure that is
much less conservative in terms of rejecting individual hypotheses
but maintains the same overall significance level.</term>
<term termid="gseries01"><linktext>Series (Charts)</linktext>Series
distinguish between the items represented within a each group of items
for a category. For example, each bar within a cluster belongs to
a different series. Each series is represented by a different color
or fill pattern on the chart legend.</term>
<term termid="series_name"><linktext>Series Name</linktext>Variable
name assigned to the calculated series.</term>
<term termid="set"><linktext>Set</linktext>Variables are listed according
to the sets to which they were assigned.</term>
<term termid="set_definition"><linktext>Set Definition</linktext>Displays
the numeric variables in the data file.</term>
<term termid="set_value"><linktext>Set Value</linktext>Allows you
to enter a value for the case-selection variable that limits the analysis
to a subset of cases having this value for the selected variable.</term>
<term termid="shape_2"><linktext>shape</linktext>The shape parameter
for the distribution. For instance, for the Weibull distribution this
is the parameter 'b' in WEIBULL(a,b).</term>
<term termid="shape_distances"><linktext>Shape Distance Measure</linktext
>Difference measure for binary data that has no lower or upper limit.
Computed from a fourfold table as ((a+b+c+d)(b+c)-(b-c)**2)/(a+b+c+d)**2
where a represents cases present on both items, d represents cases
absent on both items, and b and c represent cases present on one item
but absent on the other.</term>
<term termid="shape1"><linktext>shape1</linktext>The shape1 parameter
for a Beta distribution of the form BETA(shape1,shape2).</term>
<term termid="shape2"><linktext>shape2</linktext>The shape2 parameter
for a Beta distribution of the form BETA(shape1,shape2).</term>
<term termid="shapiro-wilk_test_explore"><linktext>Shapiro-Wilk Test
(Explore)</linktext>Tests the hypothesis that the sample is from a
normal population.</term>
<term termid="shepards_rough_nondegeneracy_index"><linktext>Shepard's
Rough Nondegeneracy Index</linktext>A measure of whether there are
sufficiently different distances, reported as a percentage of different
data elements or distances.  Larger values indicate solutions that
are less likely to be degenerate.</term>
<term termid="short_string_variable"><linktext>Short String Variable</linktext
>A string variable with a width of 8 or fewer bytes.</term>
<term termid="show_case_numbers"><linktext>Show Case Numbers</linktext
>Displays case numbers from the data file.</term>
<term termid="sidak"><linktext>Sidak</linktext>This method provides
tighter bounds than the Bonferroni approach.</term>
<term termid="sidak_post_hoc"><linktext>Sidak (Post Hoc)</linktext
>Pairwise multiple comparison test based on a t statistic. Sidak adjusts
the significance level for multiple comparisons and provides tighter
bounds than Bonferroni.</term>
<term termid="sig._f"><linktext>Sig. F</linktext>These significance
values should not be used to test hypotheses about the F values in
this table. Cluster analysis specifically attempts to maximize between-group
variance, and the significance values reported here do not reflect
this.</term>
<term termid="sig.chisq"><linktext>SIG.CHISQ</linktext>SIG.CHISQ(quant,
df). Numeric. Returns the cumulative probability that a value from
the chi-square distribution, with df degrees of freedom, will be greater
than quant</term>
<term termid="sig.f"><linktext>SIG.F</linktext>SIG.F(quant, df1, df2).
Numeric. Returns the cumulative probability that a value from the
F distribution, with degrees of freedom df1 and df2, will be greater
than quant.</term>
<term termid="sign_test"><linktext>Sign Test</linktext>A nonparametric
procedure used with two related samples to test the hypothesis that
two variables have the same distribution. The differences between
the two variables for all cases are computed and classified as either
positive, negative, or tied. If the two variables are similarly distributed,
the numbers of positive and negative differences will not be significantly
different.</term>
<term termid="significance"><linktext>Significance</linktext>Often
referred to as alpha, this is the probability that the null hypothesis
is rejected when it is in fact true (the probability of a Type I error).</term>
<term termid="sig_alterantive_hypothesis"><linktext>Significance (by
Alternative Hypothesis)</linktext>Reports significance values for
each of the listed hypotheses.</term>
<term termid="significance_model_fit"><linktext>Significance (model
fit)</linktext>A statistical test of whether the model adequately
fits the data. Small values (less than .10) indicate that there are
significant differences between the model and the data, indicating
a poor fit of the data.</term>
<term termid="significance_score_statistic"><linktext>Significance
(score statistic)</linktext>The level of statistical significance
indicated by the Score test for the coefficient. Small values (less
than 0.05) indicate that the coefficient is statistically different
from zero.</term>
<term termid="significance_criterion"><linktext>significance criterion</linktext
>A rule used to determine whether or not a test statistic is     
  significant.  Generally, the 0.05 criterion is used to indicate
significance and the 0.10 criterion is used to indicate lack of significance.</term>
<term termid="significance_level"><linktext>Significance Level</linktext
>The conditional probability that a relationship as strong as the
one observed in the data would be present, if the null hypothesis
were true. It is often called the p-value. Typically a value of less
than 0.05 is considered significant.</term>
<term termid="significance_levels_factor_analysis"><linktext>Significance
Levels (Factor Analysis)</linktext>One-tailed significance levels
of coefficients in the correlation matrix.</term>
<term termid="significance_of_bartletts_test_of_sphericity"><linktext
>Significance of Bartlett's test of sphericity</linktext>The probability
associated with the computed chi-square. Small values indicate that
there are significant relationships among the variables.</term>
<term termid="significance_of_f"><linktext>Significance of F</linktext
>The observed significance level of F. If this probability is small
enough, usually less than 0.05 or 0.01, the null hypothesis, that
there is no linear relationship between the dependent and independent
variables, is rejected.</term>
<term termid="significance_of_f_change"><linktext>Significance of
F Change</linktext>If the significance of F change is small (smaller
than say 0.05) then the null hypothesis that inclusion of an additional
variable does not result in a significant increase in R-squared is
rejected.</term>
<term termid="significance_of_the_change"><linktext>Significance of
the change</linktext>Significance of the change in -2 log likelihood
for the current step. Small values (less than .05) indicate a statistically
significant difference between the previous step and the current step.</term>
<term termid="significance_value"><linktext>significance value</linktext
>Also known as a p-value.  Most statistical tests, regardless of the
statistic used in the calculation of the test, report significance
values that lie on a common scale from 0 to 1.  Decreasing significance
values indicate evidence against the null hypothesis.  Generally,
values less than 0.05 mean the data are inconsistent with the null;
values greater than 0.10 mean the data are consistent with the null.</term>
<term termid="simple"><linktext>Simple</linktext>This model is appropriate
for series in which there is no trend or seasonality. Its only smoothing
parameter is level. Simple exponential smoothing is most similar to
an ARIMA model with zero orders of autoregression, one order of differencing,
one order of moving average, and no constant.</term>
<term termid="simple_bivariate_scatterplot"><linktext>Simple (Bivariate)
Scatterplot</linktext>Plots two variables on two scale axes. One variable
defines the horizontal axis and the other defines the vertical axis.
The values of the variables for any given case serve as the coordinates
of the point for that case.</term>
<term termid="simple_area_chart"><linktext>Simple Area Chart</linktext
>The chart has a single line that connects a series of points, one
for each category, case or variable on the category axis. The area
below the line is shaded.</term>
<term termid="simple_bar_chart"><linktext>Simple Bar Chart</linktext
>The chart contains a single bar for each category, case or variable
on the category axis.</term>
<term termid="simple_box_plot"><linktext>Simple Box Plot</linktext
>Contains a single box plot for each category or variable on the category
axis. Box plots show the median, quartiles, and extreme values for
the category or variable.</term>
<term termid="simple_contrast_1"><linktext>Simple Contrast</linktext
>Compares the mean of each level to the mean of a specified level.
This type of contrast is useful when there is a control group.</term>
<term termid="simple_contrast"><linktext>Simple Contrast</linktext
>In a simple contrast, each category except one of the predictor variables
or factors is compared to a reference category.</term>
<term termid="simple_contrasts"><linktext>Simple Contrasts</linktext
>Each category except one of the predictor variables or factors is
compared to a reference category. Select either First or Last as the
reference category.</term>
<term termid="simple_error_bar"><linktext>Simple Error Bar</linktext
>Contains a single error bar for each category or variable on the
category axis. Error bars show the confidence interval, standard error,
or standard deviation for a variable or category.</term>
<term termid="simple_matching_similarity_measure"><linktext>Simple
Matching Similarity Measure</linktext>Similarity measure for binary
data. The ratio of the number of matches to the total number of matches
and nonmatches. Computed from a fourfold table as (a+d)/(a+b+c+d)
where a represents present on both items, d represents cases absent
on both items, and b and c represent cases present on one item but
absent on the other.</term>
<term termid="gexsmooth01"><linktext>Simple Model (Exponential Smoothing)</linktext
>The simple model assumes that the series has no trend and no seasonal
variation.</term>
<term termid="simple_pareto_chart"><linktext>Simple Pareto Chart</linktext
>Contains a single bar for each category, case or variable on the
category axis, sorted in descending order. Bars represent counts or
sums. A line displaying the cumulative sum is superimposed.</term>
<term termid="simple_seasonal"><linktext>Simple seasonal</linktext
>This model is appropriate for series with no trend and a seasonal
effect that is constant over time. Its smoothing parameters are level
and season. Simple seasonal exponential smoothing is most similar
to an ARIMA model with zero orders of autoregression, one order of
differencing, one order of seasonal differencing, and orders 1, p,
and p + 1 of moving average, where p is the number of periods in a
seasonal interval (for monthly data, p = 12).</term>
<term termid="sin_radians"><linktext>SIN(radians)</linktext>SIN(radians).
Numeric. Returns the sine of radians, which must be a numeric value,
measured in radians.</term>
<term termid="single_and_multiple_fit"><linktext>Single and multiple
fit</linktext>Measures of goodness of fit of the single- and multiple-category
coordinates/category quantifications with respect to the objects.</term>
<term termid="single_category_coordinates"><linktext>Single Category
Coordinates</linktext>The single category coordinates equal the weights
times the quantifications, and represent the locations of the categories
on a line in the object space.</term>
<term termid="single_fit"><linktext>Single Fit</linktext>This is the
model fit that would result from restricting variables from being
treated as multiple nominal.</term>
<term termid="single_loss"><linktext>Single Loss</linktext>This is
the loss in fit that would result from restricting variables from
being treated as multiple nominal.</term>
<term termid="single_measures"><linktext>Single Measures</linktext
>Single measure applies to individual measurements, for example, the
ratings of judges, individual item scores, or the body weights of
individuals.</term>
<term termid="single_solution"><linktext>Single Solution</linktext
>Displays cluster memberships for a single-cluster solution with a
specified number of clusters. Enter an integer greater than 1.</term>
<term termid="single-variable"><linktext>Single-variable</linktext
>Single-variable rules consist of a fixed set of checks that apply
to a single variable.</term>
<term termid="singular_value"><linktext>Singular value</linktext>Measure
of the importance of a dimension.</term>
<term termid="size_difference_measure"><linktext>Size Difference Measure</linktext
>Dissimilarity measure for binary data. Computed from a fourfold table
as (b-c)**2/(a+b+c+d)**2 where a represents cases present on both
items, d represents cases absent on both items, and b and c represent
cases present on one item but absent on the other. Has a minimum value
of 0 and no upper limit.</term>
<term termid="skewness"><linktext>Skewness</linktext>A measure of
the asymmetry of a distribution. The normal distribution is symmetric
and has a skewness value of 0. A distribution with a significant positive
skewness has a long right tail. A distribution with a significant
negative skewness has a long left tail. As a guideline, a skewness
value more than twice its standard error is taken to indicate a departure
from symmetry.</term>
<term termid="slope"><linktext>Slope</linktext>A measure of the amount
of change in the dependent variable associated with each unit of change
in the independent variable. If the slope is zero, it means that there
is no linear relationship between the dependent variable and the independent
variable.</term>
<term termid="smoothed_trend-cycle_series"><linktext>smoothed trend-cycle
series</linktext>A smoothed version of the seasonally adjusted series
which shows both trend and cyclic components.</term>
<term termid="student-newman-keuls"><linktext>S-N-K (Student-Newman-Keuls)</linktext
>Makes all pairwise comparisons between means using the Studentized
range distribution. With equal sample sizes, it also compares pairs
of means within homogeneous subsets, using a stepwise procedure. Means
are ordered from highest to lowest, and extreme differences are tested
first.</term>
<term termid="sokal_and_sneath_1_similarity_measure"><linktext>Sokal
and Sneath 1 Similarity Measure</linktext>A matching coefficients
similarity measure for binary data in which all matches are included
in the denominator, joint absences are included in the numerator,
and double weight is given to matches. Computed from a fourfold table
as 2(a+d)/(2(a+d)+b+c) where a represents cases present on both items,
b and c represent cases present on one item but absent on the other,
and d represents cases absent on both items.</term>
<term termid="sokal_and_sneath_2_similarity_measure"><linktext>Sokal
and Sneath 2 Similarity Measure</linktext>A matching coefficients
similarity measure for binary data in which joint absences are excluded
from both the numerator and the denominator, and double weight is
given to nonmatches. Computed from a fourfold table as a/(a+2(b+c))
where a represents cases present on both items, and b and c represent
cases present on one item but absent on the other.</term>
<term termid="sokal_and_sneath_3_similarity_measure"><linktext>Sokal
and Sneath 3 Similarity Measure</linktext>A matching coefficients
similarity measure for binary data in which all matches are excluded
from the denominator, joint absences are included in the numerator,
and equal weight is given to matches and nonmatches. This measure
is undefined when there are no nonmatches. Computed from a fourfold
table as (a+d)/(b+c) where a represents cases present on both items,
b and c represent cases present on one item but absent on the other,
and d represents cases absent on both items.</term>
<term termid="sokal_and_sneath_4_similarity_measure"><linktext>Sokal
and Sneath 4 Similarity Measure</linktext>Similarity measure for binary
data that yields the conditional probability that a characteristic
of one item is in the same state (presence or absence) as the characteristic
of the other item. The measure is an average over both items acting
as predictors. It has a range of 0 to 1.</term>
<term termid="sokal_and_sneath_5_similarity_measure"><linktext>Sokal
and Sneath 5 Similarity Measure</linktext>Similarity measure for binary
data. This measure has a range of 0 to 1. Computed from a fourfold
table as ad/(SQRT((a+b)(a+c)(b+d)(c+d))) where a represents cases
present on both items, b and c represent cases present on one item
but absent on the other, and d represents cases absent on both items.</term>
<term termid="somers_d"><linktext>Somers' d</linktext>A measure of
association between two ordinal variables that ranges from -1 to 1.
Values close to an absolute value of 1 indicate a strong relationship
between the two variables, and values close to 0 indicate little or
no relationship between the variables. Somers' d is an asymmetric
extension of gamma that differs only in the inclusion of the number
of pairs not tied on the independent variable. A symmetric version
of this statistic is also calculated.</term>
<term termid="sort_file_before_aggregating"><linktext>Sort file before
aggregating</linktext>In very rare instances with large data files,
you may find it necessary to sort the data file by values of the break
variables prior to aggregating. This option is not recommended unless
you encounter memory or performance problems.</term>
<term termid="sorted_by_size"><linktext>Sorted By Size</linktext>Sorts
factor loading and structure matrices so that variables with high
loadings on the same factor appear together.</term>
<term termid="source_13"><linktext>Source</linktext>The sources of
proximities in the analysis.</term>
<term termid="source"><linktext>Source</linktext>Lists the different
sources of variability.</term>
<term termid="source_11"><linktext>Source</linktext>The model effect
being tested.</term>
<term termid="source_of_variation"><linktext>Source of Variation</linktext
>The component to which a portion of the variation in the dependent
variables is attributed.</term>
<term termid="source_variable_rank_cases"><linktext>Source Variable
(Rank Cases)</linktext>The name of the original variable. The values
of this variable remain unchanged.</term>
<term termid="source_variable_list"><linktext>Source Variable List</linktext
>Lists the dependent variable (DEPENDNT) and the following predicted
and residual variables: Standardized predicted values (*ZPRED), Standardized
residuals (*ZRESID), Deleted residuals (*DRESID), Adjusted predicted
values (*ADJPRED), Studentized residuals (*SRESID), Studentized deleted
residuals (*SDRESID).</term>
<term termid="source_variable_list_2"><linktext>Source Variable List</linktext
>Displays the numeric and string variables in the data file.</term>
<term termid="source_variable_list_1"><linktext>Source Variable List</linktext
>Displays the numeric variables in the data file.</term>
<term termid="sources_3"><linktext>Sources</linktext>The number of
sources of proximities.</term>
<term termid="gspectra09"><linktext>Span</linktext>The range of consecutive
values across which the smoothing is carried out. Generally, an odd
integer is used. Larger spans smooth the spectral density plot more
than smaller spans.</term>
<term termid="spearman_correlation"><linktext>Spearman Correlation</linktext
>A nonparametric version of the Pearson correlation coefficient, based
on the ranks of the data rather than the actual values. It is appropriate
for ordinal data, or for interval data that do not satisfy the normality
assumption. Values of the coefficient range from -1 to +1. The sign
of the coefficient indicates the direction of the relationship, and
its absolute value indicates the strength, with larger absolute values
indicating stronger relationships.</term>
<term termid="spearman_correlation_coefficient"><linktext>Spearman
Correlation Coefficient</linktext>Commonly used nonparametric measure
of correlation between two ordinal variables. For all of the cases,
the values of each of the variables are ranked from smallest to largest,
and the Pearson correlation coefficient is computed on the ranks.</term>
<term termid="spearman-brown_coefficient"><linktext>Spearman-Brown
Coefficient</linktext>The Spearman-Brown Coefficient is a measure
of reliability for split scales that treats the split scales as items
on a two-item scale and uses the correlation between forms as the
basis for the statistic.  The unequal-length version of the statistic
adjusts for split scales with different numbers of items.</term>
<term termid="special"><linktext>Special</linktext>A special contrast
is a user-defined contrast.</term>
<term termid="specifications"><linktext>Specifications</linktext>The
specifications used to generate the tree model</term>
<term termid="specificity"><linktext>Specificity</linktext>A measure
of the usefulness of a classification scheme. Specificity is the probability
that a "negative" case is correctly classified.  1-specificity is
the false positive rate, and is plotted on the x-axis in an ROC Curve.</term>
<term termid="specificity_prefscal"><linktext>Specificity (prefscal)</linktext
>Specificity is a measure of how different an individual space is
from the common space. An individual space that was identical to the
common space would have identical dimension weights and a specificity
of 0, while an individual space that was specific to a particular
dimension would have a single large dimension weight and a specificity
of 1.</term>
<term termid="specified_prior"><linktext>Specified Prior</linktext
>User-specified prior probabilities that do not sum to 1. A prior
probability is an estimate of the likelihood that a case belongs to
a particular group when no other information about it is available.</term>
<term termid="spectral_analysis"><linktext>Spectral Analysis</linktext
>A technique whereby a time series is decomposed into a sum of periodic
functions plus an error term.</term>
<term termid="gspectra02"><linktext>Spectral Density Plot</linktext
>A periodogram that has been smoothed to remove irregular variation.</term>
<term termid="sphericity_assumed"><linktext>Sphericity Assumed</linktext
>The values in this table are calculated assuming sphericity. Use
Mauchly's test to check sphericity for a repeated measures model.</term>
<term termid="spline_nominal"><linktext>Spline Nominal</linktext>Categories
are treated as unordered. Objects in the same category obtain the
same quantification. Categories will be on a straight line through
the origin. The resulting transformation is a smooth piecewise polynomial.</term>
<term termid="spline_ordinal"><linktext>Spline Ordinal</linktext>Categories
are treated as ordered. The order of the categories of the observed
variable is preserved in the optimally scaled variable. Categories
will be on a straight line through the origin. The resulting transformation
is a smooth nondecreasing piecewise polynomial of the chosen Degree.</term>
<term termid="split_values"><linktext>Split values</linktext>The values
of the primary independent variable (predictor) that define the node.</term>
<term termid="spss_12_compatible"><linktext>Version 12 Compatible</linktext
>The random number generator used in version 12 and previous releases.
If you need to reproduce randomized results generated in previous
releases based on a specified seed value, use this random number generator.</term>
<term termid="sqrt_numexpr"><linktext>SQRT(numexpr)</linktext>SQRT(numexpr).
Numeric. Returns the positive square root of numexpr, which must be
numeric and not negative.</term>
<term termid="square_root"><linktext>Square Root</linktext>Specifies
that a square root transformation was performed on the variable.</term>
<term termid="square_root_design_effect"><linktext>Square Root Design
Effect</linktext>This is a measure of the effect of specifying a complex
design, where values further from 1 indicate greater effects.</term>
<term termid="squared_coherency"><linktext>Squared coherency</linktext
>The product of the gains of the two series.</term>
<term termid="squared_euclidean_distance_measure_binary_data"><linktext
>Squared Euclidean Distance Measure (Binary Data)</linktext>The binary
squared Euclidean dissimilarity measure. Computed from a fourfold
table as b+c, where b and c represent cases present on one item but
absent on the other. Its minimum value is 0, and it has no upper limit.</term>
<term termid="squared_euclidean_distance_measure_interval_data"><linktext
>Squared Euclidean Distance Measure (Interval Data)</linktext>A measure
of distance between pairs of cases. The distance between two cases
is the sum of the squared differences between the values of the cases.
This dissimilarity measure is for continuous data.</term>
<term termid="squared_multiple_correlation"><linktext>Squared Multiple
Correlation</linktext>A measure for examining the relationship between
individual items and the total scale, this is the R-squared value
of the multiple regression in which the given item is the dependent
variable and the remaining items are predictors.  Smaller values indicate
the given item is not well correlated with the others.</term>
<term termid="sscp_glm"><linktext>SSCP (GLM)</linktext>Sums-of-squares
and cross-product matrices. Hypothesis and error SSCP matrices are
displayed for each effect in the design. Each between-subjects effect
has a different SSCP matrix, but there is a single error matrix for
all between-subjects effects. For repeated measures, each within-subjects
effect has both a hypothesis and an error matrix.</term>
<term termid="stacked_area_chart"><linktext>Stacked Area Chart</linktext
>The chart has two or more lines, with the area below each line shaded
differently. Each line connects one point for each category, case
or variable on the category axis. Lines can represent groups of cases,
separate variables, or individual cases.</term>
<term termid="stacked_bar_chart"><linktext>Stacked Bar Chart</linktext
>Bar segments are stacked on top of one another. There is one bar
stack for each category, case, or variable on the category axis. Segments
within each stack can represent groups of cases, separate variables,
or individual cases.</term>
<term termid="stacked_pareto_chart"><linktext>Stacked Pareto Chart</linktext
>Bar segments are stacked on top of one another. There is one bar
stack for each category on the category axis. Segments within each
stack can represent groups of cases, separate variables, or individual
cases. Stacks are sorted in descending order, and a line displaying
the cumulative sum is superimposed.</term>
<term termid="stage"><linktext>Stage</linktext>Stage at which the
clusters were joined</term>
<term termid="stage_2"><linktext>Stage</linktext>The sampling specifications
are presented by stage.  Multiple stages are useful when you want
to subsample from groups defined by the cluster variables.</term>
<term termid="stage_cluster_first_appears"><linktext>Stage cluster
first appears</linktext>Indicates stage at which the cluster was formed.
0 indicates that the cluster was a single case prior to this stage.</term>
<term termid="sd_mi"><linktext>Standard Deviation</linktext>The standard
deviation of the valid cases is reported for scale variables.</term>
<term termid="standard_deviation_8"><linktext>Standard Deviation</linktext
>The result of summing the squared deviations of the ratios about
the mean, dividing the result by the total number of ratios minus
one, and taking the positive square root.</term>
<term termid="standard_deviation_9"><linktext>standard deviation</linktext
>A measure of dispersion around the mean, equal to the square root
of the variance. The standard deviation is measured in the same units
as the original variable.</term>
<term termid="standard_deviation"><linktext>Standard deviation</linktext
>An estimate of the spread of the singular value. This measure indicates
the stability of the singular value in each dimension.</term>
<term termid="gbar11"><linktext>Standard Deviation</linktext>A measure
of dispersion around the mean. In a normal distribution, 68% of cases
fall within one standard deviation of the mean and 95% of cases fall
within two standard deviations. For example, if the mean age is 45,
with a standard deviation of 10, 95% of the cases would be between
25 and 65 in a normal distribution.</term>
<term termid="standard_deviation_spchart"><linktext>Standard Deviation</linktext
>The process standard deviation.</term>
<term termid="standard_deviation_aggregate_function"><linktext>Standard
Deviation (Aggregate Function)</linktext>The standard deviation of
the source variable for all cases in the break group. </term>
<term termid="standard_deviation_lt_d"><linktext>Standard Deviation
&lt; d</linktext>The standard deviation for these variables falls
below a chosen threshold.</term>
<term termid="standard_deviation_in_dimension"><linktext>Standard
deviation in dimension</linktext>An estimate of the spread of a point.
This measure indicates the stability of the point in each dimension.</term>
<term termid="standard_deviation_of_jonckheere-terpstra_statistic"
><linktext>Standard Deviation of Jonckheere-Terpstra Statistic</linktext
>The standard deviation of the Jonckheere-Terpstra statistic.</term>
<term termid="standard_deviation_of_marginal_homogeneity_statistic"
><linktext>Standard Deviation of Marginal Homogeneity Statistic</linktext
>The standard deviation of the marginal homogeneity statistic.</term>
<term termid="standard_error"><linktext>Standard Error</linktext>A
measure of how much the value of a test statistic varies from sample
to sample. It is the standard deviation of the sampling distribution
for a statistic. For example, the standard error of the mean is the
standard deviation of the sample means.</term>
<term termid="standard_error_of_kurtosis"><linktext>Standard Error
of Kurtosis</linktext>The ratio of kurtosis to its standard error
can be used as a test of normality (that is, you can reject normality
if the ratio is less than -2 or greater than +2). A large positive
value for kurtosis indicates that the tails of the distribution are
longer than those of a normal distribution; a negative value for kurtosis
indicates shorter tails (becoming like those of a box-shaped uniform
distribution).</term>
<term termid="gfreq13"><linktext>Standard Error of Mean</linktext
>A measure of how much the value of the mean may vary from sample
to sample taken from the same distribution. It can be used to roughly
compare the observed mean to a hypothesized value (that is, you can
conclude the two values are different if the ratio of the difference
to the standard error is less than -2 or greater than +2).</term>
<term termid="standard_error_of_skewness"><linktext>Standard Error
of Skewness</linktext>The ratio of skewness to its standard error
can be used as a test of normality (that is, you can reject normality
if the ratio is less than -2 or greater than +2). A large positive
value for skewness indicates a long right tail; an extreme negative
value indicates a long left tail.</term>
<term termid="standard_error_of_survival_k-m"><linktext>Standard Error
of Survival</linktext>Standard error of the cumulative survival estimate.
The default variable name is the prefix se_ with a sequential number
appended to it. For example, if se_1 already exists, Kaplan-Meier
assigns the variable name se_2.</term>
<term termid="standard_error_of_the_difference"><linktext>Standard
Error of the Difference</linktext>The standard deviation of the sample
differences.</term>
<term termid="standard_error_of_the_estimate"><linktext>Standard Error
of the Estimate</linktext>The standard error of model-estimated values
of the dependent variable.</term>
<term termid="standard_variable_attributes"><linktext>Standard variable
attributes</linktext>Standard dictionary attributes such as format,
variable and value labels, missing values. Does not include user-defined
custom variable attributes.</term>
<term termid="standardize_4"><linktext>standardize</linktext>Specifies
whether standardization was applied. Standardization transforms the
sequence or time series variables into a sample with a mean of 0 and
a standard deviation of 1.</term>
<term termid="standardize_values_probability_plot"><linktext>Standardize
Values (Probability Plot)</linktext>Transforms the data into a sample
with a mean of 0 and a standard deviation of 1.</term>
<term termid="standardized"><linktext>Standardized</linktext>The predicted
value for each case, standardized to have a mean of 0 and a standard
deviation of 1.</term>
<term termid="standardized_4"><linktext>Standardized</linktext>The
residual divided by an estimate of its standard error. Standardized
residuals are also known as Pearson residuals.</term>
<term termid="standardized_cross-product"><linktext>Standardized Cross-Product</linktext
>Equivalent to Pearson's correlation, except the means are assumed
to be 0, since regression is performed through the origin.</term>
<term termid="standardized_dfbeta"><linktext>Standardized DfBeta</linktext
>Standardized difference in beta value. The change in the regression
coefficient that results from the exclusion of a particular case.
You may want to examine cases with absolute values greater than 2
divided by the square root of N, where N is the number of cases. A
value is computed for each term in the model, including the constant.</term>
<term termid="standardized_dffit"><linktext>Standardized DfFit</linktext
>Standardized difference in fit value. The change in the predicted
value that results from the exclusion of a particular case. You may
want to examine standardized values which in absolute value exceed
2 times the square root of p/N, where p is the number of parameters
in the model and N is the number of cases.</term>
<term termid="standardized_jonckheere-terpstra_statistic"><linktext
>Standardized Jonckheere-Terpstra Statistic</linktext>The standardized
Jonckheere-Terpstra statistic is the statistic minus the mean, divided
by the standard deviation.</term>
<term termid="standardized_marginal_homogeneity_statistic"><linktext
>Standardized Marginal Homogeneity Statistic</linktext>The standardized
marginal homogeneity statistic is the statistic minus the mean, divided
by the standard deviation.</term>
<term termid="standardized_predicted_value"><linktext>Standardized
Predicted Value</linktext>A transformation of each predicted value
into its standardized form. That is, the mean predicted value is subtracted
from the predicted value, and the difference is divided by the standard
deviation of the predicted values. Standardized predicted values have
a mean of 0 and a standard deviation of 1.</term>
<term termid="standardized_residuals"><linktext>Standardized Residuals</linktext
>The residual divided by an estimate of its standard deviation. Standardized
residuals, which are also known as Pearson residuals, have a mean
of 0 and a standard deviation of 1.</term>
<term termid="standardized_scores"><linktext>Standardized Scores</linktext
>Scores that tells you how many standard deviation units above or
below the mean a value falls.</term>
<term termid="start"><linktext>Start</linktext>The seed and penalized
stress (and the penalized stress's component parts) are displayed
for each random start.  The start with the lowest penalized stress
is used as the initial configuration.</term>
<term termid="stat_type"><linktext>Stat Type</linktext>Dimension containing
statistical measures.</term>
<term termid="stationarity"><linktext>Stationarity</linktext>A condition
that must be met by the time series to which you fit an ARIMA model.
Pure MA series will be stationary; however, AR and ARMA series might
not be. A stationary series has a constant mean and a constant variance
over time.</term>
<term termid="stationary_r-squared"><linktext>Stationary R-squared</linktext
>A measure that compares the stationary part of the model to a simple
mean model. This measure is preferable to ordinary R-squared when
there is a trend or seasonal pattern. Stationary R-squared can be
negative with a range of negative infinity to 1. Negative values mean
that the model under consideration is worse than the baseline model.
Positive values mean that the model under consideration is better
than the baseline model.</term>
<term termid="statistic"><linktext>Statistic</linktext>Summary measures
used to describe the sample.</term>
<term termid="statistic_1"><linktext>Statistic</linktext>Displays
the value of the statistic. Click on the actual statistic name for
more useful information.</term>
<term termid="statistics_22"><linktext>Statistics</linktext>The value
of the Ljung-Box Q statistic for up to 18th order serial correlation
in the residuals.</term>
<term termid="statistics_4"><linktext>Statistics</linktext>Requests
optional statistical output including regression coefficients, descriptives,
model fit statistics, Durbin-Watson test, and collinearity diagnostics.</term>
<term termid="statistics_10"><linktext>Statistics</linktext>Dimension
containing statistics.</term>
<term termid="statistics"><linktext>Statistics</linktext>Requests
statistics, and measures of association.</term>
<term termid="statistics_5"><linktext>Statistics</linktext>Requests
optional descriptive statistics, function coefficients, and matrices.</term>
<term termid="statistics_6"><linktext>Statistics</linktext>Allows
you to obtain optional statistics.</term>
<term termid="statistics_checkbox"><linktext>Statistics Checkbox</linktext
>Displays statistics. To suppress statistical output, deselect this
item.</term>
<term termid="statistics_pushbutton_1"><linktext>Statistics Pushbutton</linktext
>Requests optional descriptive and summary statistics for numeric
variables.</term>
<term termid="statistics_pushbutton"><linktext>Statistics Pushbutton</linktext
>Requests optional statistics, including additional descriptive statistics
and robust estimators; displays outliers, or percentiles. You must
choose to display either Statistics or Both before you can request
optional statistics.</term>
<term termid="statistics_pushbutton_2"><linktext>Statistics Pushbutton</linktext
>Allows you to display an agglomeration schedule, a distance matrix
or cluster memberships.</term>
<term termid="statistics_radio_button"><linktext>Statistics Radio
Button</linktext>Displays statistics only and suppresses plots. Basic
descriptive statistics for each variable are displayed by default.</term>
<term termid="status_1"><linktext>Status</linktext>An X in the status
column indicates the case is not the selected case defined by the
selection variable.</term>
<term termid="status_2"><linktext>Status</linktext>Indicates whether
the case experienced the terminal event or was censored.</term>
<term termid="stem-and-leaf"><linktext>Stem-and-Leaf</linktext>Displays
the distribution of values for a variable. Each observed value is
subdivided into two components--the leading digits (stem) and trailing
digits (leaf).</term>
<term termid="step_3"><linktext>Step</linktext>Step number.</term>
<term termid="step_limit_nonlinear_regression"><linktext>Step Limit
(Nonlinear Regression)</linktext>The maximum permissible change in
the length of the parameter vector. You may specify any positive value.</term>
<term termid="step_statistics"><linktext>Step statistics</linktext
>Summary statistics for the specified step of the stepwise procedure</term>
<term termid="stepwise"><linktext>Stepwise</linktext>At each step,
the independent variable not in the equation that has the smallest
probability of F is entered, if that probability is sufficiently small.
Variables already in the regression equation are removed if their
probability of F becomes sufficiently large. The method terminates
when no more variables are eligible for inclusion or removal.</term>
<term termid="stopping_rule"><linktext>Stopping Rule</linktext>Displays
the stopping rule that caused the algorithm to terminate iterations.</term>
<term termid="strapplymodel_function"><linktext>StrApplyModel(handle,
"function", category)</linktext>StrApplyModel(handle, "function",
value). String. Applies a particular scoring function to the input
case data using the model specified by handle and where "function"
is one of the following string literal values enclosed in quotes:
predict, stddev, probability, confidence, nodeid, cumhazard, neighbor,
distance. The model handle is the name associated with the external
XML file, as defined on the MODEL HANDLE command. The optional third
argument applies when the function is "probability", "neighbor", or
"distance". For "probability", it specifies a category for which the
probability is calculated. For "neighbor" and "distance", it specifies
a particular neighbor (as an integer) for nearest neighbor models.
StrApplyModel returns a blank string if a value cannot be computed.</term>
<term termid="stratification"><linktext>Stratification</linktext>Stratification
variables define distinct subpopulations, or strata.  Separate samples
are obtained for each stratum.</term>
<term termid="stratum_label"><linktext>Stratum label</linktext>Label
identifying each stratum</term>
<term termid="stress_measures"><linktext>Stress measures</linktext
>The stress measures the misfit of the solution.  A lower stress means
a better fit.</term>
<term termid="string_variable"><linktext>String Variable</linktext
>Values of a string variable are not numeric, and hence not used in
calculations. They can contain any characters up to the defined length.
Upper and lower case letters are considered distinct. Also known as
alphanumeric variables.</term>
<term termid="gstringvariable"><linktext>String Variable</linktext
>Values of a string variable are not numeric, and hence are not used
in calculations. String variables can be converted to or from numeric
variables using the NUMBER and STRING functions in the transformation
language. In command syntax, values of string variables are specified
within single or double quotes.</term>
<term termid="string_numexpr_format"><linktext>STRING(numexpr,format)</linktext
>STRING(numexpr,format). String. Returns the string that results when
numexpr is converted to a string according to format. STRING(-1.5,F5.2)
returns the string value '-1.50'. The second argument format must
be a format for writing a numeric value.</term>
<term termid="sampling_zeros_1"><linktext>Structural Zeros</linktext
>Cells that you have specified to be excluded from the analysis, through
the use of the Cell Structure variable.</term>
<term termid="strunc"><linktext>STRUNC (strexp, length)</linktext
>STRUNC(strexp, length).  String.  Returns strexp truncated to length
(in bytes) and then trimmed of any trailing blanks. Truncation removes
any fragment of a character that would be truncated. </term>
<term termid="studentized"><linktext>Studentized</linktext>The residual
divided by an estimate of its standard deviation that varies from
case to case, depending on the distance of the case's values of the
independent variables from the means of the independent variables.</term>
<term termid="studentized_deleted"><linktext>Studentized Deleted</linktext
>The deleted residual for a case divided by its standard error. The
difference between a Studentized deleted residual and its associated
Studentized residual indicates how much difference eliminating a case
makes on its own prediction.</term>
<term termid="studentized_deleted_residual"><linktext>Studentized
Deleted Residual</linktext>The deleted residual for a case divided
by its standard error. The difference between a Studentized deleted
residual and its associated Studentized residual indicates how much
difference eliminating a case makes on its own prediction.</term>
<term termid="studentized_residual"><linktext>Studentized Residual</linktext
>The residual divided by an estimate of its standard deviation that
varies from case to case, depending on the distance of each case's
values on the independent variables from the means of the independent
variables.</term>
<term termid="studentized_residual_1"><linktext>Studentized Residual</linktext
>The change in the model deviance if a case is excluded.</term>
<term termid="student-newman-keuls_test"><linktext>Student-Newman-Keuls
Test</linktext>The Student-Newman-Keuls test makes all pairwise comparisons
between means using the Studentized range distribution. It also compares
pairs of means within homogeneous subsets, using a stepwise procedure.
Means are ordered from highest to lowest, and extreme differences
are tested first.</term>
<term termid="students_t_variates"><linktext>Student's t Variates</linktext
>Error terms are randomly drawn from a t distribution with the specified
degrees of freedom, and scaled by the root mean squared error (RMSE).</term>
<term termid="subject"><linktext>Subject</linktext>Respondent in a
conjoint study.</term>
<term termid="subject_variable"><linktext>Subject Variable</linktext
>The combination of values of the specified variables should uniquely
define subjects within the dataset. For example, a single Patient
ID variable should be sufficient to define subjects in a single hospital,
but the combination of Hospital ID and Patient ID may be necessary
if patient identification numbers are not unique across hospitals.
In a repeated measures setting, multiple observations are recorded
for each subject, so each subject may occupy multiple cases in the
dataset.</term>
<term termid="subject_variables"><linktext>Subject Variables</linktext
>This lists the variables used to define subjects for this effect.</term>
<term termid="subjects_cscoxreg"><linktext>Subject (CSCoxreg) </linktext
>A single subject in the study can be spread across multiple cases
in order to accommodate piecewise-constant predictors.</term>
<term termid="subpoplation_size"><linktext>Subpoplation Size</linktext
>The number of units in the subpopulation at this stage.</term>
<term termid="subpopulation_subject_size"><linktext>Subpopulation
Subject Size</linktext>The number of subjects in the specified subpopulation.</term>
<term termid="subpopulations_1"><linktext>Subpopulations</linktext
>The number of distinct subpopulations defined by the factors and
covariates.</term>
<term termid="subset"><linktext>Subset</linktext>The procedure creates
a sequence of subsets with an increasing number of predictors in each
subset.</term>
<term termid="substr_strexpr_pos"><linktext>SUBSTR(strexpr,pos)</linktext
>SUBSTR(strexpr,pos). String. Returns the substring beginning at position
pos of strexpr and running to the end of strexpr.</term>
<term termid="substr_strexpr_pos_length"><linktext>SUBSTR(strexpr,pos,length)</linktext
>SUBSTR(strexpr,pos[,length]). String. Returns the substring beginning
at byte position pos of strexpr. The optional third argument represents
the number of bytes in the substring. If the optional argument length
is omitted, returns the substring beginning at byte position pos of
strexpr and running to the end of strexpr.</term>
<term termid="sum_18"><linktext>Sum</linktext>The sum across dimensions.</term>
<term termid="sum_19"><linktext>Sum</linktext>A measure of central
tendency that represents the arithmetic sum of the values of the variable
for the population.</term>
<term termid="sum"><linktext>Sum</linktext>The sum or total of the
values, across all cases with nonmissing values.</term>
<term termid="sum_col_pct_tables_statistic"><linktext>Sum Col % (Tables
Statistic)</linktext>The percentage whose numerator is the sum of
the summarized variable over all cases in the cell (times 100), and
whose denominator is its sum over all cases in the column. Available
for summarized variables and their totals.</term>
<term termid="sum_layer_pct_tables_statistic"><linktext>Sum Layer
% (Tables Statistic)</linktext>The percentage whose numerator is the
sum of the summarized variable over all cases in the cell (times 100),
and whose denominator is its sum over all cases in the layer. Available
for summarized variables and their totals.</term>
<term termid="sum_of_case_weights_rank_cases"><linktext>Sum of Case
Weights (Rank Cases)</linktext>The value of the new variable equals
the sum of case weights. The new variable is a constant for all cases
in the same group.</term>
<term termid="sum_of_ranks"><linktext>Sum of Ranks</linktext>The sum
of the ranks is used in calculating the test statistic.</term>
<term termid="sum_of_squares"><linktext>Sum of Squares</linktext>The
sum of the squared deviations about some quantity.</term>
<term termid="sum_of_squares_5"><linktext>Sum of Squares</linktext
>The sum of squares is a measure of variation in the dependent variable,
usually used to evaluate the performance of a model.  For example,
the residual sum of squares for a model is the sum of squared deviations
between the observed and model predicted values of the dependent variable.</term>
<term termid="sum_of_squares_change"><linktext>Sum of squares change</linktext
>The iterative algorithm stops if the adjusted sums of squares does
not decrease by 0.001% from one iteration to the next. You can choose
a smaller or larger value for more or less precision in the parameter
estimates. For greater precision, it may be necessary to increase
the maximum iterations.</term>
<term termid="sum_of_squares_convergence"><linktext>Sum of Squares
Convergence</linktext>If successive iterations fail to change the
sum of squares by this proportion, the procedure stops. To override
the default value, select an alternate convergence value from the
drop-down list or select disable to disable this criterion.</term>
<term termid="sum_of_values_aggregate_function"><linktext>Sum of Values
(Aggregate Function)</linktext>The sum of the values of the source
variable for all cases in the break group. </term>
<term termid="gbar10"><linktext>Sum of Values (Graph Summary Function)</linktext
>The sum of the values within the category. This is the default for
stacked bar charts, stacked area charts, or pie charts.</term>
<term termid="sum_percentage"><linktext>Sum Percentage</linktext>The
cell sum as a percentage of the total.</term>
<term termid="sum_row_pct_tables_statistic"><linktext>Sum Row % (Tables
Statistic)</linktext>The percentage whose numerator is the sum of
the summarized variable over all cases in the cell (times 100), and
whose denominator is its sum over all cases in the row. Available
for summarized variables and their totals.</term>
<term termid="sum_subtable_pct_tables_statistic"><linktext>Sum Subtable
% (Tables Statistic)</linktext>The percentage whose numerator is the
sum of the summarized variable over all cases in the cell (times 100),
and whose denominator is its sum over all cases in the smallest subtable
containing the cell. Available for summarized variables and their
totals.</term>
<term termid="sum_table_pct_tables_statistic"><linktext>Sum Table
% (Tables Statistic)</linktext>The percentage whose numerator is the
sum of the summarized variable over all cases in the cell (times 100),
and whose denominator is its sum over all cases in the table. Available
for summarized variables and their totals.</term>
<term termid="sum_numexpr_numexpr_.."><linktext>SUM(numexpr,numexpr[,..])</linktext
>SUM(numexpr,numexpr[,..]). Numeric. Returns the sum of its arguments
that have valid, nonmissing values. This function requires two or
more arguments, which must be numeric. You can specify a minimum number
of valid arguments for this function to be evaluated.</term>
<term termid="threshold_sum"><linktext>Sum (threshold)</linktext>The
sum of the column vectors corresponding to threshold parameters.</term>
<term termid="summary"><linktext>Summary</linktext>Displays a Summary
Table. For each step the output indicates entered or removed variables
and gives the resulting value of the statistic used for variable selection
and its significance.</term>
<term termid="summary_inertia"><linktext>Summary inertia</linktext
>Generalized variance. Measure of the total dispersion of the points,
taking their marginal frequencies into account. This measure is the
square of the singular value.</term>
<term termid="summary_total"><linktext>Summary total</linktext>Displays
statistics for the full dimensional solution.</term>
<term termid="sums_of_squares_and_cross_products"><linktext>Sums of
Squares and Cross Products</linktext>The residual SSCP matrix is a
square matrix that consists of sums of squares and cross products
of residuals. The dimension of this matrix is the same as the number
of dependent variables in the model.</term>
<term termid="gscatter06"><linktext>Sunflower</linktext>A symbol that
represents one or more cases that occur very close together on a scatterplot.
Cases are represented as sunflower "petals," spikes extending from
the sunflower.</term>
<term termid="supplementary_cases"><linktext>Supplementary Cases</linktext
>Cases (objects) that are to be analyzed using the solution found.</term>
<term termid="suppress_absolute_values_less_than_n"><linktext>Suppress
Absolute Values Less Than n</linktext>Suppresses coefficients with
absolute values less than the specified value. The default is 0.10.
To override the default, enter a number between 0 and 1.</term>
<term termid="suppress_tables"><linktext>Suppress Tables</linktext
>Displays crosstabulation statistical measures without showing the
actual tables. If no statistics are selected on the Crosstabs Statistics
dialog box, no output will be generated.</term>
<term termid="suppress_tables_with_more_than_n_categories"><linktext
>Suppress Tables with More Than n Categories</linktext>Suppresses
frequency tables for variables with more than the specified number
of categories. Particularly useful if your variable list contains
a mixture of categorical and continuous variables.</term>
<term termid="surrogates_tree"><linktext>Surrogate</linktext>The best
replacement independent variable (predictor) to use for cases with
missing values on the primary predictor.  If the best predictor to
be used for a split has a missing value at a node, then CRT or QUEST
will substitute the surrogate predictor.</term>
<term termid="survival_k-m"><linktext>Survival</linktext>Cumulative
survival probability estimate. The default variable name is the prefix
sur_ with a sequential number appended to it. For example, if sur_1
already exists, Kaplan-Meier assigns the variable name sur_2.</term>
<term termid="survival"><linktext>Survival</linktext>The value of
the cumulative survival function for a given time.  It equals the
probability of survival to that time period.</term>
<term termid="survival_plot"><linktext>Survival Plot</linktext>Displays
the cumulative survival function on a linear scale.</term>
<term termid="survival_time_interval"><linktext>Survival Time Interval</linktext
>If a pattern is specified with piecewise-constant predictors, this
column shows the interval for which each set of predictor values applies.</term>
<term termid="symmetric"><linktext>Symmetric</linktext>The form calculated
by summing the numerators and denominators of the two forms where
each variable in turn is classified as dependent and calculating the
ratio.</term>
<term termid="symmetric_measures"><linktext>Symmetric Measures</linktext
>Symmetric measures of association are those in which interchanging
the two variables in the calculation does not alter the value of the
measure.</term>
<term termid="sysmis_numvar"><linktext>SYSMIS(numvar)</linktext>SYSMIS(numvar).
Logical. Returns 1 or true if the value of numvar is system-missing.
The argument numvar must be the name of a numeric variable in the
active dataset.</term>
<term termid="system_file"><linktext>System File</linktext>A new data
file containing the variance components, covariance matrix, or correlation
matrix.</term>
<term termid="system-missing"><linktext>System-Missing</linktext>Recodes
specified old values into the system-missing value. The system-missing
value is not used in calculations, and cases with the system-missing
value are excluded from many procedures. Not available for string
variables.</term>
<term termid="gsysmis"><linktext>System-Missing Values</linktext>Values
assigned by the program when values in your data are undefined according
to the format type you have specified, when a numeric field is blank,
or when a value resulting from a transformation command is undefined.
Numeric system-missing values are displayed as periods. String variables
cannot have system-missing values, since any character is legal in
a string variable.</term>
<term termid="t"><linktext>t</linktext>A test statistic which, under
the null hypothesis, has a t distribution.</term>
<term termid="t_statistic_2"><linktext>t Statistic</linktext>Test
statistic used to determine if a specified contrast is significant.</term>
<term termid="t_statistic"><linktext>t Statistic</linktext>Statistic
used for testing the null hypothesis that two population means are
equal.</term>
<term termid="t_statistic_1"><linktext>t statistic</linktext>Statistic
used to test the null hypothesis that there is no linear relationship
between a dependent variable and an independent variable or, in other
words, that a regression coefficient is equal to 0. When the significance
level is small (less than 0.10) the coefficient is considered significant.</term>
<term termid="t_statistic_4"><linktext>t statistic</linktext>Statistic
used to test the null hypothesis that the coefficient of the associated
model parameter is zero.</term>
<term termid="t_test"><linktext>t test</linktext>A statistical test
that compares the mean values of two groups.  When the test result
is statistically significant, the means are different.</term>
<term termid="table"><linktext>Table</linktext>This table displays
information for the variables below.</term>
<term termid="def_table"><linktext>Table (output item type)</linktext
>Many statistical procedures generate output in pivot tables. You
can activate a pivot table and use the Pivot Table editor to pivot
different dimensions of the table or reformat its labels, data cells,
or table look in general.</term>
<term termid="tablelook"><linktext>TableLook</linktext>Displays the
name of the current TableLook file.</term>
<term termid="tablelook_files"><linktext>TableLook Files</linktext
>Lists available TableLook files that can be applied to pivot tables.
A number of TableLooks are included with the software, or you can
create your own.</term>
<term termid="tamhanes_t2_post_hoc"><linktext>Tamhane's T2 (Post Hoc)</linktext
>Conservative pairwise comparisons test based on a t test. This test
is appropriate when the variances are unequal.</term>
<term termid="target_spchart"><linktext>Target</linktext>The target
value.</term>
<term termid="target_variable"><linktext>Target Variable</linktext
>A variable that contains the result of a transformation.</term>
<term termid="target_variable_list_1"><linktext>Target Variable List</linktext
>Displays the variables to be summarized.</term>
<term termid="target_variable_list"><linktext>Target Variable List</linktext
>Displays the variables in the current layer. Separate crosstabulations
are produced for each category of each control variable. Each layer
divides the crosstabulation into smaller groups. Crosstabulations
are produced within each combination of categories for each 1st-layer
variable with each 2nd-layer variable; and so on.</term>
<term termid="tarones_statistic"><linktext>Tarone's statistic</linktext
>Tarone's statistic is similar to the Breslow-Day statistic, but it
corrects for the estimator used to measure the common odds ratio.</term>
<term termid="tarone-ware"><linktext>Tarone-Ware</linktext>A test
for comparing the equality of survival distributions. Time points
are weighted by the square root of the number of cases at risk at
each time point.</term>
<term termid="temporary_variable"><linktext>Temporary Variable</linktext
>Variables used to save model results to the working file</term>
<term termid="term_removed"><linktext>Term Removed</linktext>The term
being evaluated for removal</term>
<term termid="territorial_map"><linktext>Territorial Map</linktext
>A plot of the boundaries used to classify cases into groups based
on function values. The numbers correspond to groups into which cases
are classified. The mean for each group is indicated by an asterisk
within its boundaries. The map is not displayed if there is only one
discriminant function.</term>
<term termid="test"><linktext>Test</linktext>Lists the types of tests.</term>
<term termid="test_28"><linktext>Test</linktext>Refers to cases held
out during model building for use in testing the model.</term>
<term termid="test_data_criterion"><linktext>Test Data Criterion</linktext
>A "best" subset criterion based on the average log-likelihood of
the test data.  Smaller values indicate better subsets.</term>
<term termid="test_mean"><linktext>Test Mean</linktext>This is a test
definition.</term>
<term termid="test_model_file"><linktext>Test model file</linktext
>The text file that contains the test sample model in XML (PMML) format.</term>
<term termid="test_of_fucntions"><linktext>Test of functions</linktext
>Indicates the functions compared by the test statistic</term>
<term termid="test_of_linearity"><linktext>Test of Linearity</linktext
>Calculates the sum of squares, degrees of freedom, and mean square
associated with linear and nonlinear components, as well as the F
ratio, R and R-squared. Linearity is not calculated if the independent
variable is a short string.</term>
<term termid="test_of_parallel_lines_ordinal_regression"><linktext
>Test of Parallel Lines (Ordinal Regression)</linktext>Performs a
test of the hypothesis that the slopes are equal across all categories
of the dependent variable. This option is valid for location-only
models.</term>
<term termid="test_of_parallel_lines_chi-square"><linktext>Test of
Parallel Lines Chi-Square</linktext>The score statistic for the test
of parallel lines is asymptotically chi-squared distributed.</term>
<term termid="test_of_parallel_lines_significance"><linktext>Test
of Parallel Lines significance</linktext>A statistical test of the
assumption that the regression coefficients are equal across the values
of the dependent variable.  Small values (less than .1) indicate that
the assumption is not valid.</term>
<term termid="test_pair_s_list"><linktext>Test Pair(s) List</linktext
>Displays the pairs of variables you have chosen for the analysis.
This list must contain at least one pair of variables to run this
procedure.</term>
<term termid="test_proportion_1"><linktext>Test Proportion</linktext
>The default null hypothesis is that the data are from a binomial
distribution with a probability of 0.5 for both groups. To change
the probabilities, enter a test proportion for the first group; the
value must be between .001 and .999 and cannot include leading zeros.</term>
<term termid="test_proportion"><linktext>Test Proportion</linktext
>The probability of success (p) for the specified binomial distribution.</term>
<term termid="test_sample"><linktext>Test sample</linktext>For split-sample
validation, the results based on  applying the training sample model
to the cases not used in the training sample..</term>
<term termid="test_value_1"><linktext>Test Value</linktext>A numeric
value against which the mean of each test variable is tested.</term>
<term termid="test_value_5"><linktext>Test Value</linktext>The hypothesized
value of the contrast.</term>
<term termid="test_value_4"><linktext>Test Value</linktext>The prespecified
value to which the estimate is compared</term>
<term termid="test_value"><linktext>Test Value</linktext>A value used
as a cutting point in the creation of a dichotomous variable. Cases
with values smaller than the test value fall into one group, cases
with values greater than or equal to the test value fall into the
other group.</term>
<term termid="test_value_3"><linktext>Test value</linktext>The value
of the significance test statistic.</term>
<term termid="test_variable_list"><linktext>Test Variable List</linktext
>Displays the variables you have chosen for the analysis. Each variable
produces a separate test.</term>
<term termid="test_variable_s"><linktext>Test Variable(s)</linktext
>Displays the variables you have chosen for the analysis. The mean
must be an appropriate summary statistic for the Test Variable(s).</term>
<term termid="test_variables"><linktext>Test Variables</linktext>Lists
the numeric variables you have selected; this procedure will compare
the distributions of these variables.</term>
<term termid="testing_partition"><linktext>Testing</linktext>The testing
sample is an independent set of data records used to track prediction
error during training in order to prevent overtraining.</term>
<term termid="def_text"><linktext>Text (output item type)</linktext
>Text output. You can activate the output as a text object.</term>
<term termid="textsmart_information"><linktext>TextSmart Information</linktext
>Indicates whether or not the data file contains TextSmart information.</term>
<term termid="the_-_operator"><linktext>The - Operator</linktext>Subtraction.
The following term is subtracted from the preceding term. Both terms
must be numeric.</term>
<term termid="the_and_operator"><linktext>The &amp; Operator</linktext
>Logical And. True if both preceding and following terms are logically
true. The terms may be logical or numeric; numeric terms not equal
to 0 or 1 are treated as missing. This operator is normally used only
in a logical condition.</term>
<term termid="the_operator"><linktext>The ( ) Operator</linktext>Grouping.
Operators and functions within parentheses are evaluated before operators
and functions outside the parentheses. If text is selected in the
Expression when you paste parentheses, the text is placed inside the
parentheses. If no text is selected, the cursor is placed inside the
parentheses.</term>
<term termid="the_mult_operator"><linktext>The * Operator</linktext
>Multiplication. The preceding and following terms are multiplied.
Both terms must be numeric.</term>
<term termid="the_exp_operator"><linktext>The ** Operator</linktext
>Exponentiation. The preceding term is raised to the power of the
following term. If the preceding term is negative, the following term
must be an integer. This operator can produce values too large or
too small for the computer to process, particularly if the following
term (the exponent) is very large or very small.</term>
<term termid="the_div_operator"><linktext>The / Operator</linktext
>Division. The preceding term is divided by the following term. Both
terms must be numeric, and the second must not be zero.</term>
<term termid="the_or_operator"><linktext>The | Operator</linktext
>Logical Or. True if either the preceding or the following term is
logically true (or if both are true). The terms may be logical or
numeric; numeric terms not equal to 0 or 1 are treated as missing.
This operator is normally used only in a logical condition.</term>
<term termid="the_not_operator"><linktext>The ~ Operator</linktext
>Logical Not. True if the following term is false. The terms may be
logical or numeric; a numeric term not equal to 0 or 1 is treated
as missing. This operator is normally used only in a logical condition.</term>
<term termid="the_noteq_operator"><linktext>The ~= Operator</linktext
>Logical Inequality. True for terms that are not exactly equal. If
string terms are of unequal length, the shorter term is padded on
the right with spaces before the comparison. This operator is normally
used only in a logical condition.</term>
<term termid="the_add_operator"><linktext>The + Operator</linktext
>Addition. The preceding term is added to the following term. Both
terms must be numeric.</term>
<term termid="the_lt_operator"><linktext>The &lt; Operator</linktext
>Logical Less Than. True for numeric terms if the preceding term is
less than the following term. True for string terms if the preceding
term appears earlier than the following term in the collating sequence
(in alphabetical order). This operator is normally used only in a
logical condition.</term>
<term termid="the_le_operator"><linktext>The &lt;= Operator</linktext
>Logical Less Than Or Equal. True for numeric terms if the preceding
term is less than or equal to the following term. True for string
terms if the preceding term appears earlier than the following term
in the collating sequence (in alphabetical order), or if the two are
equal. This operator is normally used only in a logical condition.</term>
<term termid="the_eq_operator"><linktext>The = Operator</linktext
>Logical Equality. True for terms that are exactly equal. If string
terms are of unequal length, the shorter term is padded on the right
with spaces before the comparison. This operator is normally used
only in a logical condition.</term>
<term termid="the_gt_operator"><linktext>The &gt; Operator</linktext
>Logical Greater Than. True for numeric terms if the preceding term
is greater than the following term. True for string terms if the preceding
term appears later than the following term in the collating sequence
(in alphabetical order). This operator is normally used only in a
logical condition.</term>
<term termid="the_ge_operator"><linktext>The &gt;= Operator</linktext
>Logical Greater Than Or Equal. True for numeric terms if the preceding
term is greater than or equal to the following term. True for string
terms if the preceding term appears later than the following term
in the collating sequence (in alphabetical order), or if the two are
equal. This operator is normally used only in a logical condition.</term>
<term termid="the_estimation_period_is"><linktext>The Estimation Period
is</linktext>This is defined with the Range subdialog box of the Select
Cases option on the Data menu. If no estimation period has been defined,
all cases are used to predict values.</term>
<term termid="threshold"><linktext>threshold</linktext>The threshold
parameter for the distribution. For instance, for the Pareto distribution
this is the parameter 'a' in PARETO(a,b).</term>
<term termid="threshold_1"><linktext>Threshold</linktext>The thresholds
or constants in the model (corresponding to the intercept in linear
regression models) depend only on which category's probability is
being predicted. Values of the predictor (independent) variables do
not affect this part of the model.</term>
<term termid="gaxis04"><linktext>Tick Mark</linktext>Displays marks
along the axis that indicate the categories or values of a variable.</term>
<term termid="ties"><linktext>Ties</linktext>Two or more observations
or variables having the same value or rank.</term>
<term termid="time_7"><linktext>Time</linktext>Time at which the functions
are evaluated. Values of time represent observed uncensored survival
times in the data.</term>
<term termid="time_8"><linktext>Time</linktext>The time at which the
event or censoring occurred.</term>
<term termid="time"><linktext>Time</linktext>Allows you to select
Time as your independent variable instead of a variable from the active
dataset. If you select Time, the dependent variable should be a time
series measure.</term>
<term termid="time_series_data"><linktext>Time Series Data</linktext
>Corresponds to the sequence of values for a single variable in ordinary
data analysis. Each case (row) in the data represents an observation
at a different time. The observations must be taken at equally spaced
time intervals.</term>
<term termid="time.days_days"><linktext>TIME.DAYS(days)</linktext
>TIME.DAYS(days). Numeric. Returns a time interval corresponding to
the indicated number of days.  The argument must be numeric. To display
the result as a time, assign a time format to the result variable.</term>
<term termid="time.hms_hours_min_sec"><linktext>TIME.HMS(hours[,min,sec])</linktext
>TIME.HMS(hours[,minutes,seconds]). Numeric. Returns a time interval
corresponding to the indicated number of  hours, minutes, and seconds.
The minutes and seconds arguments are optional. Minutes and seconds
must resolve to numbers less than 60 if any higher-order argument
is non-zero. All arguments except the last non-zero argument must
resolve to integers. For example TIME.HMS(25.5) and TIME.HMS(0,90,25.5)
are valid, while TIME.HMS(25.5,30) and TIME.HMS(25,90) are invalid.
All arguments must resolve to either all positive or all negative
values. To display the result as a time, assign a time format to the
result variable. </term>
<term termid="def_title"><linktext>Title (output item type)</linktext
>The output from each procedure has a title preceding the results.
You can activate it as a text object.</term>
<term termid="toeplitz"><linktext>Toeplitz</linktext>This covariance
structure has homogenous variances and heterogeneous correlations
between elements.  The correlation between adjacent elements is homogenous
across pairs of adjacent elements.  The correlation between elements
separated by a third is again homogenous, and so on.</term>
<term termid="tolerance_regression"><linktext>Tolerance (Regression)</linktext
>A statistic used to determine how much the independent variables
are linearly related to one another (multicollinear). The proportion
of a variable's variance not accounted for by other independent variables
in the equation. A variable with very low tolerance contributes little
information to a model, and can cause computational problems. It is
calculated as 1 minus R squared for an independent variable when it
is predicted by the other independent variables already included in
the analysis.</term>
<term termid="total_10"><linktext>Total</linktext>Total variance explained
for the given variable/factor/component.</term>
<term termid="total_12"><linktext>Total</linktext>Total number of
cases for each group</term>
<term termid="total_14"><linktext>Total</linktext>The total number
of cases/objects.</term>
<term termid="total_19"><linktext>Total</linktext>Total number of
cases for all subcategories</term>
<term termid="total"><linktext>Total</linktext>The total number of
cases used in producing a result.</term>
<term termid="total_21"><linktext>Total</linktext>The total number
of cases across subgroups, including those excluded from the computations.</term>
<term termid="total_4"><linktext>Total</linktext>Displays the statistics
for the entire sample.</term>
<term termid="total_23"><linktext>Total</linktext>The total dispersion
in the data.</term>
<term termid="total_9"><linktext>Total</linktext>The total variation
in the dependent variable about the origin.</term>
<term termid="total_eigenvalue"><linktext>Total (Eigenvalue)</linktext
>The sum of the variance accounted for by Multiple Nominal and Non
Multiple Nominal variables.</term>
<term termid="cumulative_generic_1"><linktext>Total (generic)</linktext
>A total of some kind.</term>
<term termid="total_covariance"><linktext>Total Covariance</linktext
>Displays a covariance matrix from all cases as if they were from
a single sample.</term>
<term termid="total_excluded"><linktext>Total excluded</linktext>The
total number of cases excluded from the analysis</term>
<term termid="total_loss"><linktext>Total Loss</linktext>The total
loss, or variance not accounted for, is equal to the centroid coordinates
loss plus the restriction of centroid to vector coordinates loss.</term>
<term termid="total_n_of_items"><linktext>Total N of Items</linktext
>The total number of items used in the associated calculations.</term>
<term termid="total_percentage_crosstabs"><linktext>Total Percentage
(Crosstabs)</linktext>The percentage of all the cases in the crosstabulation,
or the current subtable of the crosstabulation, that are within a
cell.</term>
<term termid="total_proximities"><linktext>Total Proximities</linktext
>The total number of proximities in the matrices; it is equal to the
number of missing and active proximities.</term>
<term termid="total_sum_of_squares"><linktext>Total Sum of Squares</linktext
>The sum of the squared distances of the observed values of a variable
from the variable's mean. The sum of squares is a measure of the total
variation in a variable.</term>
<term termid="total_vaf"><linktext>Total VAF</linktext>The total variance
accounted for by the model.  Better models generally explain more
variance.</term>
<term termid="totals"><linktext>Totals</linktext>Requests totals in
each dimension of the table. If variables are nested in any dimension,
subgroup totals are calculated for that dimension.</term>
<term termid="tp_diagonal"><linktext>TP diagonal</linktext>The homogenous
diagonal term for the Toeplitz covariance structure.</term>
<term termid="tp_rho_1"><linktext>TP rho ^1</linktext>A correlation
term for the Toeplitz covariance structure.</term>
<term termid="tph_rho_1"><linktext>TPH rho ^1</linktext>A correlation
term for the Heterogeneous Toeplitz covariance structure.</term>
<term termid="gaxis10"><linktext>Trailing Character</linktext>A character
added to the end of all the labels on the scale axis. For example,
a trailing character might be a percentage sign at the end of each
label.</term>
<term termid="training"><linktext>Training</linktext>Refers to cases
used to build, or train, the model.</term>
<term termid="training_model_file"><linktext>Training model File</linktext
>The text file that contains the training sample model in XML (PMML)
format.</term>
<term termid="training_partition"><linktext>Training</linktext>The
training sample comprises the data records used to train the model.</term>
<term termid="training_sample"><linktext>Training sample</linktext
>For split-sample validation, the results for the training sample.
That model is then tested by applying the model to the remaining cases
not used in the training sample. If split-sample validation is not
in effect, the training sample model is the model based on all cases
available for the analysis.</term>
<term termid="training_time"><linktext>Training time</linktext>The
total time used to train the model.</term>
<term termid="transform"><linktext>Transform</linktext>To analyze
the dependent variable in a logarithmic scale, select one of the alternatives
on the drop-down list. If you select a log transformation, ARIMA transforms
the predicted values (fit) and confidence intervals (lcl and ucl)
that it creates back into the original metric but leaves the residuals
(err) in the log metric for diagnostic purposes.</term>
<term termid="transformation_type"><linktext>Transformation Type</linktext
>The optimal scaling level: numerical, ordinal or nominal.</term>
<term termid="transformed"><linktext>Transformed</linktext>Transforms
the data according to the function that you specify and plots the
interquartile range and median of the transformed data. After selecting
Transformed, select a transformation function.</term>
<term termid="transformed_variable"><linktext>Transformed Variable</linktext
>A variable transformed by the coefficients in the M matrix, in the
model LBM=K.</term>
<term termid="transient"><linktext>Transient</linktext>An outlier
whose impact decays exponentially to 0.</term>
<term termid="trend"><linktext>Trend</linktext>Long-term changes in
the level of a series. A time series with trend cannot be meaningfully
described by the overall mean and standard deviation, for both statistics
will be influenced by the point and span in time at which the series
is observed. The autocorrelation function plot of a series characterized
by trend dies out in a slow linear fashion, so that autocorrelations
significantly different from 0 can be found at high lags. Typically,
trend is removed by the differencing transformation.</term>
<term termid="gx11arima03"><linktext>Trend Cycle</linktext>The smoothed
original series.</term>
<term termid="gexsmooth06"><linktext>Trend Gamma Value (Exponential
Smoothing)</linktext>Exponential smoothing parameter that controls
the relative weight given to recent observations in estimating the
present series trend. It ranges from 0 to 1, with higher values giving
more weight to recent values. Gamma is used only for exponential smoothing
models with a linear or exponential trend, or a damped trend and no
seasonal component. It is not used for the simple model.</term>
<term termid="gexsmooth08"><linktext>Trend Modification Phi Value
(Exponential Smoothing)</linktext>Exponential smoothing parameter
that controls the rate at which a trend is "damped," or reduced in
magnitude over time. It ranges from 0 to 1 (but cannot equal 1), with
values near 1 representing more gradual damping. Phi is used for exponential
smoothing models with a damped trend. It is not used for the simple,
Holt, or Winters models.</term>
<term termid="trends_date_information"><linktext>Trends Date Information</linktext
>Indicates whether or not the data file contains Trends information.</term>
<term termid="gtrichotomy"><linktext>Trichotomous Variable</linktext
>A variable that has precisely three distinct values, apart from missing
values.</term>
<term termid="trimmed_control_group_span"><linktext>Trimmed Control
Group Span</linktext>The span of the control group after outliers
have been trimmed from each end of the control group. Outliers can
distort the range of the span. See Observed Control Group Span for
a definition of the span.</term>
<term termid="trimmed_mean"><linktext>trimmed mean</linktext>The arithmetic
mean calculated when the largest n% and the smallest n% of the cases
have been eliminated.  Eliminating extreme cases from the computation
of the mean results in a better estimate of central tendency, especially
when the data are nonnormal.</term>
<term termid="true_variance"><linktext>True Variance</linktext>The
"true" variance of a scale is the portion of the common variance that
is not due to error.  Common variance = true variance + error variance.</term>
<term termid="trunc_numexpr"><linktext>TRUNC(numexpr,mult,fuzzbits)</linktext
>TRUNC(numexpr[,mult,fuzzbits]). Numeric. Returns the value of numexpr
truncated toward 0. The optional second argument, mult, specifies
that the result is an integer multiple of this value&#x2014;for example,
TRUNC(4.579,0.1) = 4.5. The value must be numeric but cannot be 0.
The default is 1. The optional third argument, fuzzbits, is the number
of least-significant bits by which numexpr may fall short of the nearest
rounding boundary and be rounded up before truncating. For example,
if numexpr is between 1.4 and 1.5 and mult is 0.1 then fuzzbits specifies
how close numexpr can be to 1.5 (the nearest boundary) and be rounded
up. If omitted, the system setting of FUZZBITS (set to 6 at install
time) is used.</term>
<term termid="truncate_case_weights"><linktext>Truncate case weights</linktext
>Case weights are truncated before use.</term>
<term termid="truncate_cell_counts"><linktext>Truncate cell counts</linktext
>Case weights are used as is but the accumulated weights in the cells
are truncated before computing any statistics.</term>
<term termid="t-test_for_equality_of_means"><linktext>t-test for Equality
of Means</linktext>Tests if two population means are equal.</term>
<term termid="tuckers_coefficient_of_congruence"><linktext>Tucker's
Coefficient of Congruence</linktext>Tucker's coefficient of congruence
is the square root of the dispersion accounted for, and is a measure
of the overall fit of the model.  The higher the Tucker's coefficient,
the better the fit.</term>
<term termid="tukey_rank_cases"><linktext>Tukey (Rank Cases)</linktext
>Uses the formula (r-1/3) / (w+1/3), where r is the rank and w is
the sum of the case weights.</term>
<term termid="gspectra04"><linktext>Tukey Window</linktext>The weights
are Wk = 0.5Dp(2 pi fk) + 0.25Dp (2 pi fk + pi/p) + 0.25Dp(2 pi fk
- pi/p), for k = 0, ..., p, where p is the integer part of half the
span and Dp is the Dirichlet kernel of order p.</term>
<term termid="gspectra03"><linktext>Tukey-Hamming Window</linktext
>The weights are Wk = .54Dp(2 pi fk) + .23Dp (2 pi fk + pi/p) + .23Dp
(2 pi fk - pi/p), for k = 0, ..., p, where p is the integer part of
half the span and Dp is the Dirichlet kernel of order p.</term>
<term termid="tukeys_probability_plot"><linktext>Tukey's (Probability
Plot)</linktext>Calculates the expected normal distribution using
the formula (r-1/3) / (n+1/3), where n is the number of observations,
and r is the rank, ranging from 1 to n.</term>
<term termid="tukeys_b_test"><linktext>Tukey's b Test</linktext>Uses
the Studentized range distribution to make pairwise comparisons between
groups. The critical value is the average of the corresponding value
for the Tukey's honestly significant difference test and the Student-Newman-Keuls.</term>
<term termid="tukeys_biweight_estimator_explore"><linktext>Tukey's
Biweight Estimator (Explore)</linktext>Tukey's biweight estimator
assigns weights of zero for observations with standardized values
greater than 4.685 and weights inversely proportional to the distance
from the center for all other observations.</term>
<term termid="tukeys_hinges_explore"><linktext>Tukey's Hinges (Explore)</linktext
>Summary values in the middle of each half of the data. They are about
a quarter of the way in from each end of the ordered values. These
values are used to compute the interquartile range for the box plots.</term>
<term termid="tukeys_honestly_significant_difference"><linktext>Tukey's
Honestly Significant Difference</linktext>Uses the Studentized range
statistic to make all of the pairwise comparisons between groups.
Sets the experimentwise error rate at the error rate for the collection
for all pairwise comparisons.</term>
<term termid="two-tailed_asymptotic_significance"><linktext>Two-Tailed
Asymptotic Significance</linktext>The probability, based on the asymptotic
distribution of a test statistic and assuming that the data set is
large, of obtaining results as extreme as the one observed, and in
either direction when the null hypothesis is true. A two-tailed significance
level tests a null hypothesis in which the direction of an effect
is not specified in advance.</term>
<term termid="two-tailed_significance"><linktext>Two-Tailed Significance</linktext
>The probability of obtaining results as extreme as the one observed,
and in either direction when the null hypothesis is true. A two-tailed
significance level tests a null hypothesis in which the direction
of an effect is not specified in advance.</term>
<term termid="type"><linktext>Type</linktext>Lists the types of residual
statistics.</term>
<term termid="type_8"><linktext>Type</linktext>Type of estimates reported</term>
<term termid="type_i_sum_of_squares"><linktext>Type I Sum of Squares</linktext
>In Type I sum of squares, each term is adjusted only for the terms
that precede it in the model. This type of sum of squares is also
known as hierarchical decomposition. Under a balanced design it is
an orthogonal decomposition, and the sums of squares in the model
add up to the total sum of squares.</term>
<term termid="type_ii_sum_of_squares"><linktext>Type II sum of Squares</linktext
>The Type II sum of squares method calculates the sum of squares of
an effect in the model adjusted for all other appropriate effects
(one that corresponds to all effects that do not contain the effect
being examined).</term>
<term termid="type_iii_sum_of_squares"><linktext>Type III Sum of Squares</linktext
>The Type III Sum of Squares method calculates the sum of squares
adjusted for any other effects that do not contain the effect of interest,
and orthogonal to any effects that contain it. This method can be
used for balanced and unbalanced designs.</term>
<term termid="type_iv_sum_of_squares"><linktext>Type IV Sum of Squares</linktext
>The Type IV sum of squares method, for effects contained in other
effects, distributes the contrasts being made among the parameters
to all higher-level effects equitably. This method is often used if
there are empty cells.</term>
<term termid="type_of_contrast"><linktext>Type of Contrast</linktext
>The type of contrast.</term>
<term termid="ucl"><linktext>UCL</linktext>Upper confidence limit
for the forecasted value.</term>
<term termid="un_1_2"><linktext>UN: (^1,^2)</linktext>This parameter
defines the specified row and column of an Unstructured covariance
matrix.</term>
<term termid="unadjusted"><linktext>Unadjusted</linktext>Estimates
are not unadjusted for factors and covariates.</term>
<term termid="unadjusted_sig."><linktext>Unadjusted Sig.</linktext
>The significance value of the test, uncorrected for multiple comparisons.</term>
<term termid="uncensored"><linktext>Uncensored</linktext>Cases that
experience the terminal event.</term>
<term termid="uncertainty_coefficient"><linktext>Uncertainty Coefficient</linktext
>A measure of association that indicates the proportional reduction
in error when values of one variable are used to predict values of
the other variable. For example, a value of 0.83 indicates that knowledge
of one variable reduces error in predicting values of the other variable
by 83%. The program calculates both symmetric and asymmetric versions
of the uncertainty coefficient.</term>
<term termid="unconditional_least_squares"><linktext>Unconditional
least squares</linktext>The forecasts are unconditional least squares
forecasts. They are also called finite memory forecasts.</term>
<term termid="unconstrained_matrix"><linktext>Unconstrained Matrix</linktext
>The inter-item covariance matrix without any constraints upon its
structure.</term>
<term termid="uncorrected_total"><linktext>Uncorrected Total</linktext
>Statistics related to the entire variance in the dependent variable.</term>
<term termid="unexplained_variance"><linktext>Unexplained Variance</linktext
>At each step, the variable that minimizes the sum of the unexplained
variation between groups is entered.</term>
<term termid="ungrouped_cases"><linktext>Ungrouped cases</linktext
>Cases which have either missing or out-of-range values for the grouping
variable. These cases are excluded when estimating the discriminant
function(s), but statistics are reported for these cases to assist
with finding potential problems in your model or dataset.</term>
<term termid="uniform"><linktext>Uniform</linktext>The observed cumulative
distribution function is compared to the uniform distribution where
the observed minimum and maximum values define the range.</term>
<term termid="uniform_parameters"><linktext>Uniform Parameters</linktext
>The parameters of the uniform distribution against which the observed
distribution is being tested.</term>
<term termid="uniform_max"><linktext>UNIFORM(max)</linktext>UNIFORM(max).
Numeric. Returns a uniformly distributed pseudorandom number between
0 and the argument max, which must be numeric (but can be negative).
You can repeat the sequence of pseudorandom numbers by setting the
same Random Number Seed (available in the Transform menu) before each
sequence.</term>
<term termid="uniform_distribution_1"><linktext>uniform_distribution</linktext
>The uniform distribution takes two parameters, a and b. The first
parameter a must be less than or equal to the second parameter b.
 A uniform variate takes values between these two parameters with
equal probability; its density function is flat.</term>
<term termid="unique"><linktext>Unique</linktext>Evaluates all effects
simultaneously, adjusting each effect for all other effects of any
type.</term>
<term termid="univariate_anovas"><linktext>Univariate ANOVAs</linktext
>Performs a one-way analysis-of-variance test for equality of group
means for each independent variable.</term>
<term termid="univariate_descriptives"><linktext>Univariate Descriptives</linktext
>Displays the number of valid observations, the mean, and the standard
deviation for each variable.</term>
<term termid="univariate_statistics_1"><linktext>univariate statistics</linktext
>These are descriptive statistics that are computed "one variable
at a time".</term>
<term termid="unrotated_factor_solution"><linktext>Unrotated Factor
Solution</linktext>Displays unrotated factor loadings (factor pattern
matrix), communalities, and eigenvalues for the factor solution.</term>
<term termid="unselected"><linktext>Unselected</linktext>Displays
information for cases not selected by the selection variable.</term>
<term termid="unselected_1"><linktext>Unselected</linktext>Variables
not selected by the procedure.</term>
<term termid="unstandardized"><linktext>Unstandardized</linktext>Displays
the unstandardized discriminant function coefficients.</term>
<term termid="unstandardized_coefficients"><linktext>Unstandardized
Coefficients</linktext>Displays the regression coefficients with their
standard errors.</term>
<term termid="unstandardized_residual_crosstabs"><linktext>Unstandardized
Residual (Crosstabs)</linktext>The difference between an observed
value and the expected value. The expected value is the number of
cases you would expect in the cell if there were no relationship between
the two variables. A positive residual indicates that there are more
cases in the cell than there would be if the row and column variables
were independent.</term>
<term termid="unstandardized_residuals"><linktext>Unstandardized Residuals</linktext
>The difference between an observed value and the value predicted
by the model.</term>
<term termid="unstructured"><linktext>Unstructured</linktext>This
is a completely general covariance matrix.</term>
<term termid="unstructured_correlations"><linktext>Unstructured Correlations</linktext
>This covariance structure has heterogeneous variances and heterogeneous
correlations.</term>
<term termid="untransformed"><linktext>Untransformed</linktext>Produces
plots of the raw data. No transformation of the data is performed.</term>
<term termid="unweighted"><linktext>Unweighted</linktext>The statistic
computed based upon the number of units in the sample.</term>
<term termid="unweighted_cases"><linktext>Unweighted Cases</linktext
>The number of cases used to produce the result shown.</term>
<term termid="unweighted_count"><linktext>Unweighted Count</linktext
>The number of units in the sample.</term>
<term termid="unweighted_least-squares_method"><linktext>Unweighted
Least-Squares Method</linktext>A factor extraction method that minimizes
the sum of the squared differences between the observed and reproduced
correlation matrices (ignoring the diagonals).</term>
<term termid="unweighted_linear_term"><linktext>Unweighted Linear
Term</linktext>The sum of squares for the linear term when the orthogonal
polynomial contrast ignores different sample sizes in the groups.</term>
<term termid="unweighted_n"><linktext>Unweighted N</linktext>Raw (unweighted)
number of cases in each category.</term>
<term termid="unweighted_n_tables_statistic"><linktext>Unweighted
N (Tables Statistic)</linktext>The unweighted number of cases in the
cell. Available for grouping variables and their totals in weighted
data files. Use Count for weighted grouping variables; Valid Value
Count for summarized variables; Respondents for multiple-response
sets.</term>
<term termid="unweighted_unfiltered_n"><linktext>Unweighted N (unfiltered)</linktext
>Unweighted number of cases, ignoring filter status.</term>
<term termid="unweighted_number_of_cases_aggregate_function"><linktext
>Unweighted Number of Cases (Aggregate Function)</linktext>The unweighted
number of cases in the break group with valid data for the source
variable, unless Missing is also selected, in which case this produces
the unweighted number of cases with missing data for the source variable.
This has no effect unless case weighting is in effect.</term>
<term termid="upcas_strexpr"><linktext>UPCASE(strexpr)</linktext>UPCASE(strexpr).
String. Returns strexpr with lowercase letters changed to uppercase
and other characters unchanged.</term>
<term termid="update_type"><linktext>Update Type</linktext>After the
initial estimates, specifies at each step whether the scoring or Newton
algorithm for updating is used.</term>
<term termid="update_type_1"><linktext>Update Type</linktext>This
specifies whether the Newton-Raphson or Fisher scoring method was
used to update the parameter estimates for the given iteration.</term>
<term termid="upper_bound"><linktext>Upper Bound</linktext>The largest
value included in the confidence interval. Values that exceed the
upper bound fall outside the range of the confidence interval.</term>
<term termid="upper_bound_2"><linktext>Upper Bound</linktext>The largest
value included in the interval. Values greater than the upper bound
fall outside the range of the interval.</term>
<term termid="usage_type"><linktext>Usage Type</linktext>The PMML
usage type. The possible values are active (a field used as input),
predicted (a field whose value is predicted by the model), and supplementary
(a field containing descriptive information not required for the model).
For more information, see http://www.dmg.org/v3-1/MiningSchema.html.</term>
<term termid="use_beginning_series_values"><linktext>Use beginning
series values</linktext>The beginning series values are used to initialize
the recursive conditional least squares forecasting algorithm.</term>
<term termid="use_f_value"><linktext>Use F Value</linktext>A variable
is entered into the model if its F value is greater than the Entry
value and is removed if the F value is less than the Removal value.
Entry must be greater than Removal, and both values must be positive.
To enter more variables into the model, lower the Entry value. To
remove more variables from the model, increase the Removal value.</term>
<term termid="use_filter_variable"><linktext>Use Filter Variable</linktext
>Use the selected numeric variable from the data file as the filter
variable. Cases with any value other than 0 or missing for the filter
variable are selected.</term>
<term termid="use_model_constant"><linktext>Use model constant</linktext
>The forecasts are computed by assuming that the unobserved past errors
are 0 and that the unobserved past values of the response series are
equal to the mean.</term>
<term termid="use_probability_of_f"><linktext>Use Probability of F</linktext
>A variable is entered into the model if the significance level of
its F value is less than the Entry value and is removed if the significance
level is greater than the Removal value. Entry must be less than Removal,
and both values must be positive. To enter more variables into the
model, increase the Entry value. To remove more variables from the
model, lower the Removal value.</term>
<term termid="use_specified_range"><linktext>Use Specified Range</linktext
>Categories are established for each value within the inclusive range,
and cases with values outside the bounds are excluded. After selecting
this option, enter integer values for the lower and upper bounds.</term>
<term termid="use_specified_values"><linktext>Use Specified Values</linktext
>After selecting this option, specify a numeric Grouping Variable
code for Group 1 and another for Group 2. All other codes of the grouping
variable will be excluded from the analysis.</term>
<term termid="use_stepwise_method"><linktext>Use Stepwise Method</linktext
>Uses stepwise analysis to control variable entry and removal.</term>
<term termid="used_in_output"><linktext>Used in Output</linktext>The
number of cases used in the output. The number of cases processed
minus the number of cases excluded.</term>
<term termid="user_missing_values"><linktext>User-Missing Values</linktext
>Values you have specified as missing. You can specify individual
missing values for numeric or string variables or a range of missing
values for numeric variables. See also system-missing values.</term>
<term termid="gusermis"><linktext>User-Missing Values (syntax)</linktext
>Values you have specified should be treated as missing, using MISSING
VALUES in command syntax. You can specify individual missing values
for numeric or string variables or a range of missing values for numeric
variables. See also system-missing values.</term>
<term termid="usl"><linktext>USL</linktext>The fixed upper control
limit.</term>
<term termid="utility_score"><linktext>Utility Score</linktext>The
utility score, or part-worth, provides a quantitative measure of the
preference for a factor level, with larger values corresponding to
greater preference. Utility scores can be negative, with larger negative
values corresponding to lower preference.</term>
<term termid="vaf"><linktext>VAF</linktext>The amount of variance
in the data that is explained by the model.  Better models generally
explain more variance.</term>
<term termid="valid_12"><linktext>Valid</linktext>Valid cases for
multiple response analysis must respond to at least one item in the
multiple response set.</term>
<term termid="valid"><linktext>Valid</linktext>Valid cases having
neither the system-missing value, nor a value defined as user-missing.</term>
<term termid="valid_11"><linktext>Valid</linktext>The number of valid
cases by category.</term>
<term termid="valid_cscoxreg"><linktext>Valid (CSCoxreg)</linktext
>Cases used in the analysis must have valid values for all design
variables, time variables, factors, and covariates.  Time variables
must be non-negative.  System- and user-missing values are always
treated as invalid for scale variables.  User-missing values for categorical
variables can optionally be treated as valid.</term>
<term termid="valid_probit"><linktext>Valid (Probit)</linktext>The
number of cases that have not been rejected for any of the reasons
shown in the table.  Valid cases are used in the analysis.</term>
<term termid="valid_active_cases"><linktext>Valid Active Cases</linktext
>The number of cases (objects) that could be used to determine a solution
and do not have missing values.</term>
<term termid="valid_cases"><linktext>Valid Cases</linktext>The number
of cases/objects that have values within the valid range for all variables.
The valid range for a variable is the range 1 through the specified
number of categories.</term>
<term termid="valid_count_percentage"><linktext>Valid Count Percentage</linktext
>The valid cell count as a percentage of the total.</term>
<term termid="valid_n_listwise"><linktext>Valid N (Listwise)</linktext
>The number of valid cases after cases with missing values for any
selected variable have been excluded.</term>
<term termid="valid_n_col_pct_tables_statistic"><linktext>Valid N
Col % (Tables Statistic)</linktext>The percentage of all the cases
in the column which have valid values for the summarized variable
that are in the cell. Available for summarized variables and their
totals.</term>
<term termid="valid_n_layer_pct_tables_statistic"><linktext>Valid
N Layer % (Tables Statistic)</linktext>The percentage of all the cases
in the layer which have valid values for the summarized variable that
are in the cell. Available for summarized variables and their totals.</term>
<term termid="valid_n_row_pct_tables_statistic"><linktext>Valid N
Row % (Tables Statistic)</linktext>The percentage of all the cases
in the row which have valid values for the summarized variable that
are in the cell. Available for summarized variables and their totals.</term>
<term termid="valid_n_subtable_pct_tables_statistic"><linktext>Valid
N Subtable % (Tables Statistic)</linktext>The percentage of all the
cases in the smallest subtable containing the cell which have valid
values for the summarized variable that are actually in the cell.
Available for summarized variables and their totals.</term>
<term termid="valid_n_table_pct_tables_statistic"><linktext>Valid
N Table % (Tables Statistic)</linktext>The percentage of all the cases
in the table which have valid values for the summarized variable that
are in the cell. Available for summarized variables and their totals.</term>
<term termid="valid_n"><linktext>Valid N</linktext>The number of valid
cases.</term>
<term termid="valid_percent"><linktext>Valid Percent</linktext>The
percentage of cases having a particular value when only cases with
nonmissing values are considered. It is obtained by dividing the valid
number of cases by the total number of cases and multiplying by 100.</term>
<term termid="validation"><linktext>Validation</linktext>The method
used to validate the tree. Crossvalidation divides the sample into
a number of subsamples, or folds. Split-sample validation generates
a model  using a training sample and then tests that model  on a hold-out
(testing) sample. Resubstitution is the tree building method without
validation.</term>
<term termid="validation_rule_violations"><linktext>Validation Rule
Violations</linktext>The list of single- and cross-variable rules
violated by the case.</term>
<term termid="value_bootstrap"><linktext>Value</linktext>The value
of the statistic, computed using the original sample.</term>
<term termid="value_17"><linktext>Value</linktext>Lists the value
of the observation.</term>
<term termid="value_18"><linktext>Value</linktext>The value of the
requested statistic.</term>
<term termid="response_rate_value"><linktext>Value</linktext>Sets
the natural response rate in the model (select this item when you
know the natural response rate in advance). Enter the natural response
proportion (the proportion must be less than 1). For example, if the
response occurs 10% of the time when the stimulus is 0, enter 0.10.</term>
<term termid="value_6"><linktext>Value</linktext>Individual old value
to be recoded into a new value. The value must be the same data type
(numeric or string) as the variable(s) being recoded.</term>
<term termid="value_20"><linktext>Value</linktext>The values of the
dichotomous test variables.</term>
<term termid="value"><linktext>Value</linktext>Enter a value for the
case-selection variable to limit the analysis to a subset of cases
having this value for the selected variable.</term>
<term termid="value_display"><linktext>Value (Display)</linktext>Variable
name and data value.</term>
<term termid="value_label"><linktext>Value Label</linktext>Descriptive
label that can be assigned to a particular value of a discrete variable.</term>
<term termid="value_labels_2"><linktext>Value Labels</linktext>The
columns contain value labels.</term>
<term termid="value_of_contrast"><linktext>Value of Contrast</linktext
>Value calculated by summing the products of the contrast coefficients
and the means of each group. A linear combination of means. For example,
for contrast coefficients of 0.5, 0.5, and -1, the value of the contrast
is 0.5 * ( Mean 1) + 0.5 * (Mean 2) - 1 * (Mean 3).</term>
<term termid="value_of_loss_function"><linktext>Value of Loss Function</linktext
>In constrained nonlinear regression problems, you can specify your
own loss function.  This is the value of the function for the current
estimates of the model parameters.</term>
<term termid="value_variable"><linktext>VALUE(variable)</linktext
>VALUE(variable). Numeric or string. Returns the value of variable,
ignoring user missing-value definitions for variable, which must be
a variable name or a vector reference to a variable name.</term>
<term termid="valuedivdf"><linktext>Value/df</linktext>The value of
the goodness-of-fit statistic (the deviance or chi-square statistic)
divided by its degrees of freedom.  This value can be useful in diagnosing
overdispersion in models where the value of the scale parameter is
thought to be a priori known.</term>
<term termid="valuelabel_varname"><linktext>VALUELABEL(varname)</linktext
>VALUELABEL(varname).  String.  Returns the value label for the value
of variable or an empty string if there is no label for the value.
 The value of varname must be a variable name; it cannot be an expression.</term>
<term termid="values"><linktext>Values</linktext>Categories have user-specified
expected proportions. Enter a value greater than 0 for each category
of the test variable, and click Add. Each time you add a value, it
appears at the bottom of the value list. The sequential order of the
values is important, since it corresponds to the ascending order of
the category values of the test variable.</term>
<term termid="values_12"><linktext>Values</linktext>Lists the values
corresponding to the categories.</term>
<term termid="values_10"><linktext>Values</linktext>The Values dimension
displays values and their corresponding significance levels.</term>
<term termid="values_are_group_midpoints"><linktext>Values Are Group
Midpoints</linktext>Calculates Percentile Value statistics and the
median under the assumption that your data have been grouped, and
the values in the data are the midpoints of the original groups.</term>
<term termid="values_listbox"><linktext>Values Listbox</linktext>Lists
the user-specified expected proportions. Each time you add a value,
it appears at the bottom of the value list. The sequential order of
the values is important, since it corresponds to the ascending order
of the category values of the test variable.</term>
<term termid="van_der_waerden_probability_plot"><linktext>Van der
Waerden (Probability Plot)</linktext>Calculates the expected normal
distribution using the formula r/(n+1), where n is the number of observations,
and r is the rank, ranging from 1 to n.</term>
<term termid="van_der_waerden_rank_cases"><linktext>Van der Waerden
(Rank Cases)</linktext>Van der Waerden's transformation, defined by
the formula r/(w+1), where w is the sum of the case weights and r
is the rank, ranging from 1 to w.</term>
<term termid="var_and_var"><linktext>Var &amp; Var</linktext>Results
are displayed for the two variables.</term>
<term termid="var_1"><linktext>Var(^1)</linktext>A variance term for
the Unstructured Correlations covariance structure.</term>
<term termid="var_intercept"><linktext>Var: Intercept</linktext>The
variance term for the intercept.</term>
<term termid="var_1_1"><linktext>Var\: [:^1:]1</linktext>The variance
term for the specified level of the specified effect.</term>
<term termid="var_pct1eqpct2_mult_1eq_2_1"><linktext>Var\: [\[%1=%2\]:*\[^1=^2\]:]1</linktext
>The variance term for the specified interaction effect.</term>
<term termid="variable_21"><linktext>Variable</linktext>Labels the
independent variables.</term>
<term termid="multiple_correspondence_analysis"><linktext>Variable
(generic)</linktext>An attribute or quantity being measured, such
as age or gender.</term>
<term termid="variable_1_and_variable_2"><linktext>Variable 1 and
Variable 2</linktext>Displays the pair of variables currently selected.</term>
<term termid="variable_impact"><linktext>Variable Impact</linktext
>This is the proportional contribution of the variable to the deviation
of the case from its peer group.</term>
<term termid="variable_label_2"><linktext>Variable Label</linktext
>Contains descriptive text of up to 256 bytes used to describe variables</term>
<term termid="variable_label_1"><linktext>Variable Label</linktext
>Descriptive label for the variable. Variable labels can be up to
256 bytes long, but some procedures do not display all 256 bytes.</term>
<term termid="variable_label"><linktext>Variable Label</linktext>Contains
descriptive text of up to 256 bytes used to label output. The variable
label typically appears beside the variable name, and serves to describe
the meaning or origin of the variable. (The output from many procedures
displays only about 40 characters from the variable label.)</term>
<term termid="variable_list"><linktext>Variable List</linktext>Leaves
the variables in the order they were selected.</term>
<term termid="variable_names"><linktext>Variable Names</linktext>A
name assigned to a variable. The name can be up to 64 bytes in length.
It must begin with a letter (A-Z) or the @ character. The remaining
characters in the name can be any letter, any digit, or any of the
five characters _.@#$.</term>
<term termid="variable_norm"><linktext>Variable Norm</linktext>The
typical value of the variable for the peer group to which this case
belongs.</term>
<term termid="variable_sets"><linktext>Variable Sets</linktext>Indicates
whether or not the data file has defined subsets of variables</term>
<term termid="variable_value"><linktext>Variable Value</linktext>The
value of the variable for this case.</term>
<term termid="variable_weight"><linktext>Variable Weight</linktext
>The weight assigned to this variable.</term>
<term termid="variable_s_1"><linktext>Variable(s)</linktext>Displays
the variables you have chosen for the analysis.</term>
<term termid="variable_s"><linktext>Variable(s)</linktext>Displays
the variables you have chosen for the analysis.</term>
<term termid="variables"><linktext>Variables</linktext>Displays the
variables you have chosen for the analysis.</term>
<term termid="variables_26"><linktext>Variables</linktext>These variables
are discarded from consideration because their values are constants
after weighting.</term>
<term termid="variables_24"><linktext>Variables</linktext>Displays
the variables selected for analysis.</term>
<term termid="variables_2"><linktext>Variables</linktext>Variables
are clustered. Distances are computed between variables.</term>
<term termid="variables_created"><linktext>Variables Created</linktext
>Lists the names of the variables created from the Save New Variables
dialog box or the SAVE subcommand. The variables appear in the Data
Editor.</term>
<term termid="variables_created_or_modified"><linktext>Variables Created
or Modified</linktext>A list of variables that were either added to
or modified in the working file.</term>
<term termid="variables_entered"><linktext>Variables Entered</linktext
>Lists the variables entered at each step.</term>
<term termid="variables_in_set"><linktext>Variables in Set</linktext
>Lists the variables you have chosen for the Multiple Response Set.
The variables need to be coded and a set name needs to be supplied.</term>
<term termid="variables_removed"><linktext>Variables Removed</linktext
>Lists the variables removed at each step.</term>
<term termid="variable_type"><linktext>Variable Type</linktext>Fundamental
variable type: string or numeric. Dates and times are numeric variables
with special formatting.</term>
<term termid="variance"><linktext>Variance</linktext>A measure of
dispersion around the mean, equal to the sum of squared deviations
from the mean divided by one less than the number of cases. The variance
is measured in units that are the square of those of the variable
itself.</term>
<term termid="variance_2"><linktext>Variance</linktext>Summary statistics
for item variances. The smallest, largest, and average item variances,
the range and variance of item variances, and the ratio of the largest
to the smallest item variances are displayed.</term>
<term termid="gbar12"><linktext>Variance (Graph Summary Function)</linktext
>A measure of how much observations vary from the mean, expressed
in squared units.</term>
<term termid="variance_component"><linktext>Variance Component</linktext
>Individual parts that are combined linearly to make up the expected
mean square of an effect.</term>
<term termid="variance_components"><linktext>Variance Components</linktext
>This structure assigns a scaled identity (ID) structure to each of
the specified random effects.</term>
<term termid="variance_distance_measure"><linktext>Variance Distance
Measure</linktext>Dissimilarity measure for binary data that has a
minimum value of 0 and no upper limit. Computed from a fourfold table
as (b+c)/(4(a+b+c+d)) where a represents cases present on both items,
d represents cases absent on both items, and b and c represent cases
present on one item but absent on the other.</term>
<term termid="variance_effect"><linktext>Variance Effect</linktext
>The values are the coefficients of the variance of the specified
effect, used in computing the expected mean square.</term>
<term termid="variance_error"><linktext>Variance error</linktext>The
values are the coefficients of the variance of error, used in computing
the expected mean square.</term>
<term termid="variance_inflation_factor_vif"><linktext>Variance Inflation
Factor (VIF)</linktext>The reciprocal of the tolerance. As the variance
inflation factor increases, so does the variance of the regression
coefficient, making it an unstable estimate. Large VIF values are
an indicator of multicollinearity.</term>
<term termid="variance_proportions"><linktext>Variance Proportions</linktext
>The proportions of the variance of the estimate accounted for by
each principal component associated with each of the eigenvalues.
Collinearity is a problem when a component associated with a high
condition index contributes substantially to the variance of two or
more variables.</term>
<term termid="variance_numexpr_numexpr_.."><linktext>VARIANCE(numexpr,numexpr[,..])</linktext
>VARIANCE(numexpr,numexpr[,..]). Numeric. Returns the variance of
its arguments that have valid values. This function requires two or
more arguments, which must be numeric. You can specify a minimum number
of valid arguments for this function to be evaluated.</term>
<term termid="variation_coefficients"><linktext>Variation Coefficients</linktext
>Coefficients of variation (standard deviation divided by the mean)
for various measurements.</term>
<term termid="varimax_method"><linktext>Varimax Method</linktext>An
orthogonal rotation method that minimizes the number of variables
that have high loadings on each factor. This method simplifies the
interpretation of the factors.</term>
<term termid="varn_dependent"><linktext>Varn Dependent</linktext>The
procedure declares each variable in turn as dependent and computes
the directional measure. The choice of whether a variable is dependent
or independent depends on the nature of the problem. These measures
range from 0 (knowledge of independent variable no help in predicting
dependent variable) to 1(independent variable perfectly predicts the
dependent variable).</term>
<term termid="var-var"><linktext>Var-Var</linktext>Results are displayed
for the difference between two variables.</term>
<term termid="vc_variance"><linktext>VC variance</linktext>A variance
term for the Variance Components covariance structure.</term>
<term termid="vertical_icicle_plot"><linktext>Vertical Icicle Plot</linktext
>A figure used to summarize the formation of clusters during a cluster
analysis. Its name comes from its resemblance to a row of icicles
hanging from eaves. Cases form the columns of the display; steps form
the rows. Cases joined horizontally form clusters.</term>
<term termid="visible_category"><linktext>Visible Category</linktext
>Displays the dimensions and categories of the currently displayed
layer.</term>
<term termid="wald"><linktext>Wald</linktext>Wald's statistic for
testing significance of parameter estimates.</term>
<term termid="wald_chi-square"><linktext>Wald Chi-Square</linktext
>This is a statistic for testing hypotheses.  It has an asymptotic
chi-square distribution with the listed degrees of freedom.  If the
significance value of the test is less than 0.05, you can reject the
null hypothesis.</term>
<term termid="wald_confidence_intervals"><linktext>Wald Confidence
Intervals</linktext>Computation of Wald confidence intervals is based
upon the asymptotic normality of the parameter estimates; thus, the
upper and lower bounds of the interval correspond to percentiles of
a normal distribution with mean equal to the parameter estimate and
standard deviation equal to the standard deviation of the parameter
estimate.</term>
<term termid="wald_confidence_intervals_1"><linktext>Wald Confidence
Intervals for Exp(B)</linktext>The upper and lower bounds of a confidence
interval for an exponentiated parameter estimate are simply the exponentiated
bounds of the confidence interval for the parameter estimate.</term>
<term termid="wald-wolfowitz_runs_test"><linktext>Wald-Wolfowitz Runs
Test</linktext>A nonparametric test of the hypothesis that two samples
come from the same population. Requires at least an ordinal scale
of measurement. The values of the observations from both samples are
combined and ranked from smallest to largest. Runs are sequences of
values from the same group. If the samples are from the same population,
the two groups should be randomly scattered throughout the ranking.</term>
<term termid="waller-duncan_post_hoc"><linktext>Waller-Duncan (Post
Hoc)</linktext>Multiple comparison test based on a t statistic; uses
a Bayesian approach.</term>
<term termid="def_warning"><linktext>Warning (output item type)</linktext
>Warnings generated during the execution of the commands. You can
activate it as a text object.</term>
<term termid="waverage"><linktext>Waverage</linktext>The pth percentile
is estimated using a weighted average around the np-1 data point.</term>
<term termid="weight_variable_wls"><linktext>Weight Variable</linktext
>The data are weighted by the reciprocal of this variable raised to
a power. The regression equation is calculated for each of a specified
range of power values and indicates the power that maximizes the log-likelihood
function.</term>
<term termid="weight_variable_file_information"><linktext>Weight Variable
(file information)</linktext>If weighting is on, name of the weight
variable.</term>
<term termid="weighted"><linktext>Weighted</linktext>The statistic
computed based upon the estimated number of units in the population.</term>
<term termid="weighted_count_1"><linktext>Weighted Count</linktext
>The estimated number of units in the population that fall into this
category.</term>
<term termid="weighted_count"><linktext>Weighted Count</linktext>Number
of cases in each category. This measure is weighted by the weight
variable.</term>
<term termid="weighted_frequency"><linktext>Weighted Frequency</linktext
>This is the positive integer weight given to the object.</term>
<term termid="weighted_least_squares_wls"><linktext>Weighted Least
Squares (WLS)</linktext>WLS gives observations different weight, perhaps
to compensate for different precision of measurement. This is not
the same as the use of case weights that change the effective sample
size. To obtain a WLS solution, specify a weighting variable. For
weighted residual analysis, save the residuals or predicted values
as new variables, then multiply these new variables by the square
root of the weighting variable specified here.</term>
<term termid="weighted_linear_term"><linktext>Weighted Linear Term</linktext
>The sum of squares for the linear term when group sizes are included
in computing the orthogonal polynomial coefficients.</term>
<term termid="weighted_mean_1"><linktext>Weighted Mean</linktext>The
result of dividing the mean of the numerator by the mean of the denominator.
It is also the mean of the ratios weighted by the denominator.</term>
<term termid="weighted_mean"><linktext>Weighted Mean</linktext>A measure
of central tendency. The arithmetic average; the sum divided by the
number of cases. This measure is weighted by the weight variable.</term>
<term termid="weighted_mean_square"><linktext>Weighted Mean Square</linktext
>The weighted mean square accounts for user-specified case weights.</term>
<term termid="weighted_median"><linktext>Weighted Median</linktext
>A measure of central tendency not sensitive to outlying values. The
value above and below which half the case weights fall, the 50th percentile.
This measure is weighted by the weight variable.</term>
<term termid="weighted_mode"><linktext>Weighted Mode</linktext>The
value with the greatest sum of weights. If several values share the
greatest sum of weights, each of them is a mode.</term>
<term termid="weighted_unfiltered_n"><linktext>Weighted N (unfiltered)</linktext
>The weighted number of cases, ignoring filter status.</term>
<term termid="weighted_percent"><linktext>Weighted Percent</linktext
>The estimated percentage of units in the population that fall into
this category.</term>
<term termid="weighted_predicted"><linktext>Weighted Predicted</linktext
>Weighted unstandardized predicted values. Available only if a WLS
variable was previously selected.</term>
<term termid="weighted_residuals"><linktext>Weighted Residuals</linktext
>Weighted unstandardized residuals. Available only if a WLS variable
was previously selected.</term>
<term termid="weighted_standard_deviation"><linktext>Weighted Standard
Deviation</linktext>A measure of dispersion around the mean, expressed
in the same units of measurement as the observations, equal to the
square root of the variance. This measure is weighted by the weight
variable.</term>
<term termid="weighted_standard_error_of_mean"><linktext>Weighted
Standard Error of Mean</linktext>A measure of how much the value of
the mean may vary from sample to sample taken from the same distribution.
It is the standard deviation of the distribution of all possible means,
if samples of the same size were repeatedly taken. This measure is
weighted by the weight variable.</term>
<term termid="weighted_sum"><linktext>Weighted Sum</linktext>The sum
or total of the values, across all cases with nonmissing values. This
measure is weighted by the weight variable.</term>
<term termid="weighted_sum_of_squares"><linktext>Weighted Sum of Squares</linktext
>The sum of squares is a measure of variation in the dependent variable,
usually used to evaluate the performance of a model.  For example,
the residual sum of squares for a model is the sum of squared deviations
between the observed and model predicted values of the dependent variable.
The weighted sum of squares accounts for user-specified case weights.</term>
<term termid="weighted_valid_n"><linktext>Weighted Valid N</linktext
>The number of valid cases after cases with missing values for any
selected variable have been excluded. This measure is weighted by
the weight variable.</term>
<term termid="weighted_variance"><linktext>Weighted Variance</linktext
>A measure of dispersion around the mean, equal to the sum of squared
deviations from the mean divided by one less than the number of cases.
The variance is measured in units that are the square of those of
the variable itself. This measure is weighted by the weight variable.</term>
<term termid="weights_prefscal"><linktext>Weights (prefscal)</linktext
>The proximity weights specified on the multidimensional unfolding
weight variables.</term>
<term termid="weights_and_component_loadings"><linktext>Weights and
component loadings</linktext>The regression coefficients in each dimension
for every quantified variable in a set, where the object scores are
regressed on the quantified variables, and the projection of the quantified
variable in the object space. Provides an indication of the contribution
each variable makes to the dimension within each set.</term>
<term termid="welch"><linktext>Welch</linktext>The Welch statistic
tests for the equality of group means.  This statistic is preferable
to the F statistic when the assumption of equal variances does not
hold.</term>
<term termid="wilcoxon_gehan_statistic"><linktext>Wilcoxon (Gehan)
Statistic</linktext>A nonparametric statistic used to test the equality
of "typical" survival times for two or more groups.  If significant,
the test indicates that the survival times are different.</term>
<term termid="wilcoxon_signed-rank_test"><linktext>Wilcoxon Signed-Rank
Test</linktext>A nonparametric procedure used with two related variables
to test the hypothesis that the two variables have the same distribution.
It makes no assumptions about the shapes of the distributions of the
two variables. This test takes into account information about the
magnitude of differences within pairs and gives more weight to pairs
that show large differences than to pairs that show small differences.
The test statistic is based on the ranks of the absolute values of
the differences between the two variables.</term>
<term termid="wilcoxon_w_mann-whitney"><linktext>Wilcoxon W (Mann-Whitney)</linktext
>The sum of ranks for the larger of the two groups in the Mann-Whitney
Wilcoxon rank sum test. If the two groups are the same size, W is
computed for the second of the two groups listed on your output.</term>
<term termid="wilks_chi-square"><linktext>Wilks' Chi-Square</linktext
>A chi-square transformation of Wilks' lambda is used to determine
significance. If the significance value is small (less than 0.10)
the group means differ.</term>
<term termid="wilks_lambda"><linktext>Wilks' Lambda</linktext>A multivariate
test of significance, sometimes called the U statistic. Lambda ranges
between 0 and 1, with values close to 0 indicating the group means
are different and values close to 1 indicating the group means are
not different (equal to 1 indicates all means are the same).</term>
<term termid="winters_additive"><linktext>Winters' additive</linktext
>This model is appropriate for series with a linear trend and a seasonal
effect that does not depend on the level of the series. Its smoothing
parameters are level, trend, and season. Winters' additive exponential
smoothing is most similar to an ARIMA model with zero orders of autoregression,
one order of differencing, one order of seasonal differencing, and
p + 1 orders of moving average, where p is the number of periods in
a seasonal interval (for monthly data, p = 12).</term>
<term termid="gexsmooth03"><linktext>Winters Model (Exponential Smoothing)</linktext
>The Winters model assumes that the series has a linear trend and
multiplicative seasonal variation (its magnitude increases or decreases
with the overall level of the series).</term>
<term termid="winters_multiplicative"><linktext>Winters' multiplicative</linktext
>This model is appropriate for series with a linear trend and a seasonal
effect that depends on the level of the series. Its smoothing parameters
are level, trend, and season. Winters' multiplicative exponential
smoothing is not similar to any ARIMA model.</term>
<term termid="with_effects"><linktext>With Effects</linktext>Enters
the covariates at the same time the factor effects are evaluated.
Available only if one or more covariates are in the model and the
Hierarchical or Experimental method is used.</term>
<term termid="within_groups"><linktext>Within Groups</linktext>Variability
of the observations in a group about their group mean. Sometimes referred
to as error variation.</term>
<term termid="within_people"><linktext>Within People</linktext>The
part of total variability in the scale that can be accounted for by
differences within people.</term>
<term termid="within_percent_of_median"><linktext>Within Percent of
Median</linktext>This reports the percentage of ratios within the
specified percentage of the median. The lower end of the interval
is equal to (1-0.01*value)*median, and the upper end is equal to (1+0.01*value)*median.</term>
<term termid="within-groups"><linktext>Within-Groups</linktext>The
pooled within-groups covariance matrix is used to classify cases.</term>
<term termid="within-groups_correlation"><linktext>Within-Groups Correlation</linktext
>Displays a pooled within-groups correlation matrix that is obtained
by averaging the separate covariance matrices for all groups before
computing the correlations.</term>
<term termid="within-groups_covariance"><linktext>Within-Groups Covariance</linktext
>Displays a pooled within-groups covariance matrix, which may differ
from the total covariance matrix. The matrix is obtained by averaging
the separate covariance matrices for all groups.</term>
<term termid="within-groups_log_determinants"><linktext>Within-Groups
Log Determinants</linktext>The pooled within-groups log determinant
is obtained by averaging the separate determinants for all groups
before computing the log determinant.</term>
<term termid="within-groups_variability"><linktext>Within-Groups Variability</linktext
>The part of total variability in the dependent variable due to error.
This estimate of variability uses the variances (standard deviations
squared) of each group.</term>
<term termid="within-subject_variable"><linktext>Within-Subject Variable</linktext
>The combination of values of the within-subject variables defines
the ordering of measurements within subjects; thus, the combination
of within-subject and subject variables uniquely defines each measurement.
For example, the combination of Period, Hospital ID and Patient ID
defines, for each case, a particular office visit for a particular
patient within a particular hospital.  If the dataset is already sorted
so that each subject's repeated measurements occur in a contiguous
block of cases and in the proper order, it is not strictly necessary
to specify a within-subjects variable; however, it's generally a good
idea to make use of within-subject variables to ensure proper ordering
of measurements.</term>
<term termid="within-subjects_3"><linktext>Within-Subjects</linktext
>A within-subjects factor distinguishes measurements made on the same
subject or case.</term>
<term termid="wls"><linktext>WLS</linktext>Allows you to obtain a
weighted least-squares model. Data points are weighted by the reciprocal
of their variances. This means that observations with large variances
have less impact on the analysis than observations associated with
small variances. If the value of the weighting variable is zero, negative,
or missing, the case is excluded from the analysis.</term>
<term termid="wls_weight"><linktext>WLS Weight</linktext>Lists the
numeric variable containing the weights for weighted least-squares
analysis. If the value of the weighting variable is zero, negative,
or missing, the case is excluded from the analysis. A variable already
used in the model cannot be used as a weighting variable.</term>
<term termid="valuedivdf_1"><linktext>Working Correlation Matrix Structure</linktext
>This correlation matrix represents the within-subject dependencies.
 Its size is determined by the number of measurements, and thus the
combination of values of within-subject variables.  The available
structures include independent, AR(1), exchangeable, m-dependent,
and unstructured.</term>
<term termid="x_variable"><linktext>X Variable</linktext>Select a
variable for the horizontal x-axis.</term>
<term termid="xmultbeta"><linktext>X*Beta</linktext>Linear predictor
score. The sum of the product of mean-centered covariate values and
their corresponding parameter estimates for each case.</term>
<term termid="x11_arima"><linktext>X11 ARIMA</linktext>A procedure
based on the Statistics Canada modification to the U.S. Bureau of
the Census Method II-X-11 seasonal adjustment program, used by many
government agencies and businesses worldwide for analyzing time series
with seasonal effects.</term>
<term termid="gcontrol01"><linktext>X-Bar, R (Range) and s (Standard
Deviation) Charts</linktext>Plot measurements of a process over time.
X-Bar charts plot the mean within each subgroup or time interval,
R charts plot subgroup ranges, and S charts plot the standard deviation
within each subgroup. X-Bar charts are generally interpreted in conjunction
with either an R Chart or an s chart.</term>
<term termid="xdate.date_datevalue"><linktext>XDATE.DATE(datevalue)</linktext
>XDATE.DATE(datevalue). Numeric. Returns the date portion from a numeric
value that represents a date. The argument can be a number, a date
format variable, or an expression that resolves to a date. To display
the result as a date, apply a date format to the variable.</term>
<term termid="xdate.hour_datevalue"><linktext>XDATE.HOUR(datetime)</linktext
>XDATE.HOUR(datetime). Numeric. Returns the hour (an integer between
0 and 23) from a value that represents a time or a datetime. The argument
can be a number, a time or datetime variable or an expression that
resolves to a time or datetime value.</term>
<term termid="xdate.jday_datevalue"><linktext>XDATE.JDAY(datevalue)</linktext
>XDATE.JDAY(datevalue). Numeric. Returns the day of the year (an integer
between 1 and 366)  from a numeric value that represents a date. The
argument can be a number, a date format variable, or an expression
that resolves to a date.</term>
<term termid="xdate.mday_datevalue"><linktext>XDATE.MDAY(datevalue)</linktext
>XDATE.MDAY(datevalue). Numeric. Returns the day of the month (an
integer between 1 and 31) from a numeric value that represents a date.
The argument can be a number, a date format variable, or an expression
that resolves to a date.</term>
<term termid="xdate.minute_datevalue"><linktext>XDATE.MINUTE(datetime)</linktext
>XDATE.MINUTE(datetime). Numeric. Returns the minute (an integer between
0 and 59) from a value that represents a time or a datetime. The argument
can be a number, a time or datetime variable, or an expression that
resolves to a time or datetime value.</term>
<term termid="xdate.month_datevalue"><linktext>XDATE.MONTH(datevalue)</linktext
>XDATE.MONTH(datevalue). Numeric. Returns the month (an integer between
1 and 12) from a numeric value that represents a date. The argument
can be a number, a date format variable, or an expression that resolves
to a date.</term>
<term termid="xdate.quarter_datevalue"><linktext>XDATE.QUARTER(datevalue)</linktext
>XDATE.QUARTER(datevalue). Numeric. Returns the quarter of the year
(an integer between 1 and 4) from a numeric value that represents
a date. The argument can be a number, a date format variable, or an
expression that resolves to a date.</term>
<term termid="xdate.second_datevalue"><linktext>XDATE.SECOND(datetime)</linktext
>XDATE.SECOND(datetime). Numeric. Returns the second (a number between
0 and 60) from a value that represents a time or a datetime. The argument
can be a number, a time or datetime variable or an expression that
resolves to a time or datetime value.</term>
<term termid="xdate.tday_timevalue"><linktext>XDATE.TDAY(timevalue)</linktext
>XDATE.TDAY(timevalue). Numeric. Returns the number of whole days
(as an integer) from a numeric value that represents a time interval.
The argument can be a number, a time format variable, or an expression
that resolves to a time interval.</term>
<term termid="xdate.time_datevalue"><linktext>XDATE.TIME(datetime)</linktext
>XDATE.TIME(datetime). Numeric. Returns the time portion  from a value
that represents a time or a datetime. The argument can be a number,
a time or datetime variable or an expression that resolves to a time
or datetime value. To display the result as a time, apply a time format
to the variable.</term>
<term termid="xdate.week_datevalue"><linktext>XDATE.WEEK(datevalue)</linktext
>XDATE.WEEK(datevalue). Numeric. Returns the week number (an integer
between 1 and 53) from a numeric value that represents a date. The
argument can be a number, a date format variable, or an expression
that resolves to a date.</term>
<term termid="xdate.wkday_datevalue"><linktext>XDATE.WKDAY(datevalue)</linktext
>XDATE.WKDAY(datevalue). Numeric. Returns the day-of-week number (an
integer between 1, Sunday, and 7, Saturday) from a numeric value that
represents a date. The argument can be a number, a date format variable,
or an expression that resolves to a date.</term>
<term termid="xdate.year_datevalue"><linktext>XDATE.YEAR(datevalue)</linktext
>XDATE.YEAR(datevalue). Numeric. Returns the year (as a four-digit
integer) from a numeric value that represents a date. The argument
can be a number, a date format variable, or an expression that resolves
to a date.</term>
<term termid="y_variable"><linktext>Y Variable</linktext>Select a
variable for the vertical y-axis.</term>
<term termid="yates_correction_for_continuity"><linktext>Yates' Correction
for Continuity</linktext>A correction sometimes applied in the computation
of chi-square for 2 X 2 tables to improve the approximation. Corrected
chi-square values are always smaller than uncorrected values.</term>
<term termid="yrmoda_year_month_day"><linktext>YRMODA(year,month,day)</linktext
>YRMODA(year,month,day). Numeric. Returns the number of days from
October 15, 1582 to the date represented by the arguments year, month,
and day, which must be integers that form a valid date since October
15, 1582. Two-digit values of year assume a century based on Options
settings (Edit menu, Options, Data tab). By default, 2-digit years
assume a range beginning 69 years prior to the current date and ending
30 years after the current date..</term>
<term termid="yules_q"><linktext>Yule's Q</linktext>A similarity measure
for binary data. The 2x2 version of Goodman and Kruskal's gamma. Q
is a function of the cross-ratio for a 2x2 table and has a range of
-1 to +1. Computed from a fourfold table as (ad-bc)/(ad+bc) where
a represents cases present on both items, b and c represent cases
present on one item but absent on the other, and d represents cases
absent on both items.</term>
<term termid="yules_y"><linktext>Yule's Y</linktext>A similarity coefficient
for binary data. Yule's Y coefficient of colligation is a function
of the cross-ratio for a 2x2 table and has a range of -1 to +1. Computed
from a fourfold table as (SQRT(ad)-SQRT(bc))/(SQRT(ad)+SQRT(bc)) where
a represents cases present on both items, b and c represent cases
present on one item but absent on the other, and d represents cases
absent on both items.</term>
<term termid="z_2"><linktext>Z</linktext>This test statistic has an
approximately normal distribution.</term>
<term termid="z"><linktext>z</linktext>A test statistic which, for
sufficiently large sample sizes, is approximately normally distributed.
Often it is the ratio of an estimate to its standard error.</term>
<term termid="z_score"><linktext>z score</linktext>Also known as a
standardized value.  To obtain z-scores for a variable, for each case
subtract the variable's mean value and divide by the standard deviation.
 Z-scores are useful for finding outliers and comparing values of
variables that are measured on different scales.</term>
<term termid="zero_matrix"><linktext>Zero Matrix</linktext>The default
contrast results matrix.</term>
<term termid="zero_weights"><linktext>Zero Weights</linktext>The number
of proximities with zero weights.</term>
<term termid="zero-order_correlations"><linktext>Zero-Order Correlations</linktext
>The ordinary correlation coefficients, with no control variables.
Values of the correlation coefficient range from -1 to 1. The sign
of the coefficient indicates the direction of the relationship, and
its absolute value indicates the strength, with larger absolute values
indicating stronger relationships.</term>
<term termid="zero-order_gammas"><linktext>Zero-Order Gammas</linktext
>Zero-order gammas are displayed for 2-way tables.</term>
</glossary></sourcedocument>
<?Pub *0000672350 0?>
